<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[饿了么面试记录（继续完善）]]></title>
    <url>%2Feleme_interview.html</url>
    <content type="text"><![CDATA[饿了么面试记录 对于技术的抽象概括，以及如何技术变现有了更好的理解 一面 介绍项目，当中使用到消息队列，请问如何解决实时性？ 对于订单反作弊状态的更新，你在项目中是通过将请求放入消息队列进行更新，频次是天，如何提高实时性变为分钟级别？ 引入消息队列，将不是必须的业务逻辑，异步处理。 消息队列推拉模式？订单系统作为请求的生产方不断轮询反作弊状态的更新？ 定时任务，根据更新频率不断去查询更新？ 消息通知？RabbitMQ的原理。 数据库的触发器设置，这种方式需要接口设计，对于系统数量增加后设计复杂重复程度会增加。 增量日志binlog来进行更新？ 参考文章： 大型网站架构之分布式消息队列 消息队列讲解 消息队列的消费方突然挂掉，时间比如是一天，当重启服务时会发生什么？ 服务不可用期间对于请求的积压对于服务重启的压力？ 消息队列一般会把接收到的消息存储到本地硬盘上（当消息被处理完之后，存储信息根据不同的消息队列实现，有可能将其删除），这样即使应用挂掉或者消息队列本身挂掉，消息也能够重新加载。 同步与异步的区别，请举出实例 同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。 而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 参考文章怎样理解阻塞非阻塞与同步异步的区别？ 其他 论技术变现，解决别人无法解决的问题，业务与技术的结合 无论系统大小，关注两个点：如何适应变化？如何保证代码的整洁？]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>eleme. interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网易游戏面试记录]]></title>
    <url>%2Fneteasegame_interview.html</url>
    <content type="text"><![CDATA[网易游戏面试记录 期望有一个好的结果 一面 如何实现一个分布式的主键ID，画出设计流程图？ 特点： 全局唯一性：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。 递增：比较低要求的条件为趋势递增，即保证下一个ID一定大于上一个ID，而比较苛刻的要求是连续递增，如1，2，3等等。 高可用高性能：高性能是指必须要在压测下表现良好，如果达不到要求则在高并发环境下依然会导致系统瘫痪。 方案： 数据库自增序列或是字段： 数字ID天然排序。 针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 1，4，7，10，Master2生成的是2，5，8，11 Master3生成的是 3，6，9，12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。 UUID： UUID是指在一台机器上生成的数字，它保证对在同一时空中的所有机器都是唯一的。通常平台会提供生成的API。UUID(Universally Unique Identifier)的标准型式包含32个16进制数字(每个字符0-F的字符代表4bit，共128bit)，以连字号分为五段，形式为8-4-4-4-12的32+4个字符。比如bc96c351-bea3-4e53-b0a8-d9806763dd69。 主要的格式如下： 时间戳＋UUID版本号，分三段占16个字符(60bit+4bit)， Clock Sequence号与保留字段，占4个字符(13bit＋3bit)，节点标识占12个字符(48bit)，version 4 基于随机数的算法，也是JDK里的算法，不管原来各个位的含义了，除了少数几个位必须按规范填，其余全部用随机数表达。 字符串存储查询效率低、存储空间大、传输量大、不可读。 UUID to Int64解决不可读问题、Comb算法解决UUID无序。 snowflake： snowflake是Twitter开源的分布式ID生成算法，结果是一个long型的ID。其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。 id = timestamp | workerid | sequence (eg. 1451063443347648410) snowflake生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和workerId作区分），并且效率较高。这个算法单机每秒内理论上最多可以生成1000*(2^12)，也就是400W的ID。 snowflake 百度的UidGenerator基于Snowflake算法的唯一ID生成器：UidGenerator 其他方案： 通过Redis生成ID(主要通过redis的自增函数)：用Redis的原子操作 INCR和INCRBY来实现。 ZooKeeper生成ID：使用zk的永久sequence策略创建节点，并获取返回值，然后删除前一个节点，这样既防止zk服务器存在过多的节点，又提高了效率;节点删除采用线程池来统一处理，提高响应速度。 MongoDB的ObjectID：MongoDB的ObjectId和snowflake算法类似。它设计成轻量型的，不同的机器都能用全局唯一的同种方法方便地生成它。MongoDB 从一开始就设计用来作为分布式数据库，处理多个节点是一个核心要求。使其在分片环境中要容易生成得多。 参考文章： 分布式id生成方案概述 分布式系统唯一ID生成方案汇总 MySQL数据库存储引擎之间的区别？ 主要区别在于：事务、锁、索引、外键、存储总行数 MyISAM 不需要事务支持（不支持） 并发相对较低（锁定机制问题） 数据修改相对较少（阻塞问题） 以读为主 数据一致性要求不是非常高 InnoDB 需要事务支持（具有较好的事务特性） 行级锁定对高并发有很好的适应能力，但需要确保查询是通过索引完成 数据更新较为频繁的场景 数据一致性要求较高 硬件设备内存较大，可以利用InnoDB较好的缓存能力来提高内存利用率，尽可能减少磁盘IO 参考文章：MySQL数据库三种常用存储引擎特性对比 MySQL表的极限有了解？ 不同的操作系统所采用的文件系统能支持的单个文件大小也不相同。 InnoDB存储引擎的表空间最大容量为64TB，不具体限制单表的大小，但受限于表空间。MySQL5.0版本后，默认MyISAM表的限制是256TB。 参考文章： 关于MySQL单表支持的最大存储空间 MySQL5.6Innodb表的特性和极限 分库分表策略？ 分库分表：sharding 路由策略： 中间变量 ＝ user_id%（库数量*每个库的表数量）； 库序号 ＝ 取整（中间变量／每个库的表数量）； 表序号 ＝ 中间变量％每个库的表数量； 参考文章： 数据库分库分表策略的具体实现方案 数据库分库分表 数据库分库分表(sharding)系列(一) 拆分规则 多列联合索引，下列哪些WHERE语句能够命中索引？ 组合索引的生效原则是 从前往后依次使用生效，如果中间某个索引没有使用，那么断点前面的索引部分起作用，断点后面的索引没有起作用。 123456789101112131415161718(0) select * from mytable where a=3 and b=5 and c=4;abc三个索引都在where条件里面用到了，而且都发挥了作用(1) select * from mytable where c=4 and b=6 and a=3;这条语句列出来只想说明 mysql没有那么笨，where里面的条件顺序在查询之前会被mysql自动优化，效果跟上一句一样(2) select * from mytable where a=3 and c=7;a用到索引，b没有用，所以c是没有用到索引效果的(3) select * from mytable where a=3 and b&gt;7 and c=3;a用到了，b也用到了，c没有用到，这个地方b是范围值，也算断点，只不过自身用到了索引(4) select * from mytable where b=3 and c=4;因为a索引没有使用，所以这里 bc都没有用上索引效果(5) select * from mytable where a&gt;4 and b=7 and c=9;a用到了 b没有使用，c没有使用(6) select * from mytable where a=3 order by b;a用到了索引，b在结果排序中也用到了索引的效果，前面说了，a下面任意一段的b是排好序的(7) select * from mytable where a=3 order by c;a用到了索引，但是这个地方c没有发挥排序效果，因为中间断点了，使用 explain 可以看到 filesort(8) select * from mytable where b=3 order by a;b没有用到索引，排序中a也没有发挥索引效果 参考文章：mysql 多列索引的生效规则 详见哔哩哔哩面试记录。 谈谈SQL解释器的工作流程原理？ SQL解析执行的主要步骤： 对提交SQL的进行判断是否已解析和生产执行计划，对已生成执行计划的SQL直接执行返回结果 对SQL进行语法规范检查 对SQL进行语义检查，使用表、字段、函数、视图等数据库对象是否存在，以及是否拥有对应的操作权限 将SQL解析成语法树，按照不同的算法策略并做一些替换其中视图、表达式，合并查询等优化，去生成由关系代数算子组成的（投影 (π)，选择 (σ)，自然连接 (⋈)，聚集运算(G)等算子）不同逻辑执行计划 再由数据库的优化器去把上述不同的逻辑执行计划转化为物理执行计划（Table Scan, Index Range Scan, Filter, NestLoopJoin, Hash join等组成），并且选择出其中最优的物理执行计划，在Oracle中基于CBO的优化器会结合数据字典和统计信息计算出不同的执行计划的cost，最终选择cost最小的为最优的执行计划 将最优的执行计划载入Share pool中，并执行返回结果 参考文章：数据库SQL解析执行过程 谈谈你对跨域的理解？有哪几种方式？ 当两个地址在协议、主机或者端口不一致的时候，即判定为跨域。 解决方案： 利用JSONP实现跨域调用：通过script标签引入的js不收同源策略的限制，而XmlHttpRequest 对象受到同源策略的影响，可以加载跨域服务器上的脚本，用 JSONP 获取的不是 JSON 数据，而是可以直接运行的 JavaScript 语句。 利用CORS实现跨域调用：res.header(“Access-Control-Allow-Origin”, “*”); //设置请求来源不受限制。 服务端代理：跨域的HTTP请求是在服务器端进行的（服务器端没有同源策略的限制），客户端并没有产生跨域的Ajax请求。这个跨域方式下不需要和目标资源签订协议，带有侵略性。 参考文章：深入理解前端跨域问题的解决方案——前端面试 是否了解Redis的持久化到磁盘的方式？ redis提供两种方式进行持久化，一种是RDB持久化（原理是将Reids在内存中的数据库记录定时dump到磁盘上的RDB持久化），另外一种是AOF持久化（原理是将Reids的操作日志以追加的方式写入文件）。 参考文章：redis持久化方法对比分析 是否了解项目中RAL调用底层实现原理，自己如果设计如何实现？ 项目中的RAL是一个支持多种交互协议和打包格式的php扩展。 RAL的特点： 高性能、高可用底层网络交互支持 交互协议和数据打包协议的抽象——目前支持nshead和http两种交互协议，string/mcpack1/mcpack2/json/form五种数据打包协议 性能和简化的追求——C实现 资源定位，配置 RPC的全称为 Remote Procedure Call(远程过程调用)。 设计需要注意的地方：错误处理、超时重试，异步调用。 参考文章：RPC原理及RPC实例分析 完善项目中RAL调用的资料 对于现有系统如果从10万访问到百万访问，如何优化？ 在MVC框架中，当访问量上升所遇到的性能瓶颈： C层：控制层通过对链接，线程，内存等资源的开销进行优化处理，先调整C层逻辑，减少M层的工作量，然后再对M层进行优化，也许这样可以达到最优效果。 增加中间层缓存，优化处理逻辑，业务异步化 分布式部署实例或者负载均衡 冷热数据分离，读写分离 参考文章：面试题：系统访问量很大的情况下首先应该优化MVC哪层？ 有了解过nginx？ Nginx是一个web服务器和反向代理服务器，用于HTTP、HTTPS、SMTP、POP3和IMAP协议。 nginx 的 upstream目前支持 4 种方式的分配： 轮询（默认）:每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 weight：指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 ip_hash：每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 fair（第三方）：按后端服务器的响应时间来分配请求，响应时间短的优先分配。 url_hash（第三方） 二面 手写你熟悉的linux命令 略 如何显示文件的权限信息？权限信息每一位代表含义？如何用awk进行累加和计算？ ls -l xxx.xxx （xxx.xxx是文件名） -rwxr-xr-x 第一个字段包含该特殊文件的权限的符号表示。该字段中的首字符（-）指定该文件的类型，本例中它是一个常规文件。其它可能的首字符还有： “d”目录 “l”符号链接 “c”字符专门设备文件 “b”块专门设备文件 “p”先进先出 “s”套接字 该字段的其余部分由三个三元组字符组成。第一个三元字符组代表文件所有者的权限，第二个代表文件的组的权限，第三个代表所有其他用户的权限 r 表示允许读（查看文件中的数据），w 表示允许写（修改文件以及删除），x 表示允许“执行”（运行程序） 手写SQL：创建数据库；创建数据库表；创建用户；用户赋予该表读权限 grant 权限 on 数据库.数据表 to ‘用户’ @ ‘主机名’` revoke 权限 on 数据库.数据表 from ‘用户’@’主机名’; flush privileges; Redis重启会如何？ Redis 默认设置会开启 RDB 持久化。有另一种持久化 AOF，两种持久化可以同时开启。即使开启了持久化并不意味着你的数据在重启后一定和重启前一致。因为 Redis 并不可能每写一次内存便写一次硬盘（如果是这样，性能会很差），根据你的配置文件里面的规则，RDB 可以被触发写硬盘，还没有来得及写入（触发写入前）的那一部分会丢失。 可以设置Redis的主从同步。 参考文章：redis重启之后 里面存在的数据应该消失了吗 crontab原理？ crontab是一个命令,crond正是它的守护进程。 三面 个人的优缺点 对于游戏的认识与喜爱]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>neteasegame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[哔哩哔哩面试记录]]></title>
    <url>%2Fbilibili_interview.html</url>
    <content type="text"><![CDATA[哔哩哔哩面试记录 只有温故而知新，才能不断进步 一面 手撕代码：数组LIS长度？O(n^2)如何提升到O(logn)？ 思路：就是用dp[i]来记录A[i]为结尾的子序列中最大递增子序列的长度，对于每一个i，令j从1到i - 1遍历，当a[j] &lt; a[i]，比较当前dp[i]和每一个dp[j] + 1的大小，将最大值赋给dp[i]，然后比较dp[i]与max，如果大于max，就给max赋值。 12345678910111213141516171819202122232425262728293031323334353637383940414243public class LIS &#123; public static void main(String[] args) &#123; int[] nums = &#123;5, 3, 4, 8, 6, 7, 2&#125;; int res = calLIS(nums); System.out.println(res); &#125; // 从小到大来去寻找关键的“状态”和“状态转移方程” // 求最长非降子序列的长度，子状态是可以理解为是所求字符串的子字符串的最长非降子序列的长度，也就是随着字符串长度的增加 // 定义d(i)，表示前i个数中以A[i]结尾的最长非降子序列的长度。其中j&lt;i,A[j]&lt;=A[i] // 前1个数的LIS长度d(1)=1(序列：5) // 前2个数的LIS长度d(2)=1(序列：3；3前面没有比3小的) // 前3个数的LIS长度d(3)=2(序列：3，4；4前面有个比它小的3，所以d(3)=d(2)+1) // 前4个数的LIS长度d(4)=3(序列：3，4，8；8前面比它小的有3个数，所以 d(4)=max&#123;d(1),d(2),d(3)&#125;+1=3) // 状态转移方程：d(i) = max&#123;1, d(j)+1&#125;,其中j&lt;i,A[j]&lt;=A[i] // 想要求d(i)，就把i前面的各个子序列中， 最后一个数不大于A[i]的序列长度加1，然后取出最大的长度即为d(i)。 当然了，有可能i前面的各个子序列中最后一个数都大于A[i]，那么d(i)=1， 即它自身成为一个长度为1的子序列。 private static int calLIS(int[] nums) &#123; int maxLen = 0; int[] dp = new int[nums.length]; //初始化 for (int i = 0; i &lt; dp.length; i++) &#123; dp[i] = 1; &#125; for (int i = 0; i &lt; dp.length; i++) &#123; for (int j = 0; j &lt; i; ++j) &#123; //划重点 //LIS并没有说连续，所以可以遍历查找元素小于当前元素，然后将遍历到的比当前元素小的元素结尾的长度+1即可，这里也就明白了为什么要把状态d(i)定义为这个样子的原因。 //逐次覆盖，遍历所有之前的d(i)，后覆盖的长度因为多了元素，是比之前的d(i)要长，满足最长非降子序列的长度定义。 if (nums[j] &lt;= nums[i] &amp;&amp; dp[j] + 1 &gt; dp[i]) &#123; dp[i] = dp[j] + 1; &#125; &#125; //找dp[]中的最大值 if (dp[i] &gt; maxLen) maxLen = dp[i]; &#125; return maxLen; &#125;&#125; 这样的时间复杂度是O(n^2)，那么如何提高时间复杂度到O(nlogn)呢？ - 将问题转换成求递增排序的数组与原数组的最长公共子序列。 - 首先对数组排序，将排序后的结果存储在辅助数组中。排序时间复杂度O(NlogN)，排序后的数组与原数组组成了LCS（N,N）问题。解决LCS问题的时间复杂度为O(N^2)，故整个算法的时间复杂度为O(N^2)，空间复杂度为O(N)。 - 参考文章：[Java-LCS最长公共子序列（动态规划实现）](http://blog.csdn.net/qq_30507287/article/details/52830324) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.Arrays;import java.util.Scanner;import java.util.Stack;/** * Created by ml on 2017/10/16. */public class LIS2LCS &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int[] nums = &#123;5, 3, 4, 8, 6, 7, 2&#125;; int[] newnums = Arrays.copyOf(nums, nums.length); Arrays.sort(newnums); int res = getLength(nums, newnums); System.out.println(res); &#125; private static int getLength(int[] nums, int[] newnums) &#123; int len1 = nums.length; int len2 = newnums.length; int[][] dp = new int[len1 + 1][len2 + 1]; for (int i = 0; i &lt; dp.length; i++) &#123; dp[i][0] = 0; &#125; for (int j = 0; j &lt; dp[0].length; j++) &#123; dp[0][j] = 0; &#125; for (int m = 1; m &lt; dp.length; m++) &#123;//利用动态规划将数组赋满值 for (int n = 1; n &lt; dp[m].length; n++) &#123; if (nums[m - 1] == newnums[n - 1]) &#123; dp[m][n] = dp[m - 1][n - 1] + 1;//动态规划公式一 &#125; else &#123; dp[m][n] = Math.max(dp[m - 1][n], dp[m][n - 1]);//动态规划公式二 &#125; &#125; &#125; Stack stack = new Stack(); int i = len1 - 1; int j = len2 - 1; while ((i &gt;= 0) &amp;&amp; (j &gt;= 0)) &#123; if (nums[i] == newnums[j]) &#123;//字符串从后开始遍历，如若相等，则存入栈中 stack.push(nums[i]); i--; j--; &#125; else &#123; if (dp[i + 1][j] &gt; dp[i][j + 1]) &#123;//如果字符串的字符不同，则在数组中找相同的字符，注意：数组的行列要比字符串中字符的个数大1，因此i和j要各加1 j--; &#125; else &#123; i--; &#125; &#125; &#125; return stack.size(); &#125;&#125; - 非DP解法——用二分查找替换来加快 - 参考文章：[最长递增子序列 LIS 时间复杂度O(nlogn)的Java实现](http://blog.csdn.net/iniegang/article/details/47681191) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com;import java.util.Scanner;/** * Created by ml on 2017/10/16. */public class LISUpdate &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int[] nums = &#123;5, 3, 4, 8, 6, 7, 2&#125;; int res = getLIS(nums); System.out.println(res); &#125; //获取最长递增子序列的长度 private static int getLIS(int[] arrayIn) &#123; int position; int len = 1; int[] arrayOut = new int[arrayIn.length + 1];//arrayOut[0]没有存放数据 arrayOut[1] = arrayIn[0]; //初始化，长度为1的LIS末尾为arrayIn[0] for (int i = 1; i &lt; arrayIn.length; i++) &#123; position = BinarySearchPosition(arrayOut, 1, len, arrayIn[i]); arrayOut[position] = arrayIn[i]; if (len &lt; position) &#123; len = position; &#125; &#125; return len; &#125; //二分查找要替换的位置 private static int BinarySearchPosition(int arrayOut[], int left, int right, int key) &#123; int mid; if (arrayOut[right] &lt; key) &#123; return right + 1; &#125; else &#123; while (left &lt; right) &#123; mid = (left + right) / 2; if (arrayOut[mid] &lt; key) &#123; left = mid + 1; &#125; else &#123; right = mid; &#125; &#125; return left; &#125; &#125;&#125; 手撕代码：生产者消费者，如何线程安全 重点在于生产者与消费者的通信过程，以及队列的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127package ProducterAndConsumer.Version3;import java.util.List;/** * 消费者 * @author ctk * */public class Consumer implements Runnable&#123; private List&lt;PCData&gt; queue; public Consumer(List&lt;PCData&gt; queue)&#123; this.queue = queue; &#125; @Override public void run() &#123; try &#123; while (true) &#123; if (Thread.currentThread().isInterrupted()) break; PCData data = null; Main.lock.lock(); if (queue.size() == 0)&#123; Main.full.signalAll(); Main.empty.await(); &#125; Thread.sleep(1000); data = queue.remove(0); Main.lock.unlock(); System.out.println(&quot;消费者ID:&quot;+Thread.currentThread().getId()+&quot; 消费了:&quot;+data.getData()+&quot; result:&quot;+(data.getData()*data.getData())); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;package ProducterAndConsumer.Version3;import java.util.List;import java.util.Random;/** * 生产者 * @author ctk * */public class Producter implements Runnable&#123; private List&lt;PCData&gt; queue; private int len; public Producter(List&lt;PCData&gt; queue,int len)&#123; this.queue = queue; this.len = len; &#125; @Override public void run() &#123; try&#123; while(true)&#123; if(Thread.currentThread().isInterrupted()) break; Random r = new Random(); PCData data = new PCData(); data.setData(r.nextInt(500)); Main.lock.lock(); if(queue.size() &gt;= len) &#123; Main.empty.signalAll(); Main.full.await(); &#125; Thread.sleep(1000); queue.add(data); Main.lock.unlock(); System.out.println(&quot;生产者ID:&quot;+Thread.currentThread().getId()+&quot; 生产了:&quot;+data.getData()); &#125; &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;package ProducterAndConsumer.Version3;import java.util.ArrayList;import java.util.List;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;public class Main &#123; public static ReentrantLock lock = new ReentrantLock(); public static Condition empty = lock.newCondition(); public static Condition full = lock.newCondition(); public static void main(String[] args) &#123; List&lt;PCData&gt; queue = new ArrayList&lt;PCData&gt;(); int length = 10; Producter p1 = new Producter(queue,length); Producter p2 = new Producter(queue,length); Producter p3 = new Producter(queue,length); Consumer c1 = new Consumer(queue); Consumer c2 = new Consumer(queue); Consumer c3 = new Consumer(queue); ExecutorService service = Executors.newCachedThreadPool(); service.execute(p1); service.execute(p2); service.execute(p3); service.execute(c1); service.execute(c2); service.execute(c3); &#125;&#125;package ProducterAndConsumer.Version3;public class PCData &#123; private int data; public int getData() &#123; return data; &#125; public void setData(int data) &#123; this.data = data; &#125;&#125; 参考文章： 生产者消费者模式-Java实现 MySQL多列索引 理解的最好方法就是自己动手创建表，创建索引，创建explain各种情况的SELECT语句 组合索引的生效原则是：从前往后依次使用生效，如果中间某个索引没有使用，那么断点前面的索引部分起作用，断点后面的索引没有起作用。 索引是key index (a,b,c). 可以支持a | a,b| a,b,c 3种组合进行查找。其中，每一种组合的顺序是可以改变的。 参考文章： MySql中的多列索引 干货：你不知道的mysql多列索引的建立和优化 EXPLAIN列的解释： table：显示这一行的数据是关于哪张表的。 type：这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、indexhe和ALL possible_keys：显示可能应用在这张表中的索引。如果为空，没有可能的索引。可以为相关的域从WHERE语句中选择一个合适的语句 key： 实际使用的索引。如果为NULL，则没有使用索引。很少的情况下，MYSQL会选择优化不足的索引。这种情况下，可以在SELECT语句中使用USE INDEX（indexname）来强制使用一个索引或者用IGNORE INDEX（indexname）来强制MYSQL忽略索引 key_len：使用的索引的长度。在不损失精确性的情况下，长度越短越好 ref：显示索引的哪一列被使用了，如果可能的话，是一个常数 rows：MYSQL认为必须检查的用来返回请求数据的行数 Extra：关于MYSQL如何解析查询的额外信息。将在表4.3中讨论，但这里可以看到的坏的例子是Using temporary和Using filesort，意思MYSQL根本不能使用索引，结果是检索会很慢 extra列返回的描述的意义 Distinct:一旦MYSQL找到了与行相联合匹配的行，就不再搜索了 Not exists: MYSQL优化了LEFT JOIN，一旦它找到了匹配LEFT JOIN标准的行，就不再搜索了 Range checked for each Record（index map:#）:没有找到理想的索引，因此对于从前面表中来的每一个行组合，MYSQL检查使用哪个索引，并用它来从表中返回行。这是使用索引的最慢的连接之一 Using filesort: 看到这个的时候，查询就需要优化了。MYSQL需要进行额外的步骤来发现如何对返回的行排序。它根据连接类型以及存储排序键值和匹配条件的全部行的行指针来排序全部行 Using index: 列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候 Using temporary 看到这个的时候，查询需要优化了。这里，MYSQL需要创建一个临时表来存储结果，这通常发生在对不同的列集进行ORDER BY上，而不是GROUP BY上 Where used 使用了WHERE从句来限制哪些行将与下一张表匹配或者是返回给用户。如果不想返回表中的全部行，并且连接类型ALL或index，这就会发生，或者是查询有问题不同连接类型的解释（按照效率高低的顺序排序） system 表只有一行：system表。这是const连接类型的特殊情况 const:表中的一个记录的最大值能够匹配这个查询（索引可以是主键或惟一索引）。因为只有一行，这个值实际就是常数，因为MYSQL先读这个值然后把它当做常数来对待 eq_ref:在连接中，MYSQL在查询时，从前面的表中，对每一个记录的联合都从表中读取一个记录，它在查询使用了索引为主键或惟一键的全部时使用 ref:这个连接类型只有在查询使用了不是惟一或主键的键或者是这些类型的部分（比如，利用最左边前缀）时发生。对于之前的表的每一个行联合，全部记录都将从表中读出。这个类型严重依赖于根据索引匹配的记录多少—越少越好 range:这个连接类型使用索引返回一个范围中的行，比如使用&gt;或&lt;查找东西时发生的情况 index: 这个连接类型对前面的表中的每一个记录联合进行完全扫描（比ALL更好，因为索引一般小于表数据）ALL:这个连接类型对于前面的每一个记录联合进行完全扫描，这一般比较糟糕，应该尽量避免 using index 是覆盖索引，说明你的查询语句可以只通过查询索引里的信息就能得到结果。using where 则表示需要查询磁盘里存储的数据，速度会慢很多 参考文章：详解MySQL中EXPLAIN解释命令 多线程 常见参数的介绍；线程池的拒绝策略： ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy：也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 二面 Spring的角色的理解？ 在SSH框假中Spring充当了容器的角色，生成对象并对其管理。IOC可以对对象进行灵活的配置管理，AOP更好的支持安全、事务和日志等程序功能实现，降低系统耦合性，提高代码可重用性。并且支持多种ORM，提供高效的数据支持。 手撕代码：如何在dao层所有方法中，监听所有方法的执行所用时间？ AOP实现 配置文件 实现拦截类 使用注解、切点（@Pointcut(“execution( com.tmg.perfomance.service.impl..*(..))”)、@Around(“serviceMethod()”)） 参考文章： AOP监控方法的运行时间 spring aop 实现方法执行时间监控 原生实现：ThreadLocal PHP如何实现OOP? 魔术方法？相关讨论：对纯面向对象的PHP程序有何看法？ Java多线程的通信方式？ 同步:这里讲的同步是指多个线程通过synchronized关键字这种方式来实现线程间的通信。 while轮询的方式 wait/notify机制 管道通信就是使用java.io.PipedInputStream 和 java.io.PipedOutputStream进行通信 参考文章：JAVA多线程之线程间的通信方式 相关有意思的练习题： 参考文章：JAVA线程间通信的几种方式 HR 你觉得自己从0到1负责一个独立模块需要多长时间？ 你觉得脉脉中有许多程序员吐槽负能量这种现象你如何看待？]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>bilibili</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前沿技术翻译系列——如何区分人工智能，机器学习和深度学习？]]></title>
    <url>%2FWhat%E2%80%99s-the-Difference-Between-Artificial-Intelligence-Machine-Learning-and-Deep-Learning.html</url>
    <content type="text"><![CDATA[前沿技术翻译文章 原文地址：https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/ 参考译文：一张图看懂AI、机器学习和深度学习的区别 这不仅是自己学习前沿技术的记录，也是提升英语阅读的记录，同时关注对比百度翻译、搜狗翻译、谷歌翻译三者的区别，刚好和标题中的三者的区别有异曲同工之妙。 What’s the Difference Between Artificial Intelligence, Machine Learning, and Deep Learning?如何区分人工智能，机器学习和深度学习？This is the first of a multi-part series explaining the fundamentals of deep learning by long-time tech journalist Michael Copeland. 这是描述深度学习的基础系列的第一部分。本系列由资深科技记者迈克尔·科普兰（Michael Copeland）编写。 Artificial intelligence is the future. Artificial intelligence is science fiction. Artificial intelligence is already part of our everyday lives. All those statements are true, it just depends on what flavor of AI you are referring to. 人工智能是未来、人工智能是科幻小说、人工智能已经成为我们日常生活的一部分。所有论断都是正确的，所有的这些都已成现实，只是要看你所谈到所认为的AI到底是什么。 For example, when Google DeepMind’s AlphaGo program defeated South Korean Master Lee Se-dol in the board game Go earlier this year, the terms AI, machine learning, and deep learning were used in the media to describe how DeepMind won. And all three are part of the reason why AlphaGo trounced Lee Se-Dol. But they are not the same things. 例如，今年早些时候，Google DeepMind开发的AlphaGo程序在围棋比赛中击败韩国围棋大师李世石。媒体在报导AlphaGo获得胜利中使用人工智能、机器学习和深度学习这些术语。AlphaGo击败李世石，这些技术都立下汗马功劳，但是它们并不是同一回事。 The easiest way to think of their relationship is to visualize them as concentric circles with AI — the idea that came first — the largest, then machine learning — which blossomed later, and finally deep learning — which is driving today’s AI explosion — fitting inside both. 同心圆是理解人工智能、机器学习和深度学习三者的关系的最简单直观的表达方式。人工智能（AI）的概念是最早出现的，同样是范围最大的；后来是机器学习的爆发式发展；最后是深度学习，正在推动着今天人工智能（AI）的爆发，两者都适合。 From Bust to Boom从衰败到繁荣AI has been part of our imaginations and simmering in research labs since a handful of computer scientists rallied around the term at the Dartmouth Conferences in 1956 and birthed the field of AI. In the decades since, AI has alternately been heralded as the key to our civilization’s brightest future, and tossed on technology’s trash heap as a harebrained notion of over-reaching propellerheads. Frankly, until 2012, it was a bit of both. 人工智能（AI）一直是我们想象力以及实验室研究的一部分，人工智能（AI）概念首先在1956年的达特茅斯会议由几位计算机科学家提出，人工智能（AI）从此诞生。在过去的几十年里，人们对人工智能（AI）的看法不断改变，有时认为人工智能（AI）是我们未来最光明文明的关键。然而有时它也被认为只是一个轻率的概念，野心过大，注定要失败，被扔进技术的垃圾堆中。坦率地说，直到2012年，人工智能（AI）这两种观念仍然同时存在。 Over the past few years AI has exploded, and especially since 2015. Much of that has to do with the wide availability of GPUs that make parallel processing ever faster, cheaper, and more powerful. It also has to do with the simultaneous one-two punch of practically infinite storage and a flood of data of every stripe (that whole Big Data movement) – images, text, transactions, mapping data, you name it. 在过去的几年里，人工智能已经发生了爆炸式增长，尤其是在2015年之后更是迅猛发展。其中很大一部分原因归功于GPU的广泛普及使用，它使并行处理变得更快、更便宜、更强大。它同时还与实际存储容量无限扩展的大环境下，各种类型的大数据不断产生有关，比如图像、文本、交易、地图数据等等你可以想到的所有数据类型。 Let’s walk through how computer scientists have moved from something of a bust — until 2012 — to a boom that has unleashed applications used by hundreds of millions of people every day. 让我们来看看计算机科学家们是如何将人工智能（AI）从萧条（直到2012年）变为繁荣，这一热潮使得每天有成千上万的人使用包含人工智能（AI）的应用。 Artificial Intelligence — Human Intelligence Exhibited by Machines人工智能——让机器展示出人类智能 King me: computer programs that played checkers were among the earliest examples of artificial intelligence, stirring an early wave of excitement in the 1950s. 介绍：玩跳棋的电脑程序是最早的人工智能的例子之一，在20世纪50年代激起了早期的人工智能兴奋浪潮。 Back in that summer of ’56 conference the dream of those AI pioneers was to construct complex machines — enabled by emerging computers — that possessed the same characteristics of human intelligence. This is the concept we think of as “General AI” — fabulous machines that have all our senses (maybe even more), all our reason, and think just like we do. You’ve seen these machines endlessly in movies as friend — C-3PO — and foe — The Terminator. General AI machines have remained in the movies and science fiction novels for good reason; we can’t pull it off, at least not yet. 回到1956年的达特茅斯会议，这些AI先驱的梦想就是构建具有与人类智慧相同特征的由当时新兴计算机构成的复杂机器。这个概念就是我们所说的“强人工智能（General AI）”，这是一个神话般的机器，具有我们所有的感觉（甚至更多），我们所有的理智，像我们一样想。你已经在电影中看到了这些机器，例如C-3PO （礼仪机器人）、敌人终结者等待。强人工智能（General AI）仍然存在于电影和科幻小说中，这是有原因的。我们不能实现强人工智能（General AI），至少现在还不能。 What we can do falls into the concept of “Narrow AI.” Technologies that are able to perform specific tasks as well as, or better than, we humans can. Examples of narrow AI are things such as image classification on a service like Pinterest and face recognition on Facebook. 目前我们能做的是有关“弱人工智能（Narrow AI）”的概念。这是一种能够执行特定任务的技术，或者比我们人类能做的更好的技术。例如，Pinterest利用AI进行图片分类，Facebook使用AI对脸部识别，这些都是“弱人工智能（Narrow AI）”的应用例子。 Those are examples of Narrow AI in practice. These technologies exhibit some facets of human intelligence. But how? Where does that intelligence come from? That get us to the next circle, Machine Learning. 这些都是对“弱人工智能（Narrow AI）”的实际实践。这些技术表现出人类智力的某些方面的特点。但是这些是如何实现的？这些智力来自哪里？带着问题我们将会来到下一个圆圈——机器学习。 Machine Learning — An Approach to Achieve Artificial Intelligence机器学习——一种实现人工智能的方法 Spam free diet: machine learning, a subset of AI (Artificial Intelligence) helps keep your inbox (relatively) free of spam. 垃圾邮件免费食谱:机器学习，人工智能的一个子集(人工智能)帮助你的收件箱(相对)不受垃圾邮件的影响。 Machine Learning at its most basic is the practice of using algorithms to parse data, learn from it, and then make a determination or prediction about something in the world. So rather than hand-coding software routines with a specific set of instructions to accomplish a particular task, the machine is “trained” using large amounts of data and algorithms that give it the ability to learn how to perform the task. 机器学习最基本的方法是使用算法来解析处理数据，从中学习，然后对世界中的某些事物做出决定或预测。因此，与其用特定的指令集编写软件程序来完成特定的任务，还不如使用大量的数据和算法“训练”机器，让它能够学习如何执行任务。 Machine learning came directly from minds of the early AI crowd, and the algorithmic approaches over the years included decision tree learning, inductive logic programming. clustering, reinforcement learning, and Bayesian networks among others. As we know, none achieved the ultimate goal of General AI, and even Narrow AI was mostly out of reach with early machine learning approaches. 机器学习的概念直接来源于早期的早期人工智能研究群体的思想。这些年来，算法的方法包括决策树学习、归纳逻辑编程、聚类、强化学习、贝叶斯网络等。 我们知道，没有一个人实现了“强人工智能（General AI）”的最终目标，甚至是通过早期的机器学习方法，甚至是“弱人工智能（Narrow AI）”都无法实现。 As it turned out, one of the very best application areas for machine learning for many years was computer vision, though it still required a great deal of hand-coding to get the job done. People would go in and write hand-coded classifiers like edge detection filters so the program could identify where an object started and stopped; shape detection to determine if it had eight sides; a classifier to recognize the letters “S-T-O-P.” From all those hand-coded classifiers they would develop algorithms to make sense of the image and “learn” to determine whether it was a stop sign. 事实证明，多年来机器学习的最佳应用领域之一是计算机视觉领域。要实现计算机视觉，它仍然需要大量的手工编码来完成工作。研究人员会去写手动编写分类器，比如边缘检测过滤器，这样程序就能识别出物体的起点和停止位置；形状检测确定是否有八面；识别字母“S-T-O-P”的分类器。从所有这些手工编写的分类器中，他们将开发算法来理解图像和学习识别图像，确定它是否是一个停止符号。 Good, but not mind-bendingly great. Especially on a foggy day when the sign isn’t perfectly visible, or a tree obscures part of it. There’s a reason computer vision and image detection didn’t come close to rivaling humans until very recently, it was too brittle and too prone to error. 这种办法可以使用，但并不是令人满意。尤其是在大雾天，当这个标志不是完全可见的时候，或者一棵树遮住了标志的某一部分，它的识别能力就会下降。计算机视觉和图像检测的能力直到最近才接近人类的原因是它太脆弱，太容易出错。 Time, and the right learning algorithms made all the difference. 时间，以及正确的学习算法让一切都变得不同。 Deep Learning — A Technique for Implementing Machine Learning深度学习——一种实现机器学习的技术 Herding cats: Picking images of cats out of YouTube videos was one of the first breakthrough demonstrations of deep learning. 牧羊猫：从YouTube视频中挑选猫的图像是深入学习的第一个突破性示范之一。 Another algorithmic approach from the early machine-learning crowd, Artificial Neural Networks, came and mostly went over the decades. Neural Networks are inspired by our understanding of the biology of our brains – all those interconnections between the neurons. But, unlike a biological brain where any neuron can connect to any other neuron within a certain physical distance, these artificial neural networks have discrete layers, connections, and directions of data propagation. “人工神经网络（Artificial Neural Networks）”是另外一种算法方法，它也是早期机器学习专家提出的，存在已经几十年了。神经网络的灵感来自于我们对大脑生物学构造的理解——所有这些神经元之间的相互联系。但是，不同之处在于任何神经元可以在一定的物理距离内连接到其他神经元的生物大脑，而人工神经网络具有离散的层、连接和数据传播方向。 You might, for example, take an image, chop it up into a bunch of tiles that are inputted into the first layer of the neural network. In the first layer individual neurons, then passes the data to a second layer. The second layer of neurons does its task, and so on, until the final layer and the final output is produced. 举个例子，你可以拿出一张图片，把它分割成一堆的小的图片碎片。它们被输入到神经网络的第一层。在第一层中单个独立神经元会将数据传递到第二层。第二层神经元同样会执行其任务，依此类推，直到产生最后一层，生成最终输出的结果。 Each neuron assigns a weighting to its input — how correct or incorrect it is relative to the task being performed. The final output is then determined by the total of those weightings. So think of our stop sign example. Attributes of a stop sign image are chopped up and “examined” by the neurons — its octogonal shape, its fire-engine red color, its distinctive letters, its traffic-sign size, and its motion or lack thereof. The neural network’s task is to conclude whether this is a stop sign or not. It comes up with a “probability vector,” really a highly educated guess, based on the weighting. In our example the system might be 86% confident the image is a stop sign, 7% confident it’s a speed limit sign, and 5% it’s a kite stuck in a tree ,and so on — and the network architecture then tells the neural network whether it is right or not. 每个神经元都将一个权重分配给它的输入，确定它与所执行任务的关系，对应正确与不正确的程度。最后的输出结果由这些权重的总和决定。以之前我们的停止标志为例子说明，我们将停止符号图像的属性切割，通过神经元来进行“检测”：它的八角形状，它的消防车类型的红色，它独特的字母，它的交通标志大小，以及它的手势。神经网络的任务是判断这个图像是否是一个停止标志。它提出了一个“概率向量”，一个基于权重的据理推测。在我们的例子中，神经网络系统可能有86%的概率确定图像是一个停止标志，7%的概率确认它是一个限速标志，5%的概率确认是一个被困在树上的风筝，等等。网络架构然后告诉神经网络结果是否正确。 Even this example is getting ahead of itself, because until recently neural networks were all but shunned by the AI research community. They had been around since the earliest days of AI, and had produced very little in the way of “intelligence.” The problem was even the most basic neural networks were very computationally intensive, it just wasn’t a practical approach. Still, a small heretical research group led by Geoffrey Hinton at the University of Toronto kept at it, finally parallelizing the algorithms for supercomputers to run and proving the concept, but it wasn’t until GPUs were deployed in the effort that the promise was realized. 即使是这样的例子也是超前的。因为直到最近，神经网络仍然被人工智能研究界所忽略。神经网络在人工智能发展的早期就出现了，但其几乎没有产生什么而“智力”。问题在于即使最基本的神经网络都是计算密集型的，因此这并不是一种实用的方法。尽管如此，由多伦多大学的Geoffrey Hinton领导的异端课题小组仍然坚持研究神经网络问题，最后在超级计算机中并行化算法来运行和证明这个概念，但一直到直到GPU的广泛部署应用后，这个承诺才被实现。 If we go back again to our stop sign example, chances are very good that as the network is getting tuned or “trained” it’s coming up with wrong answers — a lot. What it needs is training. It needs to see hundreds of thousands, even millions of images, until the weightings of the neuron inputs are tuned so precisely that it gets the answer right practically every time — fog or no fog, sun or rain. It’s at that point that the neural network has taught itself what a stop sign looks like; or your mother’s face in the case of Facebook; or a cat, which is what Andrew Ng did in 2012 at Google. 如果我们再回到识别停止标志的例子。随着网络的调整或“训练”，结果就会变得更好。它会产生很多的错误答案，它需要的是更多的训练。研究人员需要收集成百上千，甚至上百万的图像，直到神经元输入的权重调整到精确，使得它几乎每次都能得到正确的答案——不管有雾或没有雾、是否有太阳或雨。此时神经网络已经学会判断停止符号的形式。神经网络还可以识别Facebook上你母亲的脸庞，或者识别出一只猫——这就是吴恩达（Andrew Ng）2012年在谷歌所做的事情。 Ng’s breakthrough was to take these neural networks, and essentially make them huge, increase the layers and the neurons, and then run massive amounts of data through the system to train it. In Ng’s case it was images from 10 million YouTube videos. Ng put the “deep” in deep learning, which describes all the layers in these neural networks. 吴恩达的突破之处在于：利用这些神经网络，本质上使它们变得巨大，增加了层和神经元，然后通过系统运行大量的数据来训练它。在吴恩达（Andrew Ng）的案例中，它是通过分析来自1000万段YouTube视频中的图片。吴恩达（Andrew Ng）真正实现了深度学习的“深度”，使得其能够描述神经网络中的所有的层次信息。 Today, image recognition by machines trained via deep learning in some scenarios is better than humans, and that ranges from cats to identifying indicators for cancer in blood and tumors in MRI scans. Google’s AlphaGo learned the game, and trained for its Go match — it tuned its neural network — by playing against itself over and over and over. 今天，在某些应用场景中，通过深入学习训练的机器获得的图像识别比人类识别的效果更好，例如识别猫、血液识别癌症、在MRI扫描中识别肿瘤等。谷歌的AlphaGo学习围棋，并通过围棋比赛来训练自己，同时反复与自己对抗来调整自己的神经网络。 Thanks to Deep Learning, AI Has a Bright Future感谢深度学习，人工智能有一个光明的未来Deep Learning has enabled many practical applications of Machine Learning and by extension the overall field of AI. Deep Learning breaks down tasks in ways that makes all kinds of machine assists seem possible, even likely. Driverless cars, better preventive healthcare, even better movie recommendations, are all here today or on the horizon. AI is the present and the future. With Deep Learning’s help, AI may even get to that science fiction state we’ve so long imagined. You have a C-3PO, I’ll take it. You can keep your Terminator. 深入学习已经使机器学习有许多实际的应用同时扩展了人工智能（AI）的整体领域。深度学习以各种方式分解任务，这使得所有的机器辅助解决问题成为可能。无人驾驶汽车，更好的预防保健，甚至更好的电影推荐，都在今天或即将到来。AI既是现在，也是未来。在深度学习的帮助下，人工智能甚至可能达到我们长久以来所设想的科幻小说描述的水平。也许未来你会拥有自己的C-3PO，我将会拿走它，；你也可以保有自己的终结者。 To learn more about where deep learning is going next, listen to our in-depth interview with AI pioneer Andrew Ng on the NVIDIA AI Podcast.想要更加深入了解深度学习的更多信息，在英伟达（NVIDIA）AI播客听我们与人工智能先驱吴恩达（Andrew Ng）的深入访谈节目。写在最后深度学习会在未来的各个新兴领域以及传统行业广泛实践应用。社会不断发展以及效率不断提升离不开对于各行各业的数据分析数据中蕴含的价值。 All in AI! Learning English way is bumpy, let us join hands to advance together! 自动翻译也是深度学习的应用领域之一。 PS：自己对于在线翻译自动翻译的排名：Google &gt;= 有道 &gt;&gt; 百度]]></content>
      <categories>
        <category>技术文章翻译</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>technical article</tag>
        <tag>translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[腾讯一面]]></title>
    <url>%2Ftencent_interview.html</url>
    <content type="text"><![CDATA[腾讯一面 意外之喜，让自己有机会去感受鹅厂的面试文化，让自己受益匪浅，非常感谢！ 1. 谈一个你自己觉得最满意的项目 实习时候完成的日志分析的项目 2. 遇到哪些难题？ 数据倾斜 3. 你认为这样的方式对于准确性来说是否有影响？日志统计的准确性是如何保证的？ 自己当时的理解是针对于项目需求来说统计频次，采用hash均分再统计，对于频次来说应该是准确的。 关于日志的准确性，自己是根据数据统计指标的定义公式进行统计计算，应该问题不大。 此外公司级别的日志分析平台，也可以作为测试数据的评判标准。 一篇关于百度Hadoop日志分析的文章 借用文章里面的话，自己的日志分析就是“洪荒时代的产物”，不完善的地方太多。 这篇文章真的很赞，自己用到的数据源都是这个大佬的作品，敬佩。 4. 日志统计指标还有其他一些什么指标？ （PC端以及移动端）PV和UV、登录用户占比、非登录用户占比、平均响应时间、历史Query占比、推荐Query占比、Query平均长度、Sug平均前缀长度、用户平均输入长度、输入长度占比、sug影响面等等。 5. 日志源是后台打印日志还是WebService打印日志？ 公司统一生成的日志，规范化日志格式。 日志来源：1. webservice调用打印日志；2. nginx的access.log日志 6. 你这样处理日志流程是否有了解过有相似功能的开源框架呢？ 没有了解ELK，自己的探索精神还需要加强。 ELK是三个开源工具组成，简单解释如下： Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 Logstash是一个完全开源的工具，它可以对你的日志进行收集、过滤，并将其存储供以后使用（如，搜索）。 Kibana 也是一个开源和免费的工具，它可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 参考文章：ELK部署参考文档 7. 报表邮件需要绘制图形吗？ 自己用到的是xlrd和xlwt这两个python第三方库，用于绘制邮件正文数据表格。 smtplib模块负责连接服务器和发送邮件 MIMEAudio：定义邮件的音频数据 MIMEImage：定义邮件的图片数据 MIMEText：定义邮件的文字数据 MIMEMultipart：负责将文字图片音频组装在一起和添加附件 python邮件中绘制Html表格：python构造html1234567891011121314151617181920212223242526272829303132333435html = &quot;&quot;&quot;\&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt;&lt;title&gt;萌店AB环境&lt;/title&gt;&lt;body&gt;&lt;div id=&quot;container&quot;&gt;&lt;p&gt;&lt;strong&gt;萌店sla状态统计&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;采集时间: &quot;&quot;&quot; + timezone + &quot;&quot;&quot;&lt;/p&gt;&lt;div id=&quot;content&quot;&gt; &lt;table width=&quot;500&quot; border=&quot;2&quot; bordercolor=&quot;red&quot; cellspacing=&quot;2&quot;&gt;&lt;tr&gt; &lt;td&gt;&lt;strong&gt;站点&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;总访问量&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;正常数&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;正常百分比&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;异常数&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;异常百分比&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt; &lt;td&gt;&lt;a href=&quot;http://log.xxx.com/#/dashboard/file/node.json&quot;&gt;node&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&quot;&quot;&quot; + a[4] + &quot;&quot;&quot;&lt;/td&gt; &lt;td&gt;&quot;&quot;&quot; + a[0] + &quot;&quot;&quot;&lt;/td&gt; &lt;td&gt;&quot;&quot;&quot; + a[1] + &quot;&quot;&quot;&lt;/td&gt; &lt;td&gt;&quot;&quot;&quot; + a[2] + &quot;&quot;&quot;&lt;/td&gt; &lt;td&gt;&quot;&quot;&quot; + a[3] + &quot;&quot;&quot;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;点击站点名可查看详细表图&lt;/strong&gt; &lt;/p&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; &quot;&quot;&quot; 参考文章：用python发送表格数据到邮箱 python邮件中绘制excel表格：pyhton构建附件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#!/usr/bin/env python# -*- coding:utf-8 -*-import smtplibfrom email.mime.multipart import MIMEMultipartfrom email.mime.text import MIMETextfrom email.mime.application import MIMEApplicationclass Mailer(object): def __init__(self,maillist,mailtitle,mailcontent): self.mail_list = maillist self.mail_title = mailtitle self.mail_content = mailcontent self.mail_host = &quot;smtp.163.com&quot; self.mail_user = &quot;your email name&quot; self.mail_pass = &quot;your email password&quot; self.mail_postfix = &quot;163.com&quot; def sendMail(self): me = self.mail_user + &quot;&lt;&quot; + self.mail_user + &quot;@&quot; + self.mail_postfix + &quot;&gt;&quot; msg = MIMEMultipart() msg[&apos;Subject&apos;] = &apos;Python mail Test&apos; msg[&apos;From&apos;] = me msg[&apos;To&apos;] = &quot;;&quot;.join(self.mail_list) #puretext = MIMEText(&apos;&lt;h1&gt;你好，&lt;br/&gt;&apos;+self.mail_content+&apos;&lt;/h1&gt;&apos;,&apos;html&apos;,&apos;utf-8&apos;) puretext = MIMEText(&apos;纯文本内容&apos;+self.mail_content) msg.attach(puretext) # jpg类型的附件 jpgpart = MIMEApplication(open(&apos;/home/mypan/1949777163775279642.jpg&apos;, &apos;rb&apos;).read()) jpgpart.add_header(&apos;Content-Disposition&apos;, &apos;attachment&apos;, filename=&apos;beauty.jpg&apos;) msg.attach(jpgpart) # 首先是xlsx类型的附件 #xlsxpart = MIMEApplication(open(&apos;test.xlsx&apos;, &apos;rb&apos;).read()) #xlsxpart.add_header(&apos;Content-Disposition&apos;, &apos;attachment&apos;, filename=&apos;test.xlsx&apos;) #msg.attach(xlsxpart) # mp3类型的附件 #mp3part = MIMEApplication(open(&apos;kenny.mp3&apos;, &apos;rb&apos;).read()) #mp3part.add_header(&apos;Content-Disposition&apos;, &apos;attachment&apos;, filename=&apos;benny.mp3&apos;) #msg.attach(mp3part) # pdf类型附件 #part = MIMEApplication(open(&apos;foo.pdf&apos;, &apos;rb&apos;).read()) #part.add_header(&apos;Content-Disposition&apos;, &apos;attachment&apos;, filename=&quot;foo.pdf&quot;) #msg.attach(part) try: s = smtplib.SMTP() #创建邮件服务器对象 s.connect(self.mail_host) #连接到指定的smtp服务器。参数分别表示smpt主机和端口 s.login(self.mail_user, self.mail_pass) #登录到你邮箱 s.sendmail(me, self.mail_list, msg.as_string()) #发送内容 s.close() return True except Exception, e: print str(e) return Falseif __name__ == &apos;__main__&apos;: #send list mailto_list = [&quot;aaa@lsh123.com&quot;,&quot;bbb@163.com&quot;] mail_title = &apos;Hey subject&apos; mail_content = &apos;Hey this is content&apos; mm = Mailer(mailto_list,mail_title,mail_content) res = mm.sendMail() print res 参考文章：python实现发送邮件及附件功能 python邮件中绘制图形： 生成图片作为附件发送。 python的绘图包matplotlib绘制图像。保存图像plt.savefig(‘test.png’)。 8. 是否了解Java虚拟机的功能？垃圾回收有了解？ JVM将java字节码解释为具体平台的具体指令。JVM负责重新解译由Java编译器生成的字节码，并和底层平台协调工作。也就是说，尽管Java编译器生成的结果是平台独立的，但JVM与特定平台相关的。 描述垃圾回收的流程： 对象在Eden区完成内存分配； 当Eden区满了，再创建对象，会因为申请不到空间，触发minorGC，进行young(eden+1survivor)区的垃圾回收； minorGC时，Eden不能被回收的对象被放入到空的survivor（Eden肯定会被清空），另一个survivor里不能被GC回收的对象也会被放入这个survivor，始终保证一个survivor是空的； 当做第3步的时候，如果发现survivor满了，将这些对象copy到old区，或者survivor并没有满，但是有些对象已经足够Old，也被放入Old区 XX:MaxTenuringThreshold； 当Old区被放满的之后，进行fullGC； 参考文章：垃圾回收过程 9. 现在内存有一个引用链，链上有一个个的内存实例，对它进行垃圾回收，流程或者是顺序是什么？ 采用可达性分析法。 通过一系列称为“GC Roots”的对象作为起始点，从这些节点向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链（即GC Roots到对象不可达）时，则证明此对象是不可用的。 GC Roots:1) 虚拟机栈（栈帧中的局部变量区，也叫做局部变量表）中引用的对象。2) 方法区中的类静态属性引用的对象。3) 方法区中常量引用的对象。4) 本地方法栈中JNI(Native方法)引用的对象。 关于引用链上的所有对象的回收时机，JVM程序无法精确控制垃圾回收的运行，垃圾回收会在合适的时候进行。当对象永久性地失去引用后，系统会在合适的时候回收它所占的内存。 当一个对象到GC Roots不可达时，在下一个垃圾回收周期中尝试回收该对象，如果该对象重写了finalize()方法，并在这个方法中成功自救(将自身赋予某个引用)，那么这个对象不会被回收。但如果这个对象没有重写finalize()方法或者已经执行过这个方法，也自救失败，该对象将会被回收。 对象在内存中的状态:当一个对象在堆内存中运行时，根据它被引用变量所引用的状态，可以把它所处的状态分成如下三种： 可达状态：当一个对象被创建后，若有一个以上的引用变量引用它，则这个对象在程序中处于可达状态，程序可通过引用变量来调用该对象的Field和方法。 可恢复状态：如果程序中某个对象不再有任何引用变量引用它，它就进入了可恢复状态。在这种状态下，系统的垃圾回收机制准备回收该对象所占用的内存，在回收该对象之前，系统会调用所有可恢复状态对象的finalize()方法进行资源清理。如果系统在调用finalize()方法时重新让一个引用变量引用该对象，则这个对象再次变为可达状态;否则，该对象将进入不可达状态。 不可达状态：当对象与所有引用变量的关联都被切断，且系统已经调用所有对象的finalize()方法后依然没有使该对象变成可达状态，那么这个对象将永久性地失去引用，最后变成不可达状态。只有当一个对象处于不可达状态时，系统才会真正回收该对象所占有的资源。 当发现对象不在引用链上时，将会发生一次标记，如果该对象没有必要执行finalize()方法时即：该对象没有finalize()方法或者finalize()方法已经被调用过，则进行回收。 当发现有finalize()方法时，则会将该对象放在一个专有的finalize线程中去执行。该线程会触发此方法，但是不保证执行成功。如果在finalize()方法中，该对象重新被引用，则逃过一劫，否则将会被回收掉。 参考文章：【Java】对象与垃圾回收 10. 垃圾回收是一次性标记完成的吗？ 在垃圾回收阶段，应用程序的执行会暂停，等待回收执行完毕后，再恢复程序的执行。一次性标记完成。 标记清除算法。在此阶段，垃圾回收器会从mutator（应用程序）根对象开始遍历。每一个可以从根对象访问到的对象都会被添加一个标识，于是这个对象就被标识为可到达对象。在清除阶段阶段中，垃圾回收器，会对堆内存从头到尾进行线性遍历，如果发现有对象没有被标识为可到达对象，那么就将此对象占用的内存回收，并且将原来标记为可到达对象的标识清除，以便进行下一次垃圾回收操作。 在使用标记清除算法时,未引用对象并不会被立即回收.取而代之的做法是,垃圾对象将一直累计到内存耗尽为止.当内存耗尽时,程序将会被挂起,垃圾回收开始执行。 11. 关于集合类，关于concurrent集合类为什么没有array的线程安全实现？ 非常有意思的一个问题，重点我的理解是解决并发瓶颈。 ConcurrentHashMap是线程安全的HashMap的实现。 CopyOnWriteArrayList是一个线程安全、并且在读操作时无锁的ArrayList。 ArrayBlockingQueue是一个基于数组、先进先出、线程安全的集合类，其特点是实现指定时间的阻塞读写，并且容量是可以限制的。 在java.util.concurrent包中没有加入并发的ArrayList实现的主要原因是：很难去开发一个通用并且没有并发瓶颈的线程安全的List。 像ConcurrentHashMap这样的类的真正价值（The real point / value of classes）并不是它们保证了线程安全。而在于它们在保证线程安全的同时不存在并发瓶颈。举个例子，ConcurrentHashMap采用了锁分段技术和弱一致性的Map迭代器去规避并发瓶颈。 CopyOnWriteArrayList是一个有趣的例子，它规避了只读操作（如get/contains）并发的瓶颈，但是它为了做到这点，在修改操作中做了很多工作和修改可见性规则。 此外，修改操作还会锁住整个List，因此这也是一个并发瓶颈。所以从理论上来说，CopyOnWriteArrayList并不算是一个通用的并发List。 参考文章：为什么java.util.concurrent 包里没有并发的ArrayList实现？ 12. MySQL与Redis你在项目是如何使用的？ mysql存储在磁盘里，redis存储在内存里，redis既可以用来做持久存储，也可以做缓存，而目前大多数公司的存储都是mysql + redis，mysql作为主存储，redis作为辅助存储被用作缓存，加快访问读取的速度，提高性能。 自己在项目中任务提交模块接口，上传文件功能采用redis存储文件内容，结果处理调用redis提高接口响应速度。 13. 你有没有考虑用其他方式来实现这样的数据缓存方式？ 线上系统php、nginx的配置无法更改，所以采用redis的方式。 或者可以文件上传到服务器，存储文件路径名称，这样不用存储文件流内容。 nginx 有个文件上传的module，利用这个模块，文件上传接收处理就直接交给nginx，nginx 直接把文件的相关参数传给php，这样php只接收文件相关参数，如文件大小、临时保存位置、类型等，不需处理提交过来的文件流 参考文章： ajaxFileUpload 异步上传文件配合PHP的使用 Web前后端缓存技术 14. 和面试官的交流- Java、Golang、python，以后需要熟练使用。 - 学习的能力，学习的潜力更加重要！ - 除了后端，针对于后台业务，仍需要掌握前端的一些基本的知识，框架，css，js等。 - 针对于后台逻辑，掌握框架。 - 针对高并发的数据处理，spark streaming等。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>tencent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[有赞一面]]></title>
    <url>%2Fyouzan_interview.html</url>
    <content type="text"><![CDATA[有赞一面 自己需要提升的是深度而不是广度，需要一直深入到最底层的原理。 1. 过往经历的自我介绍2. 多线程管理这方面你是如何去使用的？线程池原理？ 自己回答的是线程池方面。 另外一种是线程组。线程组存在的意义，首要原因是安全。java默认创建的线程都是属于系统线程组，而同一个线程组的线程是可以相互修改对方的数据的。但如果在不同的线程组中，那么就不能“跨线程组”修改数据，可以从一定程度上保证数据安全。 线程池是一种多线程处理形式，处理过程中将任务添加到队列，然后在创建线程后自动启动这些任务。线程池的基本思想还是一种对象池的思想，开辟一块内存空间，里面存放了众多(未死亡)的线程，池中线程执行调度由池管理器来处理。当有线程任务时，从池中取一个，执行完成后线程对象归池，这样可以避免反复创建线程对象所带来的性能开销，节省了系统的资源。 线程池的优点： 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 java中常用的线程池类主要有Executors类和ThreadPoolExecutor类。 【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 Java的线程池实现从根本上来说只有两个：ThreadPoolExecutor类和ScheduledThreadPoolExecutor类，这两个类还是父子关系，但是Java为了简化并行计算，还提供了一个Exceutors的静态类，它可以直接生成多种不同的线程池执行器，比如单线程执行器、带缓冲功能的执行器等，但归根结底还是使用ThreadPoolExecutor类或ScheduledThreadPoolExecutor类的封装类。 参数介绍： corePoolSize：核心池的大小，这个参数跟后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中。 maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程。 keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0。 unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性。 workQueue：一个阻塞队列，用来存储等待执行的任务。 threadFactory：线程工厂，主要用来创建线程。 handler：表示当拒绝处理任务时的策略。 线程池工作流程： 如果当前池大小 poolSize 小于 corePoolSize ，则创建新线程执行任务。 如果当前池大小 poolSize 大于 corePoolSize ，且等待队列未满，则进入等待队列 如果当前池大小 poolSize 大于 corePoolSize 且小于 maximumPoolSize ，且等待队列已满，则创建新线程执行任务。 如果当前池大小 poolSize 大于 corePoolSize 且大于 maximumPoolSize ，且等待队列已满，则调用拒绝策略来处理该任务。 线程池里的每个线程执行完任务后不会立刻退出，而是会去检查下等待队列里是否还有线程任务需要执行，如果在 keepAliveTime 里等不到新的任务了，那么线程就会退出。 3. 线程池的拒绝策略？ ThreadPoolExecutor.AbortPolicy //丢弃任务并抛出RejectedExecutionException异常。 ThreadPoolExecutor.DiscardPolicy //也是丢弃任务，但是不抛出异常。 ThreadPoolExecutor.DiscardOldestPolicy //丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） ThreadPoolExecutor.CallerRunsPolicy //由调用线程处理该任务 4. 线程池线程数量大小如何确定？ 一般需要根据任务的类型来配置线程池大小： 如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1 如果是IO密集型任务，参考值可以设置为2*NCPU 原理： 设线程池工作队列长度为m,且m&gt;&gt;n,则此时会导致CPU频繁切换线程来执行(如果CPU使用的是FCFS,则不会频繁切换,如使用的是其他CPU调度算法,如时间片轮转法,最短时间优先,则可能会导致频繁的线程切换)。所以这种情况下,无需设置过大的线程池工作队列,(工作队列长度 = CPU核心数 || CPU核心数+1) 即可。 对于计算密集型的任务，在拥有N个处理器的系统上，当线程池的大小为N+1时，通常能实现最优的效率。(即使当计算密集型的线程偶尔由于缺失故障或者其他原因而暂停时，这个额外的线程也能确保CPU的时钟周期不会被浪费。 1个线程对应1个方法栈,线程的生命周期与方法栈相同。比如某个线程的方法栈对应的入站顺序为:controller()-&gt;service()-&gt;DAO(),由于DAO长时间的I/O操作,导致该线程一直处于工作队列,但它又不占用CPU,则此时有1个CPU是处于空闲状态的。所以,这种情况下,应该加大线程池工作队列的长度(如果CPU调度算法使用的是FCFS,则无法切换),尽量不让CPU空闲下来,提高CPU利用率。 I/O密集型任务执行过程中，等待I/O的时间系那个对于其使用CPU的时间长，而处于I/O等待状态的线程并不会消耗CPU资源，对应线程池的大小可以调整为大于CPU核数，同时要注意的是I/O操作会引起上下文切换，核心池线程为1，最大线程池设置为2*CPU 核数，这样I/O操作带来的上下文切换是最少的。 参考文章： -Java并发之——线程池 -线程池线程数与(CPU密集型任务和I/O密集型任务)的关系 4. 场景：抓取网页几T数据，查询网页中是否有java关键字，并且在文件中找出出现java频次比较高的文件，根据频次高低排序，找出最高的前几个文件。 如何快速匹配关键词以及匹配 读取每行判断；使用多个线程，分割读取指定的大文件。获取我们所需要的信息。 12RandomAccessFile函数randomFile.seek(start);跳跃读取，从这里开始读。 -参考文章：java多线程读取大文件 在海量文件中找出出现频率最高的前K个 针对top k类问题，通常比较好的方案是【分治+trie树/hash+小顶堆】，即先将数据集按照hash方法分解成多个小数据集，然后使用trie树或者hash统计每个小数据集中的query词频，之后用小顶堆求出每个数据集中出频率最高的前K个数，最后在所有top K中求出最终的top K。 单机+单核+受限内存 这种情况下，需要将原数据文件切割成一个一个小文件，如，采用hash(x)%M，将原文件中的数据切割成M小文件，如果小文件仍大于内存大小，继续采用hash的方法对数据文件进行切割，直到每个小文件小于内存大小，这样，每个文件可放到内存中处理。 参考文章： -怎样从10亿查询词找出出现频率最高的10个 6. CPU对于线程的调度过程？ CPU的调度决策会发生在如下四种情况下：（1）运行–&gt;等待（2）运行–&gt;就绪（3）等待–&gt;就绪（4）运行–&gt;终止 先来先服务（FCFS）最简单的CPU调度算法，根据进程进入就绪队列的先后顺序调度进程执行。但是这种方法的平均等待时间通常较长。 最短作业优先调度（SJF）将每个进程与下一个CPU区间段相关联，当CPU空闲时，他回赋给具有最短CPU区间的进程。如果两个进程具有相同CPU区间长度，可以使用FCFS来调度。这里的CPU区间长度是进程的下一个CPU区间长度，而不是进程的CPU区间总长度。此算法可证明是最佳的，其难点在于如何知道进程的下一个CPU区间的长度。近似的情况下只能通过预测来做。 优先级调度每个进程都有一个优先级相关联，具有最高优先级的进程会被分配到CPU，具有相同优先级的进程根据FCFS顺序调度。实际上短作业优先就是优先级调度算法的一个特例。优先级调度算法的一个主要问题是导致低优先级的进程产生无穷阻塞或饥饿（可以运行但是缺乏CPU），其解决问题之一就是老化，即以逐渐增加在系统中等待很长时间的进程的优先级。 时间片轮转法调度（RR）为分时系统而设计，类似于FCFS，不过每个进程只分配不超过一个时间片的CPU，对就绪队列中的进程依次循环分配时间执行。 多级队列调度将就绪队列中的进程根据进程的属性不同而永久分配到多个独立的队列中，每个队列都有自己的调度算法。同时队列中也有调度，通常采用固定优先级抢占调度。 多级反馈队列调度跟多级队列调度相似，最大的不同是允许进程在队列之间移动。主要思想是根据不同CPU区间的特点以区分进程，如果进程使用过多CPU时间，那么它会被转移到更低优先级队列。多级反馈队列调度是最通用的CPU调度算法但是因为要设置很多调度参数，因此也是最复杂的算法。 参考文章：进程、线程以及CPU调度 7. 如何控制多线程的并发？ 在JDK中，提供了多种途径实现多线程间的并发控制。比如常用的：内部锁、重入锁、读写锁和信号量。 同步关键字synchronized是Java语言中最为常用的同步方法之一。synchronized的用法：synchronized修饰方法和synchronized修饰代码块。 当两个并发线程访问同一个对象object中的这个synchronized(this)同步代码块时，一个时间内只能有一个线程得到执行。另一个线程必须等待当前线程执行完这个代码块以后才能执行该代码块。 然而，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。 尤其关键的是，当一个线程访问object的一个synchronized(this)同步代码块时，其他线程对object中所有其它synchronized(this)同步代码块的访问将被阻塞。 第三个例子同样适用其它同步代码块。也就是说，当一个线程访问object的一个synchronized(this)同步代码块时，它就获得了这个object的对象锁。结果，其它线程对该object对象所有同步代码部分的访问都被暂时阻塞。 以上规则对其它对象锁同样适用. synchronized 修饰方法时锁定的是调用该方法的对象。它并不能使调用该方法的多个对象在执行顺序上互斥。 1234567891011public synchronized void method()&#123;&#125;当method()方法被调用时，调用线程首先必须获得当前对象所，若当前对象锁被其他线程持有，这调用线程会等待，调用结束后，对象锁会被释放，以上方法等价于下面的写法：public void method()&#123;synchronized(this)&#123;// do something …&#125;&#125;public synchronized static void method()&#123;&#125;这个地方一定要注意，synchronized的锁是加在 当前Class对象 上，因此，所有对该方法的调用，都必须获得Class对象的锁。 synchronized的缺陷：当某个线程进入同步方法获得对象锁，那么其他线程访问这里对象的同步方法时，必须等待或者阻塞，这对高并发的系统是致命的，这很容易导致系统的崩溃。如果某个线程在同步方法里面发生了死循环，那么它就永远不会释放这个对象锁，那么其他线程就要永远的等待。这是一个致命的问题。 参考文章：JAVA 同步之 synchronized 修饰方法 8. MySQL的索引的结构？ MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。 MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 9. InnoDB中所有的索引都是B+树吗？普通索引是什么样子的？ innodb存储引擎支持两种常见的索引：B+树索引和哈希索引。innodb支持哈希索引是自适应的，innodb会根据表的使用情况自动生成哈希索引。 哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。 Mysql各种索引区别： 普通索引：最基本的索引，没有任何限制 唯一索引：与”普通索引”类似，不同的就是：索引列的值必须唯一，但允许有空值。 主键索引：它是一种特殊的唯一索引，不允许有空值。 全文索引：仅可用于 MyISAM 表，针对较大的数据，生成全文索引很耗时好空间。 组合索引：为了更多的提高mysql效率可建立组合索引，遵循”最左前缀“原则。 10. 聚簇索引与非聚簇索引的区别？ 聚簇索引的叶子节点就是数据节点，而非聚簇索引的叶子节点仍然是索引节点，只不过有指向对应数据块的指针。 聚簇索引是顺序结构与数据存储物理结构一致的一种索引，并且一个表的聚簇索引只能有唯一的一条。 非聚簇索引记录的物理顺序与逻辑顺序没有必然的联系，与数据的存储物理结构没有关系；一个表对应的非聚簇索引可以有多条，根据不同列的约束可以建立不同要求的非聚簇索引。 11. 一张大表千万级数据，如何实现性能比较高的分页查询？12分页查询语句：SELECT * FROM 表名称 LIMIT M,N 原因/缺点: 全表扫描,速度会很慢 且有的数据库结果集返回不稳定(如某次返回1,2,3,另外的一次返回2,1,3). Limit限制的是从结果集的M位置处取出N条输出,其余抛弃。 OFFSET过大导致MySQL耗费了大量随机I/O在查询聚簇索引的数据上。 改进措施： 利用覆盖索引的查询。 123SELECT * FROM product WHERE ID &gt; =(select id from product limit 866613, 1) limit 20SELECT * FROM product a JOIN (select id from product limit 866613, 20) b ON a.ID = b.id 复合索引 如果对于有where 条件，又想走索引用limit的，必须设计一个索引，将where 放第一位，limit用到的主键放第2位，而且只能select主键！ 利用MySQL支持ORDER操作可以利用索引快速定位部分元组,避免全表扫描1SELECT * FROM your_table WHERE pk&gt;=1000 ORDER BY pk ASC LIMIT 0,20 12. MySQL事务由哪些特性？ 事务的四大特性（ACID）： 原子性（atomicity）：一个事务必须视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性。 一致性（consistency）：数据库总数从一个一致性的状态转换到另一个一致性的状态。 隔离性（isolation）：一个事务所做的修改在最终提交以前，对其他事务是不可见的。 持久性（durability）：一旦事务提交，则其所做的修改就会永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。 数据库事务的隔离级别有4种，由低到高分别为Read uncommitted 、Read committed 、Repeatable read 、Serializable 。而且，在事务的并发操作中可能会出现脏读，不可重复读，幻读。 13. Linux分析日志，例如nginx中access.log中请求前100的IP有哪些？如果nginx日志过大如何处理？1awk &apos;&#123;print $1&#125;&apos; access.log | sort -n |uniq -c | sort -rn | head -n 100 编辑每日定时脚本，切割日志日志（根据服务器内存大小指定分割大小），然后压缩一下。这样磁盘空间以及内存占用占用就小了。 12345678#!/bin/bash local_path=/home/work/tp/log/webserver #找到您服务器中存放access日志的目录 cd $local_path #进入这个目录 mv access_log $local_path/access_log` date +%Y%m%d%H` #把当前的access_log挪到这个时期下，其实就是相当于日志的切分 nginx_pid=`ps -ef |grep -v grep |grep “nginx: master process “|awk -F” ” ‘&#123;print $2&#125;’` #找到您nginx的进程 `kill -USR1 $nginx_pid` #执行usr1 通过定时任务来让这个nginx_time.sh脚本按每小时来进行切分10 */1 * * * sh /xxx(您这个脚本的存放命令)/nginx.sh 按每小时切割 参考文章：nginx中access日志如何做到按时间完美切割 14. 照常的提问环节~]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>youzan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[迅雷一面]]></title>
    <url>%2Fxunlei_interview.html</url>
    <content type="text"><![CDATA[迅雷一面 面试官让我感觉Java与安卓互联互通~ 1. 线程不安全的栈如何实现 statck：堆栈类，先进后出，线程安全 用数组实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.ArrayList;import java.util.List;import java.util.EmptyStackException;public class Statck&lt;E extends Object&gt; &#123; private List&lt;E&gt; pool = new ArrayList&lt;E&gt;(); public Statck() &#123; &#125; public void clear() &#123; pool.clear(); &#125; public boolean isEmpty() &#123; return pool.isEmpty(); &#125; /** * 获取栈顶元素 * */ public E getTopObjcet() &#123; if (isEmpty()) &#123;return null;&#125; return pool.get(pool.size()-1); &#125; /** * 弹出栈操作 * */ public E pop() &#123; if (isEmpty()) &#123;throw new EmptyStackException();&#125; return pool.remove(pool.size() - 1); &#125; /** * 压入栈 * */ public void push(E e) &#123; if (isEmpty()) &#123;throw new EmptyStackException();&#125; pool.add(e); &#125; /** * 获取当前栈大小 * */ public int getStatckSize() &#123; if (isEmpty()) &#123;throw new EmptyStackException();&#125; return pool.size(); &#125;&#125; 用链表实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class MyStack &#123; class Node &#123;// 定义节点 private Node next; public Object value; &#125; Node top = null; void init() &#123;//初始话头结点 top = new Node(); top.next = null; top.value = null; &#125; public void push(Object element) &#123;//采用头插发的方式模拟入栈 Node e = new Node(); e.value = element; if (top.next == null) &#123; top.next = e; &#125; else &#123; e.next = top.next; top.next = e; &#125; &#125; public Object pop() &#123;//弹出栈顶元素，也就是头结点后面的第一个元素 Object ele = null; if (top.next == null) &#123; System.out.println(&quot;栈为空!&quot;); &#125; else &#123; ele = top.next.value; top.next = top.next.next;//移动指针。相当于删除链表中第一个元素 &#125; return ele; &#125; public Object peek()//返回栈顶元素,不执行出栈操作 &#123; if(top.next==null) &#123; return -1; &#125; else return top.next.value; &#125; public boolean isempty()//判断栈是否为空 &#123; return top.next==null?true:false; &#125; public int size() &#123;//返回栈的大小，含有的元素个数 Node temp = top; int i = 0; while (temp.next != null) &#123; i++; temp = temp.next; &#125; return i; &#125; public void print() &#123;//打印栈中存在的元素 Node temp = top; if(temp.next==null) &#123; System.out.println(&quot;栈为空！&quot;); &#125; while (temp.next != null) &#123; System.out.print(temp.next.value + &quot; &quot;); temp = temp.next; &#125; &#125; public static void main(String[] args) &#123; MyStack stack = new MyStack(); stack.init(); for (int i = 0; i &lt; 5; i++) &#123; stack.push(i); &#125; /*Object ele1 = stack.pop(); Object ele2 = stack.pop(); Object ele3 = stack.pop(); Object ele4 = stack.pop(); Object ele5 = stack.pop(); System.out.println(ele1); System.out.println(ele2); System.out.println(ele3); System.out.println(ele4); System.out.println(ele5);*/ Object ele1 = stack.pop(); System.out.println(&quot;此次弹出的元素为:&quot;+ele1); System.out.print(&quot;栈中剩余的元素为:&quot;); stack.print(); System.out.println(); System.out.println(&quot;栈顶元素为:&quot;+stack.peek()); &#125;&#125; 用队列实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import java.util.LinkedList; public class QueuesToStack &#123; LinkedList&lt;Integer&gt; queue1=new LinkedList&lt;Integer&gt;(); LinkedList&lt;Integer&gt; queue2=new LinkedList&lt;Integer&gt;(); public void push(int value)//入栈 &#123; queue1.addLast(value); &#125; public int pop()//出栈 必须是非空的栈才能出栈啊 &#123; if(sSize()!=0)//栈不为空 &#123; //移动一个队的n-1个到另一个中 if(!queue1.isEmpty())//q1 空 &#123; putN_1ToAnthor(); return queue1.removeFirst(); &#125; else //q2 空 &#123; putN_1ToAnthor(); return queue2.removeFirst(); &#125; &#125; else &#123; System.out.println(&quot;栈已经为空啦，不能出栈&quot;); return -1; &#125; &#125; public int sSize() &#123; return queue1.size()+queue2.size(); &#125; public void putN_1ToAnthor()//从非空中出队n-1个到另一个队列 因为队列总是一空一非空 &#123; if(!queue1.isEmpty()) &#123; while(queue1.size()&gt;1) &#123; queue2.addLast(queue1.removeFirst()); &#125; &#125; else if(!queue2.isEmpty()) &#123; while(queue2.size()&gt;1) &#123; queue1.addLast(queue2.removeFirst()); &#125; &#125; &#125; public static void main(String[] args) &#123; QueuesToStack stack=new QueuesToStack(); stack.push(1); stack.push(2); stack.push(3); stack.push(4); System.out.println(stack.pop()); System.out.println(stack.pop()); stack.push(5); stack.push(6); System.out.println(stack.pop()); System.out.println(stack.pop()); System.out.println(stack.pop()); System.out.println(stack.pop()); System.out.println(stack.pop()); &#125; &#125; -参考文章： -两个栈实现队列+两个队列实现栈—-java -java用单链表实现栈的基本操作 2.反射底层是如何实现的 Method获取 调用Class类的getDeclaredMethod可以获取指定方法名和参数的方法对象Method。 privateGetDeclaredMethods方法从缓存或JVM中获取该Class中申明的方法列表，searchMethods方法将从返回的方法列表里找到一个匹配名称和参数的方法对象。 如果找到一个匹配的Method，则重新copy一份返回，即Method.copy()方法 所次每次调用getDeclaredMethod方法返回的Method对象其实都是一个新的对象，且新对象的root属性都指向原来的Method对象，如果需要频繁调用，最好把Method对象缓存起来。 在privateGetDeclaredMethods方法中，如果通过reflectionData()获得的ReflectionData对象不为空，则尝试从ReflectionData对象中获取declaredMethods属性，如果是第一次，或则被GC回收之后，重新初始化后的类属性为空，则需要重新到JVM中获取一次，并赋值给ReflectionData，下次调用就可以使用缓存数据了。 Method调用 获取到指定的方法对象Method之后，就可以调用它的invoke方法。 MethodAccessor对象是invoke方法实现的关键，一开始methodAccessor为空，需要调用acquireMethodAccessor生成一个新的MethodAccessor对象，MethodAccessor本身就是一个接口 在acquireMethodAccessor方法中，会通过ReflectionFactory类的newMethodAccessor创建一个实现了MethodAccessor接口的对象 在ReflectionFactory类中，有2个重要的字段：noInflation(默认false)和inflationThreshold(默认15)，在checkInitted方法中可以通过-Dsun.reflect.inflationThreshold=xxx和-Dsun.reflect.noInflation=true对这两个字段重新设置，而且只会设置一次；如果noInflation为false，方法newMethodAccessor都会返回DelegatingMethodAccessorImpl对象，DelegatingMethodAccessorImpl的类实现 DelegatingMethodAccessorImpl对象就是一个代理对象，负责调用被代理对象delegate的invoke方法，其中delegate参数目前NativeMethodAccessorImpl对象，所以最终Method的invoke方法调用的是NativeMethodAccessorImpl对象invoke方法 generateMethod方法在生成MethodAccessorImpl对象时，会在内存中生成对应的字节码，并调用ClassDefiner.defineClass创建对应的class对象，每次都生成新的类加载器，是为了性能考虑，在某些情况下可以卸载这些生成的类，因为类的卸载是只有在类加载器可以被回收的情况下才会被回收的，如果用了原来的类加载器，那可能导致这些新创建的类一直无法被卸载，从其设计来看本身就不希望这些类一直存在内存里的，在需要的时候有就行了。 参考文章： 3.如何实现自己加载的classloader 父加载器不能完成加载任务时，会调用findClass(name)函数，这个就是我们自己实现的ClassLoader的查找类文件的规则，所以在继承后，我们只需要覆盖findClass()这个函数，实现我们在本加载器中的查找逻辑，而且还不会破坏双亲委托模型。 我们在创建自己的ClassLoader时只需要覆写findClass(name)和findResource()即可。从classpath中加载类资源。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.tzx.reflection;import java.io.ByteArrayOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;public class MyClassLoader extends ClassLoader&#123; static &#123; System.out.println(&quot;MyClassLoader&quot;); &#125; public static final String driver = &quot;/home/im/Desktop/&quot;; public static final String fileTyep = &quot;.class&quot;; public Class findClass(String name) &#123; byte[] data = loadClassData(name); return defineClass(data, 0, data.length); &#125; public byte[] loadClassData(String name) &#123; FileInputStream fis = null; byte[] data = null; try &#123; File file = new File(driver + name + fileTyep); System.out.println(file.getAbsolutePath()); fis = new FileInputStream(file); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int ch = 0; while ((ch = fis.read()) != -1) &#123; baos.write(ch); &#125; data = baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); System.out.println(&quot;loadClassData-IOException&quot;); &#125; return data; &#125;&#125;// 使用public class ClassLoaderTest &#123; public static void main(String[] args) &#123; MyClassLoader cl1 = new MyClassLoader(); //磁盘中/home/im/Desktop/Hello.class文件存在 try &#123; Class c1 = cl1.loadClass(&quot;Hello&quot;); Object object = c1.newInstance(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); System.out.println(&quot;main-ClassNotFoundException&quot;); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 参考文章：自定义ClassLoader和双亲委派机制 4.classloader与增量更新 增量更新：省流量更新原理可以这么理解：服务端可以借助bsdiff工具，比对新旧apk包的文件，获取到差分文件之后下发到客户端，而这个差分文件的大小肯定是小于新的apk文件大小的。客户端得到这个差分文件之后，本地在使用bspatch工具进行差分文件和本地已经安装的旧apk包进行合并成新的apk包文件，然后在进行升级安装。 参考文章： 5.Switch支持哪些数据类型 总共6种类型： byte、short、char、int、String、枚举 6.如何实现swith对String的支持 通过对case后面的String对象调用hashCode()方法，得到一个int类型的Hash值，然后用这个Hash值来唯一标识着这个case。那么当匹配的时候，首先调用这个字符串的hashCode()方法，获取一个Hash值（int类型），用这个Hash值来匹配所有的case，如果没有匹配成功，说明不存在；如果匹配成功了，接着会调用字符串的equals()方法进行匹配。 对于String类型，使用字符串的哈希值作为switch语句表达式的值。经过转换，Java 虚拟机看到的仍然是与整数类型兼容的类型。这里要注意的是，在case字句中对应的语句块中仍然需要使用String的equals方法来进行字符串比较，这是因为哈希函数在映射的时候可能存在冲突，这样更加保险了。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>xunlei</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同花顺一面记录]]></title>
    <url>%2Ftonghuashun_interview.html</url>
    <content type="text"><![CDATA[同花顺一面 墨菲定律的含义就是，不能有侥幸心理。 1. 手撕代码：Jdbc加载过程实现查询语句 所需步骤： 有以下步骤需要创建一个新的数据库使用JDBC应用程序： 导入包: 需要包括软件包包含了数据库编程所需的JDBC类。大多数情况下，使用导入的java.sql.*就足够了。 注册JDBC驱动: 需要初始化驱动程序，这样就可以打开与数据库的通信通道。 打开一个连接: 需要使用DriverManager.getConnection()方法来创建一个Connection对象，它代表一个与数据库服务器的物理连接。 执行一个查询: 需要使用的对象的类型声明用于构建和提交一个SQL语句来选择（即取）从表中的记录。 提取数据 . 一旦执行SQL查询，可以从表中获取记录。 清理环境 . 需要明确关闭所有的数据库资源，对依赖于JVM的垃圾收集。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768//STEP 1. Import required packagesimport java.sql.*;public class JDBCExample &#123; // JDBC driver name and database URL static final String JDBC_DRIVER = &quot;com.mysql.jdbc.Driver&quot;; static final String DB_URL = &quot;jdbc:mysql://localhost/STUDENTS&quot;; // Database credentials static final String USER = &quot;username&quot;; static final String PASS = &quot;password&quot;; public static void main(String[] args) &#123; Connection conn = null; Statement stmt = null; try&#123; //STEP 2: Register JDBC driver Class.forName(&quot;com.mysql.jdbc.Driver&quot;); //STEP 3: Open a connection System.out.println(&quot;Connecting to a selected database...&quot;); conn = DriverManager.getConnection(DB_URL, USER, PASS); System.out.println(&quot;Connected database successfully...&quot;); //STEP 4: Execute a query System.out.println(&quot;Creating statement...&quot;); stmt = conn.createStatement(); String sql = &quot;SELECT id, first, last, age FROM Registration&quot;; ResultSet rs = stmt.executeQuery(sql); //STEP 5: Extract data from result set while(rs.next())&#123; //Retrieve by column name int id = rs.getInt(&quot;id&quot;); int age = rs.getInt(&quot;age&quot;); String first = rs.getString(&quot;first&quot;); String last = rs.getString(&quot;last&quot;); //Display values System.out.print(&quot;ID: &quot; + id); System.out.print(&quot;, Age: &quot; + age); System.out.print(&quot;, First: &quot; + first); System.out.println(&quot;, Last: &quot; + last); &#125; rs.close(); &#125;catch(SQLException se)&#123; //Handle errors for JDBC se.printStackTrace(); &#125;catch(Exception e)&#123; //Handle errors for Class.forName e.printStackTrace(); &#125;finally&#123; //finally block used to close resources try&#123; if(stmt!=null) conn.close(); &#125;catch(SQLException se)&#123; &#125;// do nothing try&#123; if(conn!=null) conn.close(); &#125;catch(SQLException se)&#123; se.printStackTrace(); &#125;//end finally try &#125;//end try System.out.println(&quot;Goodbye!&quot;);&#125;//end main&#125;//end JDBCExample by www.yiibai.com 参考文章：JDBC - Select查询数据记录例子 2. Spring bean加载过程 加载XML文件，封装成Resource对象 调用Reader对象方法读取XML文件内容，并将相关属性放到BeanDefinition实例 将BeanDefinition对象放到BeanFactory对象 3. 你认为一个好的团队是什么样子的？ 好的团队，我的理解，就是互相学习，互相沟通，互相信任。leader是开放包容，开发是专注坚持。 参考文章：据说一个成功的研发团队应具备这9大属性]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>tonghuashun</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海康威视一面记录]]></title>
    <url>%2Fhikvision_interview.html</url>
    <content type="text"><![CDATA[# 海康威视一面记录 &gt; 面试中记录显示了自己的深度的不足，敬佩面试官对于技术深度以及开源项目的态度。 ## 1. 手撕SQL：两张表（薪水表、部门表）求每一个部门薪水前三的结果 该Employee表包含所有员工。每个员工都有一个ID，并且还有一个部门ID的列。 | Id | Name| salary| DepartmentId | |--|--|--|--| | 1 | 乔| 70000 | 1 | | 2 | 亨利| 80000 | 2 | | 3 | 山姆| 60000 | 2 | | 4 | Max | 90000 | 1 | | 5 | 珍妮特| 69000 | 1 | | 6 | 兰迪| 85000 | 1 | 该Department表包含公司的所有部门。 | Id | Department| |--|--| | 1 | IT | | 2 | 销售| 编写一个SQL查询来查找每个部门获得前三名薪水的员工。对于上述表，您的SQL查询应返回以下行。 | Department| employeeName| Salary| |--|--|--| | IT | Max | 90000 | | IT | 兰迪| 85000 | | IT | 乔| 70000 | | 销售| 亨利| 80000 | | 销售| 山姆| 60000 | &gt; 解题思路： &gt; 自己当时想法简单直接，直接大笔一挥inner join，group by，order by，limit全部堆上去，too样toosimple。 12345678d.name AS department ,e.name AS EmployeeName, e.salary AS SalaryFROMemployee as eINNER JOIN department as dON e.dept_id = d.idGROUP BY d.name, e.nameORDER BY e.salary DESCLIMIT 3 &gt; 显然，ORDER BY 把GROUP BY分好组的结果打乱，LIMIT最终结果也只能取到3条数据，显然不符合题目要求。 1234567891011select d.name AS department ,e1.name AS EmployeeName, e1.salary AS Salary from employee e1 left join department d on e1.dept_id=d.id where ( select count(1) from employee e2 where e2.dept_id=e1.dept_id and e2.salary&gt;=e1.salary ) &lt;=3 /*这里的数值表示你想取前几名*/order by d.name, e1.salary desc; &gt; 看了才知道这样的思路很妙，employee表自身连接来实现取薪水前几的功能。 &gt; Oracle自带分组函数，row_number(),mysql可以模拟实现该函数。 12345678select * from (select dept_id,name,salary, row_number() over( partition by dept_id order by salary desc) rn from employee ) where rn&lt;=3; ## 2. 你了解的开源软件有哪些？你知道Dubbo？Dubbo的协议扩展？ &gt; 自己对于Github上面比较好的后端项目只是fork没有好好看，自己需要合理计划学习时间进度。 &gt; Dubbo是Alibaba开源的分布式服务框架，它最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。 &gt; Dubbo的扩展点加载从JDK标准的SPI(Service Provider Interface)扩展点发现机制加强而来。 &gt; Dubbo改进了JDK标准的SPI的以下问题： 1. JDK标准的SPI会一次性实例化扩展点所有实现，如果有扩展实现初始化很耗时，但如果没用上也加载，会很浪费资源。 2. 如果扩展点加载失败，连扩展点的名称都拿不到了。比如：JDK标准的ScriptEngine，通过getName();获取脚本类型的名称，但如果RubyScriptEngine因为所依赖的jruby.jar不存在，导致RubyScriptEngine类加载失败，这个失败原因被吃掉了，和ruby对应不起来，当用户执行ruby脚本时，会报不支持ruby，而不是真正失败的原因。 3. 增加了对扩展点IoC和AOP的支持，一个扩展点可以直接setter注入其它扩展点。 &gt; 在扩展类的jar包内，放置扩展点配置文件：META-INF/dubbo/接口全限定名，内容为：配置名=扩展实现类全限定名，多个实现类用换行符分隔。 类似于key-value的形式。 &gt; com.alibaba.dubbo.common.extension &gt; ExtensionLoader没有提供public的构造方法，但是提供了一个public static的getExtensionLoader，这个方法就是获取ExtensionLoader实例的工厂方法。其public成员方法中有三个比较重要的方法： - getActivateExtension ：根据条件获取当前扩展可自动激活的实现 - getExtension ： 根据名称获取当前扩展的指定实现 - getAdaptiveExtension : 获取当前扩展的自适应实现 - 参考文章：[](http://blog.csdn.net/jdluojing/article/details/44947221) ## 3. java8新特性哪些是虚拟机层面的？ 1. JDK8 HotSpot JVM 将移除永久区，使用本地内存来存储类元数据信息并称之为：元空间（Metaspace） ![](http://incdn1.b0.upaiyun.com/2015/02/a346dd9605bff60e7e7af9e8f3d82064.png) 2. java7引入了invokedynamic在JVM层面支持方法调用的动态化。通过引入新的指令集和新的常量，编译时不再需要确定方法引用，而是由invokedynamic指令在运行时调用bootstrap方法在运行时决定方法的定义。除了对动态语言的支持之外，invokedynamic还为Java8引入lambda语言特性提前做好了JVM层面的准备。 &gt; lambda就可以利用invokedynamic指令的动态调用特性实现lambda表达式的调用。 &gt; invokedynamic指令的工作原理通过调用bootstrap方法获得CallSite，然后执行这个CallSite中包含的方法句柄完成方法的调用，调用的结果会返回一个Function的实例。 3. 新的垃圾收集器: G1 &gt; Full GC主要会进行老年代的垃圾回收，主流的老年代垃圾回收器CMS虽然设计目标是获得最短的回收停顿时间，但是仍然会导致一定时间的停服务(Stop The World)，所以会影响到服务的正常响应。 ![](http://warrentalk.site/img/5959b0413dfb5f1969000002.png) &gt; CMS主要的问题在于： - 垃圾回收过程依然会停用户服务，如果是大内存场景下发生Full GC，这个时间可能会比较长。 - 采用标记清除算法，会导致大量的碎片，影响大对象的空间分配。 &gt; 在G1中依然是分代回收的概念，但是每一代的空间不再是简单地连续分配，他们被分散成多个大小相等的存储块，每个块都可以进行独立的垃圾回收，他们被称作Region。因为可以独立进行垃圾回收，那就有能力将一次很长的GC分解成多个短GC，通过调整每一次需要回收的Region数量来控制停顿时间的长短。 &gt; G1的优势在于： - 将垃圾回收化整为零，减少对用户服务的影响 - 垃圾回收时间可配置 - 避免内存碎片 - 参考文章：[从Java6到Java8: 你应该知道的JVM新特性](http://warrentalk.site/2017/07/13/%E4%BB%8EJava6%E5%88%B0Java8-%E4%BD%A0%E5%BA%94%E8%AF%A5%E7%9F%A5%E9%81%93%E7%9A%84JVM%E6%96%B0%E7%89%B9%E6%80%A7/) ## 4. 如何保证流媒体的实时传输与高并发？ &gt; 内容分发网络（Content Delivery Network），是在现有 Internet 中增加的一层新的网络架构，由遍布全国的高性能加速节点构成。这些高性能的服务节点都会按照一定的缓存策略存储您的业务内容，当您的用户向您的某一业务内容发起请求时，请求会被调度至最接近用户的服务节点，直接由服务节点快速响应，有效降低用户访问延迟，提升可用性。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>hikvison</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap三两事]]></title>
    <url>%2Fhashmap_note.html</url>
    <content type="text"><![CDATA[HashMap链表转为红黑树的临界值为8的原因 参考文章： 深入理解哈希表 Jdk1.8中的HashMap实现原理 Java8系列之重新认识HashMap HashMap原理 在JDK1.6中，HashMap采用位桶+链表实现，即使用链表处理冲突，同一hash值的Entity都存储在一个链表里。但是当位于一个桶中的元素较多，即hash值相等的元素较多时，通过key值依次查找的效率较低。 而JDK1.8(JDK版本号为：1.8.0_25)中，HashMap采用位桶+链表+红黑树实现，当链表长度超过阈值（8）时，将链表转换为红黑树，这样大大减少了查找时间（查找时间复杂度由O(n)变为O(lgn)）。 123456// 这是一个阈值，当桶(bucket)上的链表数大于这个值时会转成红黑树，put方法的代码里有用到。put的时候如果两个值// 经过hash后都落在同一个bucket上这叫发生一次hash碰撞，往里不断put数据，当一个bucket发生8次碰撞就转成红黑树。static final int TREEIFY_THRESHOLD = 8; // 也是阈值同上一个相反，当桶(bucket)上的链表数小于这个值时树转链表static final int UNTREEIFY_THRESHOLD = 6; TREEIFY_THRESHOLD = 8的原因理想状态下哈希表的每个“箱子”中，元素的数量遵守泊松分布: 泊松分布的参数λ是单位时间(或单位面积)内随机事件的平均发生率。 泊松分布适合于描述单位时间内随机事件发生的次数。（来自百度百科） 理想来说，在负载因子为0.75的条件下，bin中Node出现的频率满足参数为0.5的泊松分布。 当负载因子为 0.75 时，上述公式中 λ 约等于 0.5，因此箱子中元素个数和概率的关系如下: 数量 概率 0 0.60653066 1 0.30326533 2 0.07581633 3 0.01263606 4 0.00157952 5 0.00015795 6 0.00001316 7 0.00000094 8 0.00000006 从概率来看，之所以链表长度超过 8 以后要变成红黑树，因为在正常情况下出现这种现象的几率小到忽略不计。一旦出现，几乎可以认为是哈希函数设计有问题导致的。 HashMap的大小要设置为2的n次幂的原因 HashMap中的数据结构是数组+单链表的组合，我们希望的是元素存放的更均匀，最理想的效果是，Entry数组中每个位置都只有一个元素，这样，查询的时候效率最高，不需要遍历单链表，也不需要通过equals去比较K，而且空间利用率最大。 123456/** * Returns index for hash code h. */ static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; h:为插入元素的hashcodelength:为map的容量大小&amp;：与操作 比如 1101 &amp; 1011=1001 1234567//计算key的hashstatic final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125;// h = key.hashCode() 为第一步 取hashCode值// h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 当length总是2的n次方时，h&amp; (length-1)运算等价于对length取模，也就是h%length，但是&amp;比%具有更高的效率。最终的bucketIndex是通过取余来运算,总是可以得到在bucket长度区间内的结果。 h的计算是通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在bucket的n比较小的时候，也能保证考虑到高低bit都参与到hash的计算中，同时不会有太大的开销。 HashMap加载因子默认为0.75的原因 HashMap的插入和搜索，复杂度都是O(1)，是非常快速的跟你的容量大小通常是没有直接关系的但是这是理想的情况。 这里说的理想，是在你所存储的对象的hashcode这个方法写的非常有效的情况下。根据hash的原理，存放一个对象是根据他的hashcode来计算的，如果没有哈希冲突，那么他的存储效率是最高，最完美的。 当通过HashCode计算地址存放，发现当前位置已经有元素，则称为元素的碰撞，需要重新计算或者其他方式放置该元素。 哈希表为了避免这种冲突，会有一点优化。简单的说，原本可以放100个数据的空间，当放到80个的时候，根据经验，接下去冲突的可能性会更加高，就好比一个靶子上80%都是箭的时候你再射一箭出去，射中箭的可能性很大。因此就自动增加空间来减小冲突可能性。 个数与碰撞几率服从柏松分布，发现在0.75处几率最小。 默认加载因子在时间和空间成本上寻求一种折衷,是对空间和时间效率的一个平衡选择。 解决hash冲突的办法 开放定址法（线性探测再散列，二次探测再散列，伪随机探测再散列） 再哈希法 链地址法 建立一个公共溢出区 HashMap允许null值的原因 ConcurrentHashmap和Hashtable都是支持并发的，这样会有一个问题，当你通过get(k)获取对应的value时，如果获取到的是null时，你无法判断，它是put（k,v）的时候value为null，还是这个key从来没有做过映射。HashMap是非并发的，可以通过contains(key)来做这个判断。 多线程下HashMap的resize()为什么会出现死循环? 存在的问题：HashMap为数组+链表结构，链表结构容易形成闭合的链路，这样在循环的时候只要有线程对这个HashMap进行get操作就会产生死循环。 只有在多线程并发的情况下才会出现这种情况，那就是在put操作的时候，如果size&gt;initialCapacity*loadFactor，那么这时候HashMap就会进行rehash操作，随之HashMap的结构就会发生翻天覆地的变化。很有可能就是在两个线程在这个时候同时触发了rehash操作，产生了闭合的回路。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879/* 初始化或者是将table大小加倍。如果为空，则按threshold分配空间，否则，加倍后，每个容器中的元素在新table中要么呆在原索引处，要么有一个2的次幂的位移 * @return the table */final Node&lt;K, V&gt;[] resize() &#123; Node&lt;K, V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; //容量加倍 oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // 阈值加倍 &#125; else if (oldThr &gt; 0) // 如果oldCap&lt;=0，初始容量为阈值threshold newCap = oldThr; else &#123; // 零初始化阈值表明使用默认值 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;, &quot;unchecked&quot;&#125;) Node&lt;K, V&gt;[] newTab = (Node&lt;K, V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K, V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K, V&gt;)e).split(this, newTab, j, oldCap); //红黑树分裂 else &#123; // 保持原有顺序 Node&lt;K, V&gt; loHead = null, loTail = null; Node&lt;K, V&gt; hiHead = null, hiTail = null; Node&lt;K, V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); //新表索引：hash &amp; (newCap - 1)---》低x位为Index //旧表索引：hash &amp; (oldCap - 1)---》低x-1位为Index //newCap = oldCap &lt;&lt; 1 //举例说明：resize()之前为低x-1位为Index，resize()之后为低x位为Index //则所有Entry中，hash值第x位为0的，不需要哈希到新位置，只需要呆在当前索引下的新位置j //hash值第x位为1的，需要哈希到新位置，新位置为j+oldCap if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。 123456789101112131415161718/** * Transfers all entries from current table to newTable. */void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 我们会发现转移的时候是逆序的。假如转移前链表顺序是1-&gt;2-&gt;3，那么转移后就会变成3-&gt;2-&gt;1。 当我们用线程一调用map.get(11)时，悲剧就出现了——Infinite Loop。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中移物联网一面]]></title>
    <url>%2Fzhongyiwulianwang_interview.html</url>
    <content type="text"><![CDATA[中移物联网一面 不断努力完善自己 1. MapReduce的启动类是什么？ 自己的理解是Java的MapReduce程序如何启动。 例如wordcount： Job的初始化过程1234567891011Job job = new Job(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class);job.setMapperClass(TokenizerMapper.class);job.setCombinerClass(IntSumReducer.class);job.setReducerClass(IntSumReducer.class);job.setOutputKeyClass(Text.class);job.setOutputValueClass(IntWritable.class);FileInputFormat.addInputPath(job, new Path(otherArgs[0]));FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));System.exit(job.waitForCompletion(true) ? 0 : 1); Hadoop Streaming 运行方式： 123456hadoop jar hadoop-streaming-2.4.1.jar \-files mapperPythonScript.py, reducerPythonScript.py-input myInputDirs \ -output myOutputDir \ -mapper mapperPythonScript.py \ -reducer reducerPythonScript.py 用-file分发文件 用-cacheFile分发文件 用-cacheArchive分发压缩包 2. 谈谈你对与Hive的理解？ Hive是一个基于Hadoop文件系统上的数据仓库架构。 它为数据仓库的管理提供了许多功能：数据ETL（抽取、转换、加载）工具、数据存储管理、大型数据集的查询与分析能力、类SQL语言（HQL，允许自定义mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作）。 Hive执行过程： 操作符是Hive的最小处理单元 每个操作符处理代表HDFS操作或MR作业 编译器把HQL转换成一组操作符 Hive通过ExecMapper和ExecReducer来执行MapReduce任务 3. 网站上网页版人工客服窗口后端的实现原理是什么？ ajax交互：使用ajax长轮询实现网页实时聊天。 websocket实现。WebSocket protocol 是HTML5一种新的协议。它实现了浏览器与服务器全双工通信(full-duplex)。 XMLHttpRequest 对象用于在后台与服务器交换数据。 在不重新加载页面的情况下更新网页 在页面已加载后从服务器请求数据 在页面已加载后从服务器接收数据 在后台向服务器发送数据 4. 如何理解父类 = new 子类这样的方式？（乱入下新华三的笔试选择题） 父类 xx = new 子类（）的定义对象 这个问题体现了成员变量继承和方法继承的区别。 在实例化一个子类的同时，系统会给子类所有实例变量分配内存，也会给它的父类的实例变量分配内存，即使父子类中存在重名的实例变量，也会两个都分配内存的，这个时候子类只是隐藏了父类的这个变量，但还是会给它分配内存，然后可以用super来访问属于父类的变量。 父类 xx = new 子类（）定义的对象只能调用继承来的方法。父类 xx = new 子类（）定义的对象调用的是子类的方法，而不是父类的。 总结：访问变量看声明，访问方法看实际对象类型（new出来的类型） 1234567891011121314151617181920212223242526272829303132333435363738public class Test &#123; public static void main(String[] args) &#123; Father a = new Father(); Chilren b = new Chilren(); Father c = new Chilren(); a.getAge(); System.out.println(a.age); b.getAge(); System.out.println(b.age); c.getAge(); System.out.println(c.age); &#125;&#125; class Father &#123; int age = 40; public void getAge() &#123; System.out.println(age); &#125;&#125; class Chilren extends Father &#123; int age = 18; public void getAge() &#123; System.out.println(age); &#125;&#125; 输出 404018181840 5. 面对薪资的问题：你对于薪资要求是否有一个具体的数组？这种问题最好的回答是什么？ 态度：谦逊平和真诚。 做好内功，做一个真正的人才！这是谈薪资最强大的依仗。 当hr问我期望薪资时，我会在回答之前，先咨询公司的薪资结构。 了解一下市场上这个职位的薪酬范围。了解一下这家公司的薪酬情况。然后在一定范围内谈。 询问公司工作时间、加班频率、福利待遇等其他条件，让自己有更多的考虑时间。 掌握了公司的薪酬结构之后，你已经能够据此判断出公司工作的完善程度。那么你可以根据这些信息，结合自己的预期，做一个有利于自己的计算和判断。当然很多信息、预案是需要在面试之前去做收集、准备的。 您这个职位预算区间大概多少？ 一定是让对方先说。薪资谈判中，千万别做先提数字的一方。 面试中薪资的问与答只是面试中展示你自身价值的15%而已，剩余的展示环节更要做足才对，毕竟最终的目的是把自己“卖出去”，顺便“卖个好价钱”。 你不必提具体数字；取而代之，你可以回答一个区间。比如说，“我期望的薪水是月薪7K～10K。” 站在HR的角度，只要期望薪资在岗位预算范围里，就差不多了。那种绕来绕去的面试者，我觉得很傻又很不坦诚。 多看知乎！！！ 参考文章：面试谈工资的时候，对方问你的期望薪资，如果反问对方「根据自己的能力能给多少」，合适吗？]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>中移物联网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[招银科技面试记录]]></title>
    <url>%2Fzhaoyinkeji_interview.html</url>
    <content type="text"><![CDATA[招银科技面试 面试的过程是一个总能发现自己不足的机会，但是平时的不断积累才是避免面试发现不足的更重要的原因。 一面 如何自定义一个异常？ 非运行时异常(Checked Exception) Java中凡是继承自Exception但不是继承自RuntimeException的类都是非运行时异常。 运行时异常（Runtime Exception/Unchecked Exception） RuntimeException类直接继承自Exception类，称为运行时异常。Java中所有的运行时异常都直接或间接的继承自RuntimeException。 Java中所有的异常类都直接或间接的继承自Exception。 Spring事务回滚机制只处理运行时异常，发生非运行时异常则不会回滚操作。 使用自定义异常继承相关的异常来抛出处理后的异常信息可以隐藏底层的异常，这样更安全，异常信息也更加的直观。自定义异常可以抛出我们自己想要抛出的信息，可以通过抛出的信息区分异常发生的位置，根据异常名我们就可以知道哪里有异常，根据异常提示信息进行程序修改。比如空指针异常NullPointException，我们可以抛出信息为“xxx为空”定位异常位置，而不用输出堆栈信息。 在 Java 中你可以自定义异常。编写自己的异常类时需要记住下面的几点。 所有异常都必须是 Throwable 的子类。 如果希望写一个检查性异常类，则需要继承 Exception 类。 如果你想写一个运行时异常类，那么需要继承 RuntimeException 类。 12345678910111213141516171819/** * 所有秒杀相关异常(运行期异常) * spring事务，只接收运行期异常，执行回滚操作 */public class SeckillException extends RuntimeException&#123; private static final long serialVersionUID = 1L; //重载构造函数 public SeckillException(String message, Throwable cause) &#123; super(message, cause); &#125; //重载构造函数 public SeckillException(String message) &#123; super(message); &#125;&#125; 通用方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104/** * @Description:通用的UncheckedException异常，继承RuntimeException，为运行期异常 */public class UncheckedException extends RuntimeException &#123; private static final long serialVersionUID = 1L; /** 错误Key，用于唯一标识错误类型 */ private String errorCode = null; /** 错误信息 */ private String errorMessage; /** 传递给变量的错误值 */ private Object[] errorParam = null; /** * 构造函数 * @param errorCode 异常编码 */ public UncheckedException(String errorCode) &#123; this.errorCode = errorCode; &#125; /** * 构造函数 * @param errorCode 异常编码 * @param errorParam Object[] 异常信息用到的参数 */ public UncheckedException(String errorCode, Object[] errorParam) &#123; this.errorCode = errorCode; this.errorParam = errorParam; &#125; /** * 重载构造函数 * @param errorCode 异常编码 * @param errorParam 异常信息用到的参数 * @param t 异常实例 */ public UncheckedException(String errorCode, Object[] errorParam, Throwable t) &#123; super(t); this.errorCode = errorCode; this.errorParam = errorParam; &#125; /** * 重载构造函数 * @param message 异常信息 * @param t 异常实例 */ public UncheckedException(String message, Throwable t) &#123; super(message, t); setErrorMessage(message); &#125; /** * 异常编码 * @return String */ public String getErrorCode() &#123; return this.errorCode; &#125; /** * 异常信息用到的参数 * @return Object[] */ public Object[] getErrorParam() &#123; return this.errorParam; &#125; /** * 错误信息 * * @return */ public String getErrorMessage() &#123; return errorMessage; &#125; /** * 错误信息 * * @param errorMessage */ public void setErrorMessage(String errorMessage) &#123; this.errorMessage = errorMessage; &#125; /** * 覆盖方法：getMessage * @return String */ @Override public String getMessage() &#123; if (errorMessage != null) &#123; return errorMessage; &#125; //异常信息以资源文件的形式保存，并且支持国际化，此处通过errorCode去读取国际化异常信息 if (errorCode != null &amp;&amp; !errorCode.trim().equals(&quot;&quot;)) &#123; setErrorMessage(AppLang.getLU().getMessage(errorCode, errorParam,Locale.SIMPLIFIED_CHINESE)); &#125; return getErrorMessage(); &#125;&#125; 参考文章： 实际springMVC项目中自定义异常、spring事务与异常的简单应用 Java异常之自定义异常 如何定义一个服务是否具有高可用性？ 计算机系统的可用性定义为：MTTF/(MTTF+MTTR) * 100%。由此可见，计算机系统的可用性定义为系统保持正常运行时间的百分比。 例如某个系统的可用性为4个9（99.99%），相当于系统一年停服务的时间不超过3652460/10000=52.56分钟。系统的可用性往往体现了系统的整体的代码质量和容错能力。 MySQL不同存储引擎的区别？ MySQL有多种存储引擎，每种存储引擎有各自的优缺点，可以择优选择使用：MyISAM、InnoDB、MERGE、MEMORY(HEAP)、BDB(BerkeleyDB)、EXAMPLE、FEDERATED、ARCHIVE、CSV、BLACKHOLE MyISAM与InnoDB的区别是什么？ 存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 存储空间MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 可移植性、备份及恢复MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。 事务支持MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 AUTO_INCREMENTMyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 表锁差异MyISAM：只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 全文索引MyISAM：支持 FULLTEXT类型的全文索引InnoDB：不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 表主键MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。 表的具体行数MyISAM：保存有表的总行数，如果select count() from table;会直接取出出该值。InnoDB：没有保存表的总行数，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 CURD操作MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。 外键MyISAM：不支持InnoDB：支持 通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储过程、视图、行级锁定等等，在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。 参考文章：MySQL存储引擎中的MyISAM和InnoDB区别详解 存储过程使用场景？ 存储过程有利有弊，最大的利处在于运算过程中不需要将数据取出数据库，会获得较高的性能，因为数据库IO通道效率一向很差，大量数据取出来很费时间。 开发规范中要求，所有牵涉到业务逻辑部分的数据库操作，必须在数据库层由存储过程实现（就所面向的金融行业而言，工商银行、中国银行、交通银行，都在开发规范中严格指定） 存储过程优点： 执行速度快。因为存储过程不需要解析。预先编译了 安全性。避免了sql注入，避免了暴露表结构和字段 存储过程缺点： 调试困难。 迁移困难。 针对于分布式数据库，如果业务逻辑封装在存储过程中。由于存储过程是依赖于某个具体的数据库的，这样实现起来难度较大。把计算层放到web层，那么web端的程序只是从多个数据库服务器聚合数据即可了。 敏捷开发在互联网公司业务开发中非常常见，而银行业，金融业不会频繁的修改功能。 解决性能瓶颈，互联网大部分还是通过使用缓存来解决的，这跟银行类系统”花费大价钱提高硬件和数据库软件的承载能力”思路是不同的。 参考文章： 大型系统必须得要存储过程和触发器吗？ 在开发过程中为什么需要写存储过程 非常好的文章！ 如何使用日志？ Java日志框架： JDK的java.util.logging包 第三方日志工具（commons-logging/slf4j，log4j/logback） 通过Logger.getLogger(name)方法可以获取logger对象，logger对象有三个比较重要的概念：level、handler、formatter。level称为日志级别，在java.util.logging包中定义了java.util.logging.Level类，里面包含SEVERE/WARNING/INFO/CONFIG/FINE/FINER/FINEST（从高到低）7种日志级别。设置日志级别会过滤掉一部分日志，例如当日志级别设置为INFO级别时，CONFIG/FINE/FINER/FINEST级别的日志就会被忽略。handler解决的问题是日志输出到哪里，是到控制台（java.util.logging.ConsoleHandler），还是到文件（java.util.logging.FileHandler），或者是写到Socket中（java.util.logging.SocketHandler）。formatter定义了日志输出的格式，可以是XML（java.util.logging.XMLFormatter），也可以自己实现JSON格式的Fomatter。 log对象初始化过程 SL4J/COMMONS-LOGGING、LOG4J、LOGBACK、JUL都是常用的Java日志工具，基本思想都是通过Factory生成logger对象，然后由LogManager管理/缓存logger对象，同时维护一个父子结构方便复用配置。logger对象包含三要素：日志级别、输出到哪里、格式化。 参考文章：Java日志 (zhuan) 为什么选择这个岗位？ 二面 针对于每日产出文件，由于数据不完整的重新产出，如何比较两个文件不一样的内容？ Linux方式：12345678diff 123.txt 234.txt通常输出由下述形式的行组成：n1 a n3，n4n1，n2 d n3n1，n2 c n3，n4 这些行类似ed命令把filel转换成file2。字母(a、d和c)之前的行号(n1，n2)是针对file1的，其后面的行号(n3，n4)是针对file2的。字母a、d和c分别表示附加、删除和修改操作。在上述形式的每一行的后面跟随受到影响的若干行，以”&lt;”打头的行属于第一个文件，以”&gt;”打头的行属于第二个文件。 注意：在使用diff命令时候，也需要预先对文件进行sort排序，不然输出可能出错。 linux下MD5计算： 12:~/maling/test$ md5sum 234.txt 8fb63e502ae6ad0c6268c128f559e7fd 234.txt Java方式：1) 先比较两个文件内容的长度；2) 在长度相同的情况下，再比较两个文件的MD5值。 MD5，Message Digest Algorithm 5，是一种被广泛使用的信息摘要算法，可以将给定的任意长度数据通过一定的算法计算得出一个 128 位固定长度的散列值。 三面 为什么你的定位是业务偏数据？ 其实这样的感觉是自己技术深度不够造成的，需要完善自己的技术深度。 未来的时代是数据的时代，目前的自己还需要从业务做起熟悉流程。 题外话：金融型IT人才这类的复合型人才，职业生涯周期会更长。 认同这个观点，懂金融的不懂IT正常，做金融IT的不仅懂金融，还必须更懂金融（偏业务的金融），相对于专业的金融人才，还需要补充自己的专业性。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>zhaoyinkeji</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[滴滴现场面试记录]]></title>
    <url>%2Fdidi_interview.html</url>
    <content type="text"><![CDATA[滴滴现场面试记录 现在写下的感受：完成了一场酣畅淋漓的运动之后的感觉，累然而爽。 一面 操作系统进程的状态，注意不是线程！ 进程状态反映进程执行过程的变化。这些状态随着进程的执行和外界条件的变化而转换。在三态模型中，进程状态分为三个基本状态，即运行态，就绪态，阻塞态。在五态模型中，进程分为新建态、终止态，运行态，就绪态，阻塞态。 运行状态：获得CPU的进程处于此状态，对应的程序在CPU上运行着 阻塞状态：为了等待某个外部事件的发生（如等待I/O操作的完成，等待另一个进程发来消息），暂时无法运行。也称为等待状态。 就绪状态：具备了一切运行需要的条件，由于其他进程占用CPU而暂时无法运行 五态模型：对于一个实际的系统，进程的状态及其转换更为复杂。引入新建态和终止态构成了进程的五态模型。 致使进程阻塞的典型事件有：请求I/O，申请缓冲空间等。通常将这种处于阻塞状态的进程也排成一个队列。有的系统则根据阻塞原因的不同而把处于阻塞状态的进程排成多个队列。 TCP三次握手详细画图，为什么每一次要加一？最后一次需不需要加一？ TCP 头部格式： Source Port和Destination Port:分别占用16位，表示源端口号和目的端口号；用于区别主机中的不同进程，而IP地址是用来区分不同的主机的，源端口号和目的端口号配合上IP首部中的源IP地址和目的IP地址就能唯一的确定一个TCP连接； Sequence Number:用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号；主要用来解决网络报乱序的问题； Acknowledgment Number:32位确认序列号包含发送确认的一端所期望收到的下一个序号，因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题； Offset:给出首部中32 bit字的数目，需要这个值是因为任选字段的长度是可变的。这个字段占4bit（最多能表示15个32bit的的字，即4*15=60个字节的首部长度），因此TCP最多有60字节的首部。然而，没有任选字段，正常的长度是20字节； TCP Flags:TCP首部中有6个标志比特，它们中的多个可同时被设置为1，主要是用于操控TCP的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下： URG：此标志表示TCP包的紧急指针域（后面马上就要说到）有效，用来保证TCP连接不被中断，并且督促中间层设备要尽快处理这些数据； ACK：此标志表示应答域有效，就是说前面所说的TCP应答号将会包含在TCP数据包中；有两个取值：0和1，为1的时候表示应答域有效，反之为0； PSH：这个标志位表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队； RST：这个标志表示连接复位请求。用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包； SYN：表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0；连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有SYN的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行TCP三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行TCP的三次握手； FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。 Window:窗口大小，也就是有名的滑动窗口，用来进行流量控制； 第二次和第三次握手有：Acknowledgment Number为x+1(Sequence Number+1) 第二次：服务端响应客户端的请求，响应中附带序列号0（由于这是服务端在该次TCP会话中发送的第一个包，所以序列号为0）和相对确认号1（表明服务端收到了客户端发送的包1中的SYN） 需要注意的是，尽管客户端没有发送任何有效数据，确认号还是被加1，这是因为接收的包中包含SYN或FIN标志位（并不会对有效数据的计数产生影响，因为含有SYN或FIN标志位的包并不携带有效数据） 第三次：和包2中一样，客户端使用确认号1响应服务端的序列号0，同时响应中也包含了客户端自己的序列号（由于服务端发送的包中确认收到了客户端发送的SYN，故客户端的序列号由0变为1）此时，通信的两端的序列号都为1，通信两端的序列号增1发生在所有TCP会话的建立过程中。 这张图说明了一切： 参考文章：理解TCP序列号（Sequence Number）和确认号（Acknowledgment Number） 排序算法 概率题 划重点：详细介绍运维所做的事情，草稿纸找不到了，非常感谢一面面试官！ 首先是服务层的稳定性，运维是作为服务上线最后一道把关人，确保服务的高可用性。 再是系统资源层面的稳定性，包括机器的CPU占用，I/O读取等稳定。 然后是系统的高效性。利用docker等虚拟化等技术来节约资源。 … 二面 针对实习项目，展开优化策略： 后端请求打到相同机器如何进行优化？ 网上找到这样的图，描绘的很形象：将请求放入队列中的，采用FIFO（First Input First Output，先进先出） 系统处理完一个队列内请求的速度根本无法和疯狂涌入队列中的数目相比。也就是说，队列内的请求会越积累越多，最终Web系统平均响应时候还是会大幅下降，系统还是陷入异常。 乐观锁 如何Hash分散请求？ 一致性哈希。 一致性哈希，其实就是把哈希函数可映射的空间(相当于普通哈希中桶的数目是固定的)固定下来了，比如固定为: 2^n - 1，并组织成环的形状。每个机器对应着一个n位的ID，并且映射到环中。每个查询键，也是一个 n 位的ID，节点的ID和查询键对应着相同的映射空间。 一致性哈希的好处就是：当添加新机器或者删除机器时，不会影响到全部数据的存储，而只是影响到这台机器上所存储的数据(落在这台机器所负责的环上的数据)。 步骤： 将整个哈希值空间组织成一个虚拟圆环，假设某哈希函数H的值空间为0-(2^32-1)，即32位无符号整数 将各节点用H函数哈希，可以将服务器的IP或主机名作为关键字哈希，这样每个节点就能确定其在哈希环上的位置 将id用H函数映射到哈希空间的一个值，沿该值向后，将遇到的第一个节点做为处理节点 引入虚拟节点，可以有效地防止物理节点(机器)映射到哈希环中出现不均匀的情况。 划重点：普及AIOPs概念，详细介绍Google，百度在AIOPs的尝试以及最近运维大会讨论的主题，非常感谢二面面试官！ 目前是自动化运维：DevOpsAIOps应用场景较少，主要在于异常检测运维的稳定性是最重要的，但是AIOps的模型训练的试错成本很高，这是一个矛盾。Google:SRE；百度运维；阿里运维。国内做的比较好的~ SRE是Site Reliability Engineer的简称，从名字可以看出Google的SRE不只是做Operation方面的工作，更多是保障整个Google服务的稳定性。SRE需要负责可用性、时延、性能、效率、变更管理、监控、应急响应和容量管理等相关的工作。 三面 三面面试官在深度细节方面让自己更加了解自己的不足 给定数组，乱序，重复，如何找到重复次数超过数组长度一半的数字？ 参考文章：数组中出现次数超过一半的数字 数组排序后，如果符合条件的数存在，则一定是数组中间那个数。 采用阵地攻守的思想：第一个数字作为第一个士兵，守阵地；count = 1；遇到相同元素，count++;遇到不相同元素，即为敌人，同归于尽,count–；当遇到count为0的情况，又以新的i值作为守阵地的士兵，继续下去，到最后还留在阵地上的士兵，有可能是主元素。再加一次循环，记录这个士兵的个数看是否大于数组一半即可。 1234567891011121314151617181920212223242526272829303132333435public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; int length=array.length; if(array==null||length&lt;=0)&#123; return 0; &#125; int result=array[0]; int times=1; for(int i=1;i&lt;length;i++)&#123; if(times==0)&#123; result=array[i]; times=1; &#125;else&#123; if(array[i]==result)&#123; times++; &#125;else&#123; times--; &#125; &#125; &#125; times=0; for(int i=0;i&lt;length;i++)&#123; if(result==array[i])&#123; times++; &#125; &#125; if(times*2&lt;length)&#123; result=0; &#125; return result; &#125;&#125; map方法： 123456789101112131415161718192021222324import java.util.HashMap;import java.util.Map;/* * 利用map存值，找出存在最多的数字，若大于长度一半，返回此数，否则返回0 */public class Solution &#123; public int MoreThanHalfNum_Solution(int [] array) &#123; if(array.length==0||array==null) return 0; Map&lt;Integer,Integer&gt; map=new HashMap&lt;Integer,Integer&gt;(); for(int i=0;i&lt;array.length;i++)&#123; if(map.containsKey(array[i]))&#123; map.put(array[i], map.get(array[i])+1); &#125;else&#123; map.put(array[i], 1); &#125; &#125; for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; if(entry.getValue()&gt;array.length/2) return entry.getKey(); &#125; return 0; &#125;&#125; 为什么要这样设计？时间复杂度是多少？还有没有别的方法？ 最优O(n) 8个球其中一个质量轻，给一个天平，如何2次称出质量轻的球？ 第一次,天平两边各放三个球： 1.1 如果两边重量一致,则有质量缺陷的球在剩余的两个中—转至“2.2” 1.2 如果两边重量不一致,则有质量缺陷的球在较轻重量的三个中—-转至“2.1” 第二次： 2.1 在较轻的三个中取两个放在天平两边,如天平平衡,那么第三个球是有质量缺陷的球；如果不平衡,那么,较轻的是有质量缺陷的球. 2.2 将剩余的两个球放在天平两边,较轻的是有质量缺陷的. 你眼中的运维？ 运维服务于整个产品，保证架构合理，系统稳定。运维只对业务稳定负责，所有的工作都是奔着这个去的。 参考文章：一个开发眼中的运维 你认为自己的优势是什么？ 自信。 HR面 “自己缺乏的是对于对方表达意思的深刻理解”，非常高兴能够从HR面试中发现自己的不足，虽然这会影响到HR对于自己的评价，但是自己还是明白了。 如何沟通？ 首先是换位思考，明白提问者想要的结果；其次是确认，确认自己的理解正确；最后总结自己的回答。 如何克服困难？ 问题在于克服的方式。通过工具？通过合作？等待，具体事例辅佐。 什么时候压力大？来源是什么？ 问题不在克服问题的流程，在于压力来源原因的分析，最后辅以客服的案例。 为什么选择运维？然而你的经历是后端？ 今天的经历告诉自己，缘分。哈哈~ 自己的问的问题。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>didi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网易游戏笔试记录]]></title>
    <url>%2Fneteasegame_writeexam.html</url>
    <content type="text"><![CDATA[网易游戏笔试记录 看到试卷要求，有句话，基础部分达不到及格线直接淘汰，基础细节最重要，太多细节没有掌握。 基础知识1. 线程1执行n++;n++; 线程2执行n+=2; 在没有锁的保证下，最后n的结果可能是几？ i++的线程安全性可以总结如下：1) 如果i是局部变量，那么是可重入的，也就是线程安全的。2) 如果i是全局变量，则同一进程的不同线程都可能访问到该变量，因而是线程不安全的。 i++不是原子操作，原因是： i++操作分为三个阶段(读、改、写)： 1) 内存到寄存器2) 寄存器自增3) 写回内存 这三个阶段可以被中断分离。 情况1：两个线程串行执行，无交叉，结果为4。 情况2：线程1执行n++后被打断，线程2开始执行，于是线程开始执行n+2，再将结果3写回内存前被打断，执行线程1的第二个n++，这个时候内存中的n为2，然后线程2开始执行写回3，结果是3。 情况3：线程2先执行取得n为0后开始运算加2操作，之后在将结果写回内存前被打断，执行线程1，线程1执行结束n的内存为2，这个时候线程2被恢复，将运算结果2再次写回内存，最后结果是2。 参考文章： 2. 数据库ACID 事务(transaction)所应该具有的四个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。 原子性是指事务是一个不可再分割的工作单位，事务中的操作要么都发生，要么都不发生。 一致性是指在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。 多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。 持久性，意味着在事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 3. 哪一种数据结构是删除最小元素和添加新元素的时间复杂度是O(logn)？ 小根堆数据结构。删除和添加本身是O（1），调整是整个树的深度，为O（logN） 4. 关于分页和分段？ 分页就是将进程的逻辑地址空间分成若干大小相等的片（即页），然后装入内存。 分段就是用户可以把自己的作业按逻辑关系划分为若干个段,每个段都是从0开始编址,并有自己的名字和长度。这就相当于程序里边的主函数段、各个子函数段、数据段、栈段等等。 页是信息的物理单位，段是信息的逻辑单位。 分页的作业地址空间是一维的,线性的,程序员只需利用一个记忆符表示一个地址；而分段的作业地址空间是二维的,程序员在表示一个地址的时候既要给出段名,又需要给出段内地址.其中,段名可以理解为函数名等，段内地址可以理解程变量等的地址。 5. 采取LRU策略，缺页次数？ 在请求分页系统中，可以通过查询页表中的状态位来确定所要访问的页面是否存在于内存中。每当所要访问的页面不在内存时，会产生一次缺页中断，此时操作系统会根据页表中的外存地址在外存中找到所缺的一页，将其调入内存。 LRU置换算法是选择最近最久未使用的页面予以淘汰。该算法赋予每个页面一个访问字段，用来记录一个页面上次被访问以来所经历的时间t，当需要淘汰一个页面时，选择现有页面中其t值最大的。 123456789101112131415161718以下用x表示缺页的情况1.S=33,4,2,1,4,5,3,4,5,1,2---------------------3 4 2 1 4 5 3 4 5 1 2 3 4 2 1 4 5 3 3 5 1 3 4 2 1 4 5 4 3 5x x x x x x x x所以F=82.S=43,4,2,1,4,5,3,4,5,1,2---------------------3 4 2 1 4 5 3 4 5 1 2 3 4 2 1 4 5 3 3 3 1 3 4 2 1 4 5 4 5 3 3 3 2 1 1 1 4 5x x x x x x x所以F=7 6. 如何统计指定目录下只包含小写字母的后缀为c的文件的个数？ 没有子目录：123ls *.txt | wc -lfind . -name &quot;*.txt&quot; | wc -l 子目录：1find . -type f -name &quot;*.txt&quot; | wc -l 7. 如何更改指定目录以及其子文件的拥有者名字？ 对Document/ 目录下的所有文件与子目录执行相同的所有者变更，修改所有者为users用户组的username用户12chown -R users Document/PS: 必须是R不是r 8. echo ‘2’.print(2)+3; 的输出结果是什么？ 首先计算的是 右边print(2)+3，这个你可以直接理解成print(2+3),得到的结果是5。而print是一个函数，它的返回值总是1。 第二步就是echo ‘2’.print(‘结果’)(返回值是1)，因此会得到21的结果。 第三步就是将之前计算的结果进行连接，并最终进行输出，得到的结果就是521了。 参考文章：echo ‘1’.print(2)+3; 的输出结果为什么是511 9. Dajano查询优化函数？ 在数据库有外键的时候，使用 select_related() 和 prefetch_related() 可以很好的减少数据库请求的次数，从而提高性能。 对于一对一字段（OneToOneField）和外键字段（ForeignKey），可以使用select_related 来对QuerySet进行优化。 select_related使用SQL的JOIN语句进行优化，通过减少SQL查询的次数来进行优化、提高性能。 prefetch_related()的解决方法是，分别查询每个表，然后用Python处理他们之间的关系。 10. nginx查询日志每秒IP访问次数 nginx access.log 日志格式： 12345678910111213141516100.109.195.91 - - [17/Feb/2017:00:08:11 +0800] &quot;GET /data/upload/shop/common/loading.gif HTTP/1.0&quot; 200 134 &quot;http://www.mall121.com/&quot; &quot;Mozilla/4.0 (compatible; MSIE 8.0; Trident/4.0; Windows NT 6.1; SLCC2 2.5.5231; .NET CLR 2.0.50727; .NET CLR 4.1.23457; .NET CLR 4.0.23457; Media Center PC 6.0; MS-WK 8)&quot; &quot;140.205.201.12&quot; 1.客户端（用户）IP地址。2.访问时间。3.访问端口。4.响应时间。5.请求时间。6.用户地理位置代码（国家代码）。7.请求的url地址（目标url地址）的host。8.请求方式（GET或者POST等）。9.请求url地址（去除host部分）。10.请求状态（状态码，200表示成功)。11.请求页面大小，默认为B（byte）。12.来源页面，即从哪个页面转到本页，专业名称叫做“referer”。13.用户浏览器语言。如：上例中的 &quot;es-ES,es;q=0.8&quot;14.用户浏览器其他信息，浏览器版本、浏览器类型等。 根据访问IP统计UV1awk &apos;&#123;print $1&#125;&apos; access.log|sort | uniq -c |wc -l 统计访问URL统计PV1awk &apos;&#123;print $7&#125;&apos; access.log|wc -l 查询访问最频繁的URL1awk &apos;&#123;print $7&#125;&apos; access.log|sort | uniq -c |sort -n -k 1 -r|more 查询访问最频繁的IP1awk &apos;&#123;print $1&#125;&apos; access.log|sort | uniq -c |sort -n -k 1 -r|more 11. POST对于Get的5个优点？ Post编码类型：application/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 Post对数据长度无限制。 Post对对数据类型无限制，也允许二进制数据。 POST 比 GET 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。 Post数据不会显示在 URL 中。 12. Java实现tail -f？ 设计思路： 利用FileInputStream的skip方法，可以跳过文件的部分内容，实现tail命令的读取最后几行的功能。利用while(true)循环，实现tail命令的-f功能，读取后续写入的内容。 利用RandomAccessFile，在代码中记录文件先前的大小，定时查看文件大小是否发生变化，如果是，则读取新的内容部分（利用RandomAccessFile的seek方法跳过之前已经读取的内容）。 12345678910111213141516171819202122232425262728293031323334String srcFilename = &quot;E:\\tmp2\\1.txt&quot;; String charset = &quot;GBK&quot;; File file = new File(srcFilename); InputStream fileInputStream = new FileInputStream(srcFilename); fileInputStream.skip(1); // skip n bytes if needed, filesystem may position to the offset directly but not read really. BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(fileInputStream, charset)); String singleLine; long fileSize = file.length(); while (true) &#123; try &#123; if ((singleLine = bufferedReader.readLine()) != null) &#123; System.out.println(singleLine); fileSize = Math.max(file.length(), fileSize); continue; &#125; &#125; catch (IOException e) &#123; // 文件被清空的时候FileInputStream会被close bufferedReader = new BufferedReader(new InputStreamReader(new FileInputStream(srcFilename), charset)); fileSize = file.length(); &#125; try &#123; if (file.length() &lt; fileSize) &#123; // 文件被清空了 bufferedReader.close(); bufferedReader = new BufferedReader(new InputStreamReader(new FileInputStream(srcFilename), charset)); fileSize = file.length(); &#125; Thread.sleep(1000L); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); bufferedReader.close(); break; &#125; &#125; 参考文章： Java实现linux的“tail -f”命令 用RandomAccessFile实现linux tail命令效果 13. 数据库索引查询速度快的原因？ 索引就是通过事先排好序，从而在查找时可以应用二分查找等高效率的算法。 索引B+树，顺序存储，查询时间复杂度O(logn) 等于查一个比较小的表。 14. Java中浮点数0.1 + 0.2 ！= 0.3。如何比较浮点数大小 根据IEEE 754，单精度的float是32位，双精度的double为64位。 1) 第一部分(s)为符号位，第二部分(exponent)为指数位，第三部分(mantissa)为基数部分。 这是科学计数法的二进制表示。那么，既然位数是固定的，要表示像 1/3=0.3333333333333…或者pi=3.1415926….. 这样的无限循环小数，就变得不可能了。根据规范，则需要将不能标识的部分舍掉。 2) 还与10进制不同的是，二进制对于一些有限的小数，也不能精确的标示。比如像0.1这样的小数，用二进制也无法精确表示。所以，也需要舍掉。关于0.1无法用二进制精确表示 在进行浮点数比较的时候，主要需要考虑3个因素 NaN 无穷大/无穷小 舍入误差 排除NaN和无穷、在精度范围内进行比较 123456public boolean isEqual(double a, double b) &#123; if (Double.isNaN(a) || Double.isNaN(b) || Double.isInfinite(a) || Double.isInfinite(b)) &#123; return false; &#125; return (a - b) &lt; 0.001d; &#125; BigDecimal的解决方案就是，不使用二进制，而是使用十进制（BigInteger）+小数点位置(scale)来表示小数。BigDecimal这个类。它什么都好，就是效率略低。使用BigDecimal的时候，要么使用newBigDecimal(String value)，要么使用BigDecimal.valueof(doublevalue)。 参考文章：Java中的浮点数比较 15. 如何设计一个抢红包系统，高可用，高并发，容灾机制 参考文章：百亿级微信红包的高并发资金交易系统设计方案]]></content>
      <categories>
        <category>笔试记录</category>
      </categories>
      <tags>
        <tag>netease</tag>
        <tag>exam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[猎豹移动二面]]></title>
    <url>%2Fcheetah_mobile_interview_2.html</url>
    <content type="text"><![CDATA[猎豹移动二面 问题回答只会回答基本面，细节不充分是自己的弱项 前面的有些忘记了，，，{[(-_-)(-_-)]}zzz1. Tornado框架介绍 Tornado是使用Python编写的一个强大的、可扩展的Web服务器。它在处理严峻的网络流量时表现得足够强健，但却在创建和编写时有着足够的轻量级，并能够被用在大量的应用和工具中。 基本流程： 执行脚本，监听 8888 端口 浏览器客户端访问 /index –&gt; http://127.0.0.1:8888/index 服务器接受请求，并交由对应的类处理该请求 类接受到请求之后，根据请求方式（post(请求) / get（获取） / delete（删除） / pot（设置））的不同调用并执行相应的方法 方法返回值的字符串内容发送浏览器 example12345678910111213141516171819#!/usr/bin/env python# -*- coding:utf-8 -*- import tornado.ioloopimport tornado.web class MainHandler(tornado.web.RequestHandler): def get(self): self.write(&quot;Hello, world&quot;) application = tornado.web.Application([ (r&quot;/index&quot;, MainHandler),]) if __name__ == &quot;__main__&quot;: application.listen(8888) tornado.ioloop.IOLoop.instance().start() 2. Supervisor功能作用 supervisor是用python开发的进程管理程序，可以将普通的命令变成后台的守护进程，并监控进程状态。 使用流程： 安装supervisor 编写应用程序配置文件 1234[program:app]command=/usr/bin/gunicorn -w 1 wsgiapp:applicationdirectory=/srv/wwwuser=www-data 重启supervisor，让配置文件生效 1# supervisorctl start app 3. nginx如何实现请求转发 反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。 nginx 配置文件：vim /etc/nginx/nginx.conf 例如tornado会启动三个进程，分别侦听8001, 8002, 8003三个端口。 12345upstream tornado.server &#123; server 127.0.0.1:8001; server 127.0.0.1:8002; server 127.0.0.1:8003;&#125; 12345678# 反向代理配置 location / &#123; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://tornado.server; &#125; upstream指令主要用于负载均衡，设置一系列的后端服务器。nginx 的 upstream默认是以轮询的方式实现负载均衡，这种方式中，每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。此外还有ip_hash、weight、url_hash、fair等负载均衡策略。 4. Hadoop的MapReduce运行过程中，如果存在某些key过于集中，是否会使得MapReduce任务执行失败？为什么？ mapreduce处理过程有一个特点，相同的key，只能是发给同一个reduce进行处理。 原因：hadoop源码中有一行代码，(key.hashcode())%numReduce，先把key进行hash然后除以reduce的个数取余，相同的key的hashcode肯定一样，而且reduce数也确认，那肯定是相同的key都发给了同一个reduce了。 而对于提交的作业(job)什么时候才算运行结束呢?是要等到最后一个reduce执行完，才算结束。所以执行量小的reduce先执行完了，也得等着，等着量最大的、最慢的执行完，才能够算结束。 5. 你对于NoSQL的了解？ NoSQL(NoSQL = Not Only SQL )，意即”不仅仅是SQL”。 CAP理论。 NoSQL数据库分类 类型 部分代表 特点 列存储 Hbase 顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。 Cassandra Hypertable 文档存储 MongoDB 文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有有机会对某些字段建立索引，实现关系数据库的某些功能。 CouchDB key-value存储 Tokyo Cabinet / Tyrant 可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能） Berkeley DB MemcacheDB Redis 图存储 Neo4J 图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。 FlockDB 对象存储 db4o 通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。 Versant xml数据库 Berkeley DB XML 高效的存储XML数据，并支持XML的内部查询语法，比如XQuery,Xpath。 BaseX 6. Redis key过期如何设置？ EXPIRE key seconds 为给定 key 设置生存时间，当 key 过期时(生存时间为 0 )，它会被自动删除。 可以对一个已经带有生存时间的 key 执行 EXPIRE 命令，新指定的生存时间会取代旧的生存时间。 TTL key # 查看剩余生存时间 如果没有设置时间，那缓存就是永不过期。 PERSIST key 移除给定 key 的生存时间，将这个 key 从『易失的』(带生存时间 key )转换成『持久的』(一个不带生存时间、永不过期的 key )。 7. Redis key过期有哪几种？ 被动删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key 主动删除：由于惰性删除策略无法保证冷数据被及时删掉，所以Redis会定期主动淘汰一批已过期的key 当前已用内存超过maxmemory限定时，触发主动清理策略 定时删除 含义：在设置key的过期时间的同时，为该key创建一个定时器，让定时器在key的过期时间来临时，对key进行删除 优点：保证内存被尽快释放 缺点：若过期key很多，删除这些key会占用很多的CPU时间，在CPU时间紧张的情况下，CPU不能把所有的时间用来做要紧的事儿，还需要去花时间删除这些key定时器的创建耗时，若为每一个设置过期时间的key创建一个定时器（将会有大量的定时器产生），性能影响严重 惰性删除 含义：key过期的时候不删除，每次从数据库获取key的时候去检查是否过期，若过期，则删除，返回null。 优点：删除操作只发生在从数据库取出key的时候发生，而且只删除当前key，所以对CPU时间的占用是比较少的，而且此时的删除是已经到了非做不可的地步（如果此时还不删除的话，我们就会获取到了已经过期的key了） 缺点：若大量的key在超出超时时间后，很久一段时间内，都没有被获取过，那么可能发生内存泄露（无用的垃圾占用了大量的内存） 定期删除 含义：每隔一段时间执行一次删除过期key操作 优点：通过限制删除操作的时长和频率，来减少删除操作对CPU时间的占用–处理”定时删除”的缺点。定期删除过期key–处理”惰性删除”的缺点 缺点:在内存友好方面，不如”定时删除”;在CPU时间友好方面，不如”惰性删除” 8. Shell常用命令你用过哪些？ ls: 类似于dos下的dir命令; cd: 用于切换用户当前工作目录; pwd：用于显示用户当前工作目录; midir：创建目录;rmdir：删除目录; cp：复制命令; mv：移动命令; rm：用于删除文件; du：显示目前的目录所占用的磁盘空间;df：显示目前磁盘剩余空间; cat：显示或连接一般的ascii文本文件; head：显示文件的头n行;tail：显示文件的尾n行，缺省情况n都为10行; wc：统计指定文件中的字节数(-c)、字数(-w)、行数(-l); grep：用于从文件面搜索包含指定模式的行并打印出来，它是一种强大的文本搜索工具，支持使用正则表达式搜索文本; 管道：利用Linux所提供的管道符“|”将两个命令隔开，管道符左边命令的输出就会作为管道符右边命令的输入; awk：awk比sed强的地方在于不仅能以行为单位还能以列为单位处理文件。awk ‘{pattern + action}’ {filenames} sed：(关键字: 编辑) 以行为单位的文本编辑工具。基本工作方式: sed [-nef] ‘[动作]’ [输入文本] ln -sf a b：b链向a; 1. 文本，每一行五个列，根据第二列排重，统计之后，倒序输出，取前100个。 命令：1cat 0916.txt | awk &apos;&#123;print $2&#125;&apos;| uniq -c | sort -r | awk &apos;&#123;print $2,$1&#125;&apos; 输入：123456789101112131415161718192021221 1 1 1 12 2 2 2 23 3 3 3 34 4 4 4 44 4 4 4 44 4 4 4 44 4 4 4 45 5 5 5 55 5 5 5 55 5 5 5 55 5 5 5 55 5 5 5 55 5 5 5 55 5 5 5 55 5 5 5 55 5 5 5 55 5 5 5 56 6 6 6 66 6 6 6 66 6 6 6 66 6 6 6 66 6 6 6 6 输出：1234567$ cat 0916.txt | awk &apos;&#123;print $2&#125;&apos;| uniq -c | sort -r | awk &apos;&#123;print $2,$1&#125;&apos;6 54 43 12 11 15 10]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>Cheetah-Mobile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[招银科技一面]]></title>
    <url>%2F%E2%80%9Czhaoyinkeji_interview%E2%80%9D.html</url>
    <content type="text"><![CDATA[招银科技一面 主要是基础知识，有些细节自己没有掌握牢固，温故知新 基础知识1. 你了解过Java的序列化吗？ Java 对象序列化是 JDK 1.1 中引入的一组开创性特性之一，用于作为一种将 Java 对象的状态转换为字节数组，以便存储或传输的机制，以后，仍可以将字节数组转换回 Java 对象原有的状态。实际上，序列化的思想是 “冻结” 对象状态，传输对象状态（写到磁盘、通过网络传输等等），然后 “解冻” 状态，重新获得可用的 Java 对象。 2. Java序列化的用途以及使用场景？ 1.当对象需要被网络传输时2.对象状态需要被持久化时 围绕根本，就有很多实际的拓展应用，例如： tomcat服务器会在服务器关闭时把session序列化存储到tomcat目录一个名为session.ser的文件中，这个过程成为session的钝化，因为有些时候当我们要重新部署项目的时候，有的用户可能在访问，这样做的目的是服务器重启之后tomcat可以反序列化这个session.ser文件，将session对象重新生成出来，用户可以使用部署之前的session进行操作，这个反序列化的过程成为session的活化。 分布式应用。 3. 类序列化需要实现什么接口？ Serializable接口。 4. 该类实现了这个接口，那么子类可以序列化吗？ 父类实现了Serializable，子类不需要实现Serializable接口。 5. 当序列化后修改serialVersionUID，还可以反序列化回来吗? 序列化的时候系统会把当前类的serialVersionUID 写入序列化的文件中（也可能是其他中介），当反序列化的时候系统会去检测文件中的serialVersionUID，看它是否和当前类的serialVersionUID一致，如果一致就说明序列化的类的版本和当前类的版本是相同的，这个时候可以成功反序列化，否则就说明当前类和序列化的类相比发生了某些变换，比如成员变量的数量，类型可能发生了改变，这个时候就会抛异常，反序列化失败。 默认情况下，也就是不声明serialVersionUID属性情况下，系统会按当前类的成员变量计算hash值并赋值给serialVersionUID。声明serialVersionUID，可以很大程度上避免反序列化过程的失败。比如当版本升级后，我们可能删除了某个成员变量，也可能增加了一些新的成员变量，这个时候我们的反序列化依然能够成功，程序依然能够最大程度地恢复数据，相反，如果不指定serialVersionUID，程序就会挂掉。当然我们还要考虑另外一种情况，如果类结构发生了非常规性改变，比如修改了类名，类型等，这个时候尽管serialVersionUID验证通过了，但是反序列化过程还是会失败，因为类结构有了毁灭性的改变。 serialVersionUID一般默认设置为固定的 1L。 Java序列化允许java类中的一些变化，如果他们可以被忽略的话。一些不会影响到反序列化处理的变化有： 在类中添加一些新的变量。 将变量从transient转变为非tansient，对于序列化来说，就像是新加入了一个变量而已。 将变量从静态的转变为非静态的，对于序列化来说，就也像是新加入了一个变量而已。 总结，serialVersionUID不一致，反序列化报错。对象序列化后更改值同样也可以读取到，序列化并不保存静态变量，可以接受一些类变量的增加操作不影响反序列化。 6. Java序列化相对于其他数据交换的方式有哪些缺点？ Java序列化写入不仅是完整的类名，也包含整个类的定义，包含所有被引用的类。一旦类定义较多，空间占用大。同时Java序列化为二进制对文件，序列化后内容都是不可读的，这会对系统的排错造成一定的影响。优点是转换效率高。 JSON以固定的格式，稳定简单的数据结构大大简化了序列化过程。缺点是转换效率低。 7. classloader加载类的机制是什么？ 遵循委派双亲加载。通过调用loadClass方法逐级往上传递委派加载请求，当找不到父ClassLoader时调用其findClass方法尝试进行查找和加载，如果当前ClassLoader找不所需的Class,则由其孩子尝试进行查找和加载，如果当前ClassLoader找了所需的Class则将该Class按请求路径逐级返回孩子。 8. 你的意思是jar包中的类不一定会全部加载？系统提供(例如JDK、String)的类是由哪一个加载器加载的？EXT加载哪些类？如果我自定义一个类加载区，它的父加载区是哪一个？ Java动态加载。Bootstrap ClassLoader。EXT加载JRE\lib\ext*.jar。拥堵自定义的类加载器的父类为系统加载器。 JVM本身包含了一个ClassLoader称为Bootstrap ClassLoader，和JVM一样，BootstrapClassLoader是用本地代码实现的，它负责加载核心JavaClass(即所有java.*开头的类)。——Load JRE\lib\rt.jar 另外JVM还会提供两个ClassLoader，它们都是用Java语言编写的，由BootstrapClassLoader加载;其中Extension ClassLoader负责加载扩展的Javaclass(例如所有javax.*开头的类和存放在JRE的ext目录下的类) ApplicationClassLoader负责加载应用程序自身的类。 9. classLoad加载class文件一般分为哪几个阶段？ 当使用到某个类，但该类还未初始化，未加载到内存中时会经历类加载、链接、初始化三个步骤完成类的初始化。 类加载： 如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的加载器都是如此，因此所有的类加载请求都会传给顶层的启动类加载器。只有当父加载器反馈自己无法完成该加载请求（该加载器的搜索范围中没有找到对应的类）时，子加载器才会尝试自己去加载。 类的链接： 当类加载完成后，系统会给为之生成一个对象；随后进入链接阶段，链接阶段负责把类的二进制数据添加到JRE中。三个阶段： 验证：检验被加载的类是否有正确的内部结构，并和其他类协调一致 准备：负责为类的类变量分配内存。并设置默认初始值 解析：将类的二进制数据中的符号引用替换成直接引用 类的初始化：JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式： 声明类变量是指定初始值 使用静态代码块为类变量指定初始值 JVM初始化步骤 假如这个类还没有被加载和连接，则程序先加载并连接该类 假如该类的直接父类还没有被初始化，则先初始化其直接父类 假如类中有初始化语句，则系统依次执行这些初始化语句 10. 数据库试图的作用？视图中的数据是存在视图中的吗？ 这个回答的不好。 视图是从一个或几个基本表（或视图）导出的表。它与基本表不同，是一个虚表。数据库只存放视图的定义，而不存放视图对应的数据，这些数据仍存放在原来的基本表中。 视图能简化用户操作 视图使用户能以多种角度看待同一数据 视图对重构数据库提供了一定程度的逻辑独立性 视图能够对机密数据提供安全保护 11. 视图的更新有没有一些限制？ 没有回答上来 利用数据库视图进行更新实质上就是对数据库的基本表进行更新。所以视图的更新update命令有很多限制。 若视图是由两个以上基本表导出的，则此视图不允许更新 若视图的字段来自字段表达式或常数，则不允许对此视图执行INSERT和UPDATE操作，但允许执行DELETE 若视图字段来自聚集函数，则此视图不允许更新 若视图定义中含有GROUP BY 子句，则此视图不允许更新 若视图中含有DISTINCT语句，则此视图不允许更新 若视图定义中含有嵌套查询，并且内层查询的FROM子句涉及的表也是导出该视图的基本表，则此视图不允许更新 一个不允许更新的视图上定义的视图也不允许更新 12. 内联结和外联结的区别？ 内连接查询操作列出与连接条件匹配的数据行，它使用比较运算符比较被连接列的列值。 外连接，返回到查询结果集合中的不仅包含符合连接条件的行，而且还包括左表(左外连接或左连接))、右表(右外连接或右连接)或两个边接表(全外连接)中的所有数据行。 left join(左联接) 返回包括左表中的所有记录和右表中联结字段相等的记录； right join(右联接) 返回包括右表中的所有记录和左表中联结字段相等的记录。 13. 索引的作用 优点： 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快 数据的检索速度，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序 子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 缺点： 创建索引和维护索引要耗费时间，这种时间随着数据 量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 14. 索引一定会加快检索速度吗？ 对于那些在查询中很少使用或者参考的列不应该创建索引。这是因 为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 对于那 些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比 例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 当修改性能远远大于检索性能时，不应该创建索 引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因 此，当修改性能远远大于检索性能时，不应该创建索引。 15. 数据库的锁分为哪几种？ 锁的类型有三种： 共享（S)锁：多个事务可封锁一个共享页；任何事务都不能修改该页； 通常是该页被读取完毕，S锁立即被释放。 排它（X)锁：仅允许一个事务封锁此页；其他任何事务必须等到X锁被释放才能对该页进行访问；X锁一直到事务结束才能被释放。 更新（U)锁：用来预定要对此页施加X锁，它允许其他事务读，但不允许再施加U锁或X锁；当被读取的页将要被更新时，则升级为X锁；U锁一直到事务结束时才能被释放。 16. 幻读产生场景？一般出现在什么样的事务中？ 不可重复读取(Non-Repeatable Reads): A 事务两次读取同一数据，B事务也读取这同一数据，但是 A 事务在第二次读取前B事务已经更新了这一数据。所以对于A事务来说，它第一次和第二次读取到的这一数据可能就不一致了。 幻读(Phantom Reads): 与不可重复读有点类似，都是两次读取，不同的是 A 事务第一次操作的比如说是全表的数据，此时 B 事务并不是只修改某一具体数据而是插入了一条新数据，而后 A 事务第二次读取这全表的时候就发现比上一次多了一条数据，发生幻觉了。 数据库事务的隔离级别有4个，由低到高依次为Read uncommitted 读未提交 、Read committed 读提交 、Repeatable read 重复读 、Serializable 序列化 ，这四个级别可以逐个解决脏读 、不可重复读 、幻读 这几类问题。 幻读出现在前三个级别中。不可重复读出现在前两个级别中。脏读出现在第一个级别中。 17. TCP如何保证重传？ 每一次发送一个片段，就开启一个重传计时器。计时器有一个初始值并随时间递减。如果在片段接收到确认之前计时器超时，就重传片段。 放置于重传队列中，计时器开始 包含数据的片段一经发送，片段的一份复制就放在名为重传队列的数据结构中，此时启动重传计时器。因此，在某些时间点，每一个片段都会放在队列里。队列按照重传计时器的剩余时间来排列，因此TCP软件可追踪那几个计时器在最短时间内超时。 确认处理 如果在计时器超时之前收到了确认信息，则该片段从重传队列中移除。 重传超时 如果在计时器超时之前没有收到确认信息，则发生重传超时，片段自动重传。当然，相比于原片段，对于重传片段并没有更多的保障机制。因此，重传之后该片段还是保留在重传队列里。重传计时器被重启，重新开始倒计时。如果重传之后没有收到确认，则片段会再次重传并重复这一过程。在某些情况下重传也会失败。我们不想要TCP永远重传下去，因此TCP只会重传一定数量的次数，并判断出现故障终止连接。 18. 域名解析过程？ 在浏览器中输入www.qq.com域名，操作系统会先检查自己本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。 如果hosts里没有这个域名的映射，则查找本地DNS解析器缓存，是否有这个网址映射关系，如果有，直接返回，完成域名解析。 如果hosts与本地DNS解析器缓存都没有相应的网址映射关系，首先会找TCP/ip参数中设置的首选DNS服务器，在此我们叫它本地DNS服务器，此服务器收到查询时，如果要查询的域名，包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析，此解析具有权威性。 如果要查询的域名，不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析，此解析不具有权威性。 如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置（是否设置转发器）进行查询，如果未用转发模式，本地DNS就把请求发至13台根DNS，根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理，并会返回一个负责该顶级域名服务器的一个IP。本地DNS服务器收到IP信息后，将会联系负责.com域的这台服务器。这台负责.com域的服务器收到请求后，如果自己无法解析，它就会找一个管理.com域的下一级DNS服务器地址(qq.com)给本地DNS服务器。当本地DNS服务器收到这个地址后，就会找qq.com域服务器，重复上面的动作，进行查询，直至找到www.qq.com主机。 如果用的是转发模式，此DNS服务器就会把请求转发至上一级DNS服务器，由上一级服务器进行解析，上一级服务器如果不能解析，或找根DNS或把转请求转至上上级，以此循环。不管是本地DNS服务器用是是转发，还是根提示，最后都是把结果返回给本地DNS服务器，由此DNS服务器再返回给客户机。 从客户端到本地DNS服务器是属于递归查询，而DNS服务器之间就是的交互查询就是迭代查询。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>zhaoyinkeji</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[猎豹移动一面]]></title>
    <url>%2Fcheetah_mobile_interview.html</url>
    <content type="text"><![CDATA[猎豹移动一面Python项目 详细介绍项目流程 Java项目 详细介绍后台管理系统的设计流程 MySQL数据库开放性问题 谈谈你对于MySQL设计的理解 自己主要根据项目经历和实习经历中使用MySQL的经验，从数据库表建立和索引建立这两方面来回答。 在网上找到了数据库的发展历史。 MySQL是一种关系数据库管理系统，关系数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。MySQL所使用的 SQL 语言是用于访问数据库的最常用标准化语言。MySQL 软件采用了双授权政策，分为社区版和商业版，由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，一般中小型网站的开发都选择 MySQL 作为网站数据库。 数据库设计原则 基本表及其字段之间的关系, 应尽量满足第三范式。但是，满足第三范式的数据库设计，往往不是最好的设计。为了提高数据库的运行效率，常常需要降低范式标准：适当增加冗余，达到以空间换时间的目的。 数据库范式 第一范式：1NF是对属性的原子性约束，要求属性具有原子性，不可再分解 第二范式：2NF是对记录的惟一性约束，要求记录有惟一标识，即实体的惟一性； 第三范式：3NF是对字段冗余性的约束，即任何字段不能由其他字段派生出来，它要求字段没有冗余。 冗余的目的是为了提高处理速度。只有低级冗余才会增加数据的不一致性，因为同一数据，可 能从不同时间、地点、角色上多次录入。因此，我们提倡高级冗余(派生性冗余)，反对低级冗余(重复性冗余)。 视图是一种虚表，它依赖数据源的实表而存在。视图是供程序员使用数据库的一个窗口，是基表数据综合的一种形式, 是数据处理的一种方法，是用户数据保密的一种手段。为了进行复杂处理、提高运算速度和节省存储空间, 视图的定义深度一般不得超过三层。 若三层视图仍不够用, 则应在视图上定义临时表, 在临时表上再定义视图。这样反复交迭定义, 视图的深度就不受限制了。 中间表是存放统计数据的表，它是为数据仓库、输出报表或查询结果而设计的，有时它没有主键与外键(数据仓库除外)。临时表是程序员个人设计的，存放临时记录，为个人所用。基表和中间表由DBA维护，临时表由程序员自己用程序自动维护。 数据库设计的实用原则是：在数据冗余和处理速度之间找到合适的平衡点。 提高数据库运行效率的办法。在给定的系统硬件和系统软件条件下，提高数据库系统的运行效率的办法是： 在数据库物理设计时，降低范式，增加冗余, 少用触发器, 多用存储过程。 当计算非常复杂、而且记录条数非常巨大时(例如一千万条)，复杂计算要先在数据库外面，以文件系统方式用C++语言计算处理完成之后，最后才入库追加到表中去。这是电信计费系统设计的经验。 发现某个表的记录太多，例如超过一千万条，则要对该表进行水平分割。水平分割的做法是，以该表主键PK的某个值为界线，将该表的记录水平分割为两个表。若发现某个表的字段太多，例如超过八十个，则垂直分割该表，将原来的一个表分解为两个表。 对数据库管理系统DBMS进行系统优化，即优化各种系统参数，如缓冲区个数。 在使用面向数据的SQL语言进行程序设计时，尽量采取优化算法。 参考文章：总结数据库设计的原则 MySQL如何面对高并发用户的访问 自己的回答主要是从主从分离以及Redis结果缓存这两方面回答。 查询SQL优化 使用SQL时，尽量把使用的索引放在选择的首列； 在查询时，不要过多地使用通配符; 在可能的情况下尽量限制尽量结果集行数; 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描; 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描; 应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描; in 和 not in 也要慎用，因为IN会使系统无法使用索引,而只能直接搜索表中的数据; 尽量避免在索引过的字符数据中，使用非打头字母搜索。这也使得引擎无法利用索引; 必要时强制查询优化器使用某个索引; 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描; 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描; 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致; 使用视图加速查询 能用UNION ALL就不要用UNION, UNION ALL不执行SELECT DISTINCT函数，这样就会减少很多不必要的资源 参考文章：MySQL大数据高并发处理之-查询的优化 分库分表 分表：例如user表按照user_id%256的策略进行分表。分表能够解决单表数据量过大带来的查询效率下降的问题，但是，却无法给数据库的并发处理能力带来质的提升。面对高并发的读写访问，当数据库Master服务器无法承载写操作压力 时，不管如何扩展Slave服务器，此时都没有意义了。 分库: 对数据库进行拆分，从而提高数据库写入能力，这就是所谓的分库。假设user_id字段的值为257,将原有的单库分为256个库，那么应用程序对数据库的访问请求将被路由到第1个库（257%256=1)。 有时数据库可能既面临着高并发访问的压力，又需要面对海量数据的存储问题，这时需要对数据库即采用分库策略，又采用分表策略，以便同时扩展系统的并发处理能力，以及提升单 表的查询性能，这就是所谓的分库分表。 中间变量=user_id% (库数量X每个库的表数量） 库=取整（中间变量/每个库的表数量） 表=中间变量％每个库的表数量。 假设将原来的单库单表order拆分成256个库，每个库包含1024个表，那么按照前面所提到的路由策略，对于user_id=262145的访问，路由的计算过程如下：中间变量=262145% (256X1024) =1;库=取整（1/1024) =0;表=1%1024=1。这意味着，对于user_id=262145的订单记录的查询和修改，将被路由到第0个库的第1个表中执行。 参考文章：大型网站架构设计-mysql分表与分库 使用队列：使用队列写入等方法来降低并发读写:Redis队列 缓存设计 采用redis数据库，前置到mysql。 例如秒杀场景：必须使用缓存，将需要秒杀的商品放入缓存中，并使用锁来处理其并发情况。当接到用户秒杀提交订单的情况下，先将商品数量递减（加锁/解锁）后再进行其他方面的处理，处理失败在将数据递增1（加锁/解锁），否则表示交易成功。当商品数量递减到0时，表示商品秒杀完毕，拒绝其他用户的请求。 多用户并发修改同一条记录。 乐观锁，就是在数据库设计一个版本号的字段，每次修改都使其+1，这样在提交时比对提交前的版本号就知道是不是并发提交了，但是有个缺点就是只能是应用中控制，如果有跨应用修改同一条数据乐观锁就没办法了，这个时候可以考虑悲观锁。 悲观锁，就是直接在数据库层面将数据锁死，类似于oralce中使用select xxxxx from xxxx where xx=xx for update，这样其他线程将无法提交数据。 除了加锁的方式也可以使用接收锁定的方式，思路是在数据库中设计一个状态标识位，用户在对数据进行修改前，将状态标识位标识为正在编辑的状态，这样其他用户要编辑此条记录时系统将发现有其他用户正在编辑，则拒绝其编辑的请求，类似于你在操作系统中某文件正在执行，然后你要修改该文件时，系统会提醒你该文件不可编辑或删除。 参考文章：mysql处理高并发，防止库存超卖]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>Cheetah Mobile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OJ基本技巧记录]]></title>
    <url>%2FOJ_skill.html</url>
    <content type="text"><![CDATA[OJ基本技巧记录OJ平台代码基本格式123456789101112131415161718import java.util.Scanner;/** * Created by ml on 2017/9/11. */public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); while (in.hasNext()) &#123; String n = in.nextLine();//读一个字符串 int m = in.nextInt();//读一个整数 long l = in.nextLong();//读一个长整数 double t = in.nextDouble();//读一个浮点数 System.out.println(cal(n));//具体处理逻辑 &#125; in.close(); &#125;&#125; OJ输入输出技巧 输入数据有多行，第一行是一个整数n，表示测试实例的个数，后面跟着n行，每行包括一个由字母和数字组成的字符串。 123456789Scanner in = new Scanner(System.in);int n = in.nextInt();//可以创建数组int[] n = new int[n];String[] str = new String[n];for(int i=0;i&lt;n;i++) &#123; n[i] = in.nextInt(); String str = in.nextLine();&#125; 读入字符串的分割 例如：Input输入数据有多组，每组占一行，数据格式为YYYY/MM/DD组成 123456789Scanner in = new Scanner(System.in);while (in.hasNext()) &#123; String str =in.nextLine(); String[] date = str.split(&quot;/&quot;); int y = Integer.parseInt(date[0]); int m = Integer.parseInt(date[1]); int d = Integer.parseInt(date[2]);&#125;in.close(); 不同类型相互转换 int转为String： String s = Integer.toString(i); String s = String.valueOf(i); String s = “” + i; 三种效率排序：Integer.toString(int i) &gt; String.valueOf(int i) &gt; i+””; String转为int int i = Integer.parseInt(s); int i = Integer.valueOf(s).intValue() 这两种都会抛出异常（NumberFormatException） Java输出 System.out.print(); //输出不换行 System.out.println(); //输出换行 System.out.format(); double d = 345.678; System.out.printf(“%9.2f”, d);// “9.2”中的9表示输出的长度，2表示小数点后的位数。 System.out.printf(“%+-9.3f”, d);// “+-“表示输出的数带正负号且左对齐。 System.out.printf(); //与上相同 DecimalFormat。DecimalFormat 类主要靠 # 和 0 两种占位符号来指定数字长度。0 表示如果位数不足则以 0 填充，# 表示只要有可能就把数字拉上这个位置。 NumberFormat formatter = new DecimalFormat( “0.00”); String s = formatter.format(-.567); // -0.57 System.out.println(s); 矩阵如何按行输出 123456789//输出int[n][m]，判断是否是每一行的最后一个，输出换行。for(int i = 0; i &lt; n ; i++) &#123; for(int j = 0; j &lt; m; j++&gt;) &#123; if(j == m - 1)&#123; system.out.println(i); &#125; else &#123; system.out.print(i + &quot; &quot;); &#125;&#125; 不定长度类型 对于整形等数据在处理中不能确定长度的情况，使用ArrayList等形式来存储数据。 对于char或者String类型可以使用StingBuffer等形式来存储。 集合遍历 数组以及List：遍历 1234List&lt;String&gt; list = new ArrayList&lt;String&gt;();for (int i = 0; i &lt; list.size(); i++) &#123; System.out.println(list.get(i));&#125; Map类 12345678910111213141516Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();//1Iterator&lt;Map.Entry&lt;String, String&gt;&gt; it = map.entrySet().iterator();while (it.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = it.next(); System.out.println(&quot;key= &quot; + entry.getKey() + &quot; and value= &quot; + entry.getValue());&#125;//2for (Map.Entry&lt;String, String&gt; entry : map.entrySet()) &#123; entry.getKey();&#125;//3for (String key : map.keySet()) &#123; String value = (String) map.get(key);&#125; 传统的for循环遍历，基于计数器。遍历整个集合的平均时间复杂度为O(n^2)。 迭代器遍历，Iterator。 foreach循环遍历。实现原理同Iterator。 排序 如果要对数组排序，请使用java.util.Arrays.sort()方法。 如果对list排序，请使用java.util.Collections.sort()方法。 自定义排序：对于要排序的类实现Comparable接口。 123456789101112131415161718192021class S1 implements Comparable&#123; int x; int y; S1(int x, int y)&#123; this.x = x; this.y = y; &#125; //实现排序方法。先比较x，如果相同比较y @Override public int compareTo(Object o) &#123; S1 obj = (S1) o; if(x != obj.x) &#123; return x - obj.x; &#125; return y - obj.y; &#125; //重写toStirng方法，改变println时的显示效果 public String toString()&#123; return &quot;(&quot;+x+&quot;, &quot;+y+&quot;)&quot;; &#125; OJ常用函数 不同类型长度： 数组：int len = n.length;//int[] n = new int[10]; 字符串：int len = s.length();//String s = “hello”; List：int len = list.size();//List list = new ArrayList(); String类： 字符串某一位置字符,charAt。 12String str = new String(&quot;asdfzxc&quot;);char ch = str.charAt(4);//ch = z 转化为char数组。toCharArray方法。 12String str1 = new String(&quot;abc&quot;);char[] c = str1.toCharArray(); 判断相等。equals方法。 123String str1 = new String(&quot;abc&quot;);String str2 = new String(&quot;ABC&quot;);boolean c = str1.equals(str2);//c=false 提取子串。substring方法：一个参数：该方法从beginIndex位置起，出剩余的字符作为一个新的字符串返回。两个参数：左闭右开原则[beginIndex,endIndex)，即[beginIndex,endIndex - 1] 123String str1 = new String(&quot;asdfzxc&quot;);String str2 = str1.substring(2);//str2 = &quot;dfzxc&quot;String str3 = str1.substring(2,5);//str3 = &quot;dfz&quot; 字符串比较。compareTo方法是对字符串内容按字典顺序进行大小比较，通过返回的整数值指明当前字符串与参数字符串的大小关系。若当前对象比参数大则返回正整数，反之返回负整数，相等返回0。 1234String str1 = new String(&quot;abc&quot;);String str2 = new String(&quot;ABC&quot;);int a = str1.compareTo(str2);//a&gt;0int b = str1.compareTo(str2);//b=0 是否包含字符。contains方法判断参数s是否被包含在字符串中，并返回一个布尔类型的值. 123String str = &quot;student&quot;;str.contains(&quot;stu&quot;);//truestr.contains(&quot;ok&quot;);//false 字符串字符替换。replace、replaceFirst、replaceAll方法。 12345String str = &quot;asdzxcasd&quot;;String str1 = str.replace(&apos;a&apos;,&apos;g&apos;);//str1 = &quot;gsdzxcgsd&quot;String str2 = str.replace(&quot;asd&quot;,&quot;fgh&quot;);//str2 = &quot;fghzxcfgh&quot;String str3 = str.replaceFirst(&quot;asd&quot;,&quot;fgh&quot;);//str3 = &quot;fghzxcasd&quot;String str4 = str.replaceAll(&quot;asd&quot;,&quot;fgh&quot;);//str4 = &quot;fghzxcfgh&quot; 字符查找。indexOf、lastIndexOf方法。 123456String str = &quot;I am a good student&quot;;int a = str.indexOf(&apos;a&apos;);//a = 2int b = str.indexOf(&quot;good&quot;);//b = 7int c = str.indexOf(&quot;w&quot;,2);//c = -1int d = str.lastIndexOf(&quot;a&quot;);//d = 5int e = str.lastIndexOf(&quot;a&quot;,3);//e = 2 StringBuffer类 添加。append方法。 12StringBuffer sb = new StringBuffer(“abc”);sb.append(true);//&quot;abctrue&quot; 插入。insert方法。 12StringBuffer sb = new StringBuffer(“TestString”);sb.insert(4,false);//&quot;TestfalseString&quot; 删除。deleteCharAt方法。 12StringBuffer sb = new StringBuffer(“Test”);sb. deleteCharAt(1);//&quot;Tst&quot; 转换为字符串 12StringBuffer sb = new StringBuffer(“Test”);String s1 = sb.toString(); //StringBuffer转换为String ArrayList类 添加：add方法 插入：insert方法 删除：remove方法 清空：clear方法 包含：contains方法 OJ常用算法 自己的初步阶段：理清思路，暴力求解，再逐步改进！]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>OJ</tag>
        <tag>skill</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美团二面记录]]></title>
    <url>%2Fmeituan_interview_2.html</url>
    <content type="text"><![CDATA[美团二面记录 第一次遇见女面试官进行面试。自己学到更多的东西。 项目相关 面试官听了我的项目，差异公司没有这样的解决方案，需要自己再去造轮子吗？这也是自己的当时最晚的感受，也就是自己理解的常说的CRUD接口操作，但是这个项目给自己最大的感受是在于自己熟悉了整套开发上线流程，明白各个部门之间的分工协作过程。 自己的目标就是希望可以自己在后端写出属于自己的通用轮子。 如何考虑故障？单台提供数据查询有没有更好的方案？ 基础知识 Redis的基本数据结构 String类型：将String类型作为元素值； Lists类型：根据插入顺序的String类型为元素的集合，基于LinkedList实现，非Array型； Sets类型：无重复且无序的String类型为元素的集合； Sorted Set类型：无重复且有序的String类型为元素的集合； Hashes类型：映射域到值类型的数据结构，其中域和值都是String类型； Redis使用场景？ 会话缓存（Session Cache）:用Redis缓存会话比其他存储（如Memcached）的优势在于：Redis提供持久化。用户的购物车信息。 全页缓存（FPC）:Redis有磁盘的持久化，用户也不会看到页面加载速度的下降。 队列：Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。Redis作为队列使用的操作，就类似于本地程序语言（如Python）对 list 的 push/pop 操作。 排行榜/计数器。Redis在内存中对数字进行递增或递减的操作实现的非常好。“ZRANGE user_scores 0 10 WITHSCORES” 发布/订阅。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用Redis的发布/订阅功能来建立聊天系统。 参考文章：Redis 的 5 个常见使用场景 如何设计多级缓存结构？考虑哪些方面？ 多级缓存，即在整个系统架构的不同系统层级进行数据缓存，以提升访问效率，这也是应用最广的方案之一。 整体流程如上图所示： 首先接入Nginx将请求负载均衡到应用Nginx，此处常用的负载均衡算法是轮询或者一致性哈希，轮询可以使服务器的请求更加均衡，而一致性哈希可以提升应用Nginx的缓存命中率；后续负载均衡和缓存算法部分我们再细聊； 接着应用Nginx读取本地缓存（本地缓存可以使用Lua Shared Dict、Nginx Proxy Cache（磁盘/内存）、Local Redis实现），如果本地缓存命中则直接返回，使用应用Nginx本地缓存可以提升整体的吞吐量，降低后端的压力，尤其应对热点问题非常有效；为什么要使用应用Nginx本地缓存我们将在热点数据与缓存失效部分细聊； 如果Nginx本地缓存没命中，则会读取相应的分布式缓存（如Redis缓存，另外可以考虑使用主从架构来提升性能和吞吐量），如果分布式缓存命中则直接返回相应数据（并回写到Nginx本地缓存）； 如果分布式缓存也没有命中，则会回源到Tomcat集群，在回源到Tomcat集群时也可以使用轮询和一致性哈希作为负载均衡算法； 在Tomcat应用中，首先读取本地堆缓存，如果有则直接返回（并会写到主Redis集群），为什么要加一层本地堆缓存将在缓存崩溃与快速修复部分细聊； 作为可选部分，如果步骤4没有命中可以再尝试一次读主Redis集群操作，目的是防止当从有问题时的流量冲击； 如果所有缓存都没有命中只能查询DB或相关服务获取相关数据并返回； 步骤7返回的数据异步写到主Redis集群，此处可能多个Tomcat实例同时写主Redis集群，可能造成数据错乱，如何解决该问题将在更新缓存与原子性部分细聊。 整体分了三部分缓存：应用Nginx本地缓存、分布式缓存、Tomcat堆缓存，每一层缓存都用来解决相关的问题，如应用Nginx本地缓存用来解决热点缓存问题，分布式缓存用来减少访问回源率、Tomcat堆缓存用于防止相关缓存失效/崩溃之后的冲击。 使用消息机制同步缓存 把写缓存改成写消息，通过消息通知数据变更； 同步缓存系统会订阅消息，并根据消息进行更新缓存； 数据一致性可以采用：消息体只包括ID、然后查库获取最新版本数据；通过时间戳和内容摘要机制(MD5)进行缓存更新； 如上方法也不能保证消息不丢失，可以采用：应用在本地记录更新日志，当消息丢失了回放更新日志；或者采用数据库binlog，采用如canal订阅binlog进行缓存更新。 对于长尾访问的数据、大多数数据访问频率都很高的场景、缓存空间足够都可以考虑不过期缓存，比如用户、分类、商品、价格、订单等，当缓存满了可以考虑LRU机制驱逐老的缓存数据。 维度化缓存与增量缓存 将数据进行维度化并增量更新（只更新变的部分）。尤其如上下架这种只是一个状态变更，但是每天频繁调用的，维度化后能减少服务很大的压力。 轮询的优点：到应用Nginx的请求更加均匀，使得每个服务器的负载基本均衡；轮询的缺点：随着应用Nginx服务器的增加，缓存的命中率会下降，比如原来10台服务器命中率为90%，再加10台服务器将可能降低到45%；而这种方式不会因为热点问题导致其中某一台服务器负载过重。 一致性哈希的优点：相同请求都会转发到同一台服务器，命中率不会因为增加服务器而降低；一致性哈希的缺点：因为相同的请求会转发到同一台服务器，因此可能造成某台服务器负载过重，甚至因为请求太多导致服务出现问题。 正常情况采用一致性哈希，如果某个请求类型访问量突破了一定的阀值，则自动降级为轮询机制。另外对于一些秒杀活动之类的热点我们是可以提前知道的，可以把相关数据预先推送到应用Nginx并将负载均衡机制降级为轮询。 参考文章：应用多级缓存模式支撑海量读服务 这是一篇非常好的文章！ 主键和唯一键的区别？ 索引是存储在数据库中的一个物理结构，键纯粹是一个逻辑概念。键代表创建来实施业务规则的完整性约束。索引和键的混淆通常是由于数据库使用索引来实施完整性约束。 （1）主键约束和唯一键约束均会隐式创建同名的唯一索引，当主键约束或者唯一键约束失效时，隐式创建的唯一索引会被删除； （2）主键约束要求列值非空，而唯一键约束和唯一索引不要求列值非空； （3）相同字段序列不允许重复创建索引。 生产者消费者实现需要考虑哪几方面？如何考虑存储？ 生产者仅仅在仓储未满时候生产，仓满则停止生产。 消费者仅仅在仓储有产品时候才能消费，仓空则等待。 当消费者发现仓库没产品可消费时候会通知生产者生产。 生产者在生产出可消费产品时候，应该通知等待的消费者去消费。 对于存储中介，它肯定是一块具有额定大小的存储空间，而这个存储空间一般来说具有FIFO的数据结构，比如JDK内置了具有阻塞作用的有界队列：ArrayBlockingQueue、LinkedBlockingQueue。 参考文章：生产者/消费者模式 如何实现O(1)时间复杂度求栈的最小值？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com;import java.util.Stack;/** * Created by ml on 2017/9/7. * 原栈中，每次添加一个新元素时，就和辅助栈的栈顶元素相比较，如果新元素小，就把新元素的值放到辅助栈和原栈中，如果新元素大，就把元素放到原栈中；出栈时，如果原栈跟辅助栈元素相同，都弹出，否则只弹出原栈栈顶元素. */public class Stackmin &#123; public static void main(String[] args) &#123; AdvancedStack&lt;Integer&gt; stack = new AdvancedStack&lt;Integer&gt;(); stack.push(5); System.out.println(stack.getMin()); stack.push(7); System.out.println(stack.getMin()); stack.push(3); System.out.println(stack.getMin()); stack.push(9); System.out.println(stack.getMin()); stack.push(3); System.out.println(stack.getMin()); stack.pop(); System.out.println(stack.getMin()); &#125; static class AdvancedStack&lt;T extends Comparable&gt; &#123; Stack&lt;T&gt; stackNormal = new Stack&lt;T&gt;(); Stack&lt;T&gt; stackMin = new Stack&lt;T&gt;(); Stack&lt;T&gt; stackMax = new Stack&lt;T&gt;(); public void push(T e) &#123; stackNormal.push(e); //最小栈为空或者push的值小于最小栈的栈顶元素 if (stackMin.isEmpty() || e.compareTo(stackMin.peek()) &lt; 0) &#123; stackMin.push(e); &#125; else if (e.compareTo(stackMin.peek()) &gt; 0) &#123; stackMin.push(stackMin.peek()); &#125; if (stackMax.isEmpty() || e.compareTo(stackMin.peek()) &gt; 0) &#123; stackMax.push(e); &#125; else if (e.compareTo(stackMax.peek()) &lt; 0) &#123; stackMax.push(stackMax.peek()); &#125; &#125; public T pop() &#123; if (!stackNormal.isEmpty() &amp;&amp; !stackMin.isEmpty() &amp;&amp; !stackMax.isEmpty()) &#123; T e = stackNormal.pop(); stackMin.pop(); stackMax.pop(); return e; &#125; else &#123; return null; &#125; &#125; public T getMin() &#123; return stackMin.peek(); &#125; public T getMax() &#123; return stackMax.peek(); &#125; &#125;&#125; 如何设计即时通信系统，采用TCP还是UDP？ 现在的移动端IM、推送系统，既面对移动互联网的不确定性，又面对智能终端频繁的系统休眠、网络切换，还要考虑服务端的承载成本，对于在线服务而言UDP是比TCP更适合的方式。但是由于数据完整性、安全性的需要，又不应完全放弃TCP的可靠与安全。 两种通信协议同时使用，各有侧重。UDP用于保持大量终端的在线与控制，应用与业务则通过TCP去实现。这个和FTP服务控制与数据分离，采取不同的连接，有异曲同工之处。 参考文章:移动端IM/推送系统的协议选型：UDP还是TCP？ 抽象类的作用？ 通过继承它实现多态。设计抽象类是为了继承它，实现代码的重用性，可以使得由它所派生的所有类具有相同的接口，使用更加灵活抽象，可以使多态的作用发挥的更好，还有一点就是程序的结构更易于更改和扩充。 接口就是更纯粹的抽象类。 kafka有了解过吗？ kafka学习笔记：知识点整理 开放问题 对你最大作用的项目？ OK 学校期间最有挑战性的项目？ 应该侧重于技术方面的挑战 除了项目和实习，你是如何学习新的技术？ OK 除了以上技术，你最喜欢的技术是什么？ 前沿技术？？]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>meituan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美团点评一面]]></title>
    <url>%2Fmeituan_interview.html</url>
    <content type="text"><![CDATA[美团点评一面 强大的基础面试让我亚历山大半个小时多的面试，很多问题自己没有扎实的理解没有深入，导致问的越多自己越慌张基础是深度的基础，深度是基础的升华 1. 介绍Sug项目1.1 项目之间的关联？ 项目背景需求讲述。 1.2 项目中最有挑战性？ 项目难点说明。 1.3 数据量是多少？ 感觉考察对于项目细节的总结。 1.4 map reduce配置多少？ Map的数量经常是由输入数据中的DFS块的数量来决定的。正确的reduce任务的个数应该是0.95或者1.75 ×（节点数 ×mapred.tasktracker.tasks.maximum参数值）。 1.5 调整的container数量是多少？ 步骤1：用户将应用程序提交到ResourceManager上； 步骤2：ResourceManager为应用程序ApplicationMaster申请资源，并与某个NodeManager通信，以启动ApplicationMaster； 步骤3：ApplicationMaster与ResourceManager通信，为内部要执行的任务申请资源，一旦得到资源后，将于NodeManager通信，以启动对应的任务。 步骤4：所有任务运行完成后，ApplicationMaster向ResourceManager注销，整个应用程序运行结束。上述步骤中，步骤2~3涉及到资源申请与使用，而这正是Container出现的地方。 （1） Container是YARN中资源的抽象，它封装了某个节点上一定量的资源（CPU和内存两类资源）。它跟Linux Container没有任何关系，仅仅是YARN提出的一个概念（从实现上看，可看做一个可序列化/反序列化的Java类）。（2） Container由ApplicationMaster向ResourceManager申请的，由ResouceManager中的资源调度器异步分配给ApplicationMaster；（3） Container的运行是由ApplicationMaster向资源所在的NodeManager发起的，Container运行时需提供内部执行的任务命令（可以使任何命令，比如java、Python、C++进程启动命令均可）以及该命令执行所需的环境变量和外部资源（比如词典文件、可执行文件、jar包等）。 Container数量=min (2CORES, 1.8DISKS, (可用内存)/最低Container的大小) 每个Container的内存大小 = max(最小Container内存大小, (总可用内存) /Container数)) 例如：集群的节点有 12 CPU cores, 48 GB RAM, and 12 磁盘.预留内存= 6 GB 系统预留 + 8 GB HBase预留最小Container内存大小 = 2 GB Container数 = min (212, 1.8 12, (48-6)/2) = min (24, 21.6, 21) = 21 每个Container的内存大小 = max (2, (48-6)/21) = max (2, 2) = 2 1.6 定时crontab挂掉有其他方式保证吗？ 百度了下，有crontab误删的解决方案：获取完整日志和cmd日志，获取所有crontab指令，从crontab.txt 中找出每一条指令，然后在cmd_temp 中匹配运行次数，重新编辑crontab添加恢复。 123456#!/bin/bash# 每天对crontab 进行备份 ，同时删除最近15天的数据DATE=$(date +%Y%m%d)crontab -l &gt; /home/work/bak/crontab_$DATE.bakfind /home/work/bak/ -mtime +15 -name &apos;*.bak&apos; -exec rm -rf &#123;&#125; \; 自己第二个理解是，crontab在单台布置任务，如果该台机器发生故障的解决方案？ 多台备份，一主多从，故障切换。多台机器上的定时任务——cronsun。cronsun 支持：多机单任务(防止单机挂掉任务不按时执行)。 1.7 Hadoop Streaming相对于传统的优点？ Hadoop Streaming是Hadoop提供的一个编程工具，它允许用户使用任何可执行文件或者脚本文件作为Mapper和Reducer。 mapper和reducer会从标准输入中读取用户数据，一行一行处理后发送给标准输出。Streaming工具会创建MapReduce作业，发送给各个tasktracker，同时监控整个作业的执行过程。如果一个文件（可执行或者脚本）作为mapper，mapper初始化时，每一个mapper任务会把该文件作为一个单独进程启动，mapper任务运行时，它把输入切分成行并把每一行提供给可执行文件进程的标准输入。 同时，mapper收集可执行文件进程标准输出的内容，并把收到的每一行内容转化成key/value对，作为mapper的输出。 默认情况下，一行中第一个tab之前的部分作为key，之后的（不包括tab）作为value。如果没有tab，整行作为key值，value值为null。对于reducer，类似。以上是Map/Reduce框架和streaming mapper/reducer之间的基本通信协议。 优点：只要是支持stdin stdout的语言都可以用来实现MapReduce算法，这对于非Java程序员来编写MapReduce程序非常友好。 缺点：慢；无法避免的数据类型转换；archive，file分发对于I/O的压力。 1.8 关于Hadoop本身的架构你有哪些了解？Yarn？（这个地方反复听还是没有听清）? 自己了解的不多，需要积累。 推荐董的博客。 1.9 MapReduce执行过程？ 1.预先加载本地的输入文件2.经过MAP处理产生中间结果3.经过shuffle将key相同的中间结果分配到同一个节点去处理4.Reduce处理产生结果输出5.将结果保存在hdfs上 1.10 你了解多少MapRedce？ 自己了解的不多，需要积累。 推荐董的博客。 2. 智能快递箱项目——Java的SSH框架 本科项目，细节遗忘比较多，但是写在简历上，就必须完全掌握。面试官很好，他采用一站到底的形式，让我明白技术深度的重要性。 2.1 SSH框架对于传统的JSP、Servlet这些的优势？ 传统J2EE开发模式：Jsp+Servlet+Javabean 其实传统J2EE的缺点就是SSH框架中的优点，框架是为了解决一个又一个在Web开发中所遇到的问题而诞生的。不同的框架，都是为了解决不同的问题。 从最开始的JSP，在Html中嵌入Java逻辑，但是功能复杂后，Html混乱的结构，数据库事务以及日志，功能逻辑固化，遇到需求更改，往往需要重写一个JSP文件。 所以有了使用servlet来处理那些业务逻辑，把JSP中的Java业务逻辑剥离抽象出来。这样的好处在于流程更加清晰，虽然只是代码从JSP移动到了Servlet。。然而，为了这么点干净，付出的是为每个servlet都在web.xml里面去做一个url的请求配置。 最终，我们需要的是：1. 数据 2. 页面展示 3. 处理具体业务的场所，任何形式实现的功能内部的核心元素都是这三种。 那么这就是MVC的概念：数据 ———— Model 页面展示 ———— View 处理具体业务的场所 ———— Control 框架的典型的三层构架体现MVC（模型Model,视图View和控制）思想、良好的可扩展性、良好的可维护性和优秀的解耦性。 框架将实现功能的流程清晰化、分离化、独立化之后，各个模块之间的通信、异常等问题就是新的问题，就是SSH框架需要解决的问题。 Struts的优点： Struts2 Action对象为每一个请求产生一个实例，因此没有线程安全问题。 Struts2强大的标签库提高开发效率。 页面脉络清晰，通过查看配置文件把握整个系统的关系，方便开发人员岗位流动时的维护。 Spring的优点： spring提高了一种对象管理方法，有效的组织系统中间层对象。是框架的完美“粘合剂”。 IOC和AOP Hibernate的优点： hibernate是JDBC轻量级的封装，占用内存较少，性能比较高。 优秀的ORM框架。 Hibernate兼容JDBC。 参考文章：深入浅出的理解框架（Struts2、Hibernate、Spring）与 MVC 设计模式 2.2 你的意思是Strtus框架是在JSP上做了一层封装？ Struts是对Servlet控制页面跳转、类型转换等做了进一步封装，解决了Servlet开发带来的诸多问题，有利于维护。 2.3 JSP中有一些内置的对象？ NO. 内置对象 类型 作用 1 pageContext javax.servlet.jsp.PageContext 使用它可以访问到本页面中所有其他对象，例如前面已经描述的request、response以及application对象等。 2 request javax.servlet.http.HttpServletRequest request内置对象中包含了有关浏览器请求的信息，并提供了多个用于获取cookie、header以及session内数据的方法。 3 response javax.servlet.http.HttpServletResponse response对象与request对象相对应，它是用于响应客户请求，向客户端输出信息。response对象提供了多个方法用来处理HTTP响应。 4 session javax.servlet.http.HttpSession session是与请求有关的会话期，用来表示和存储当前页面的请求信息。 5 application javax.servlet.ServletContext 用于实现用户之间的数据共享（多使用于网络聊天系统）。一般来说，一个用户对应着一个session，并且随着用户的离开session中的信息也会消失，所以不同客户之间的会话必须要确保某一时刻至少有一个客户没有终止会话；而applicat则不同，它会一直存在，类似于系统的“全局变量”，而且只有一个实例。 6 config javax.servlet.ServletConfig 在Servlet初始化的时候，JSP引擎通过config向它传递信息。这种信息可以是属性名/值匹配的参数，也可以是通过ServletContext对象传递的服务器的有关信息。 7 out javax.servlet.jsp.JspWriter out对象用于向客户端发送文本数据。 8 page java.lang.Object page对象有点类似于Java编程中的this指针，就是指当前JSP页面本身。 9 exception java.lang.Throwable exception内置对象是用来处理页面出现的异常错误。 2.4 JSP是如何在web容器中跑起来的？经历那几个阶段？如何传输？ 当客户端第一次请求一个jsp资源的时候，jsp引擎会查找这个jsp文件并且将它转换成为一个java文件，然后编译成为一个servlet类。 Web容器实例化这个servlet。 Web容器调用init()方法。在这个init方法中，调用针对jsp的JspInit()方法。 Web容器调用service()方法。在service()方法中，调用_jspService()方法。 JSP和Servlet的本质是一样的，因为JSP最终需要编译成Servlet才能运行，换句话说JSP是生成Servler的草稿文件。 参考文章:jsp是如何被容器调用和执行的？ 2.5 Servlet的生命周期？ 初始化阶段：调用init()方法 响应客户请求阶段：调用service()方法 终止阶段：调用destroy()方法 JSP的优点是擅长于网页制作，生成动态页面比较直观，缺点是不容易跟踪与排错。Servlet是纯Java语言，擅长于处理流程和业务逻辑，缺点是生成动态网页不直观。 2.7 SSH中Spring、Hibernate各自负责的角色？ Spring充当了管理容器的角色，起到的主要作用是解耦，创建对象处理对象的依赖关系以及框架整合。IOC、AOP。 Struts主要控制逻辑关系的处理。 hibernate 是数据持久化层。 2.8 SSH中的ORM？ ORM框架可作为面向对象程序语言和数据库之间的桥梁。ORM框架是面向对象程序设计语言与关系数据库发展不同步时的中间解决方案。 ORM的基本映射方式： 数据类型映射模式 类映射模型 关联映射模式 引用映射模式 Employee.hbm.xml 对象的映射 (映射文件)hibernate.cfg.xml 数据库连接配置、加载所用的映射(*.hbm.xml) 123456789101112131415161718192021222324252627282930313233343536package sram.hello;import java.util.Date;import org.hibernate.Session;import org.hibernate.SessionFactory;import org.hibernate.Transaction;import org.hibernate.cfg.Configuration;import org.junit.Test;public class App &#123; @Test public void testHello() throws Exception&#123; //对象 Employee emp = new Employee(); emp.setEmpName(&quot;Alice&quot;); emp.setWorkDate(new Date()); //获取加载配置文件的管理类对象 Configuration config = new Configuration(); config.configure();//默认加载src/hibernate.cfg.xml文件 //创建session工厂 SessionFactory sf = config.buildSessionFactory(); //创建session(代表一个会话，与数据库连接的会话) Session session = sf.openSession(); //开启事务 Transaction tx = session.beginTransaction(); //保存-数据库 session.save(emp); //提交事务 tx.commit(); //关闭 session.close(); sf.close(); &#125;&#125; 2.9 通常把什么样的功能放在Spring中？ 核心容器(Spring core) 核心容器提供Spring框架的基本功能。Spring以bean的方式组织和管理Java应用中的各个组件及其关系。Spring使用BeanFactory来产生和管理Bean，它是工厂模式的实现。BeanFactory使用控制反转(IoC)模式将应用的配置和依赖性规范与实际的应用程序代码分开。BeanFactory使用依赖注入的方式提供给组件依赖。 Spring上下文(Spring context) Spring上下文是一个配置文件，向Spring框架提供上下文信息。Spring上下文包括企业服务，如JNDI、EJB、电子邮件、国际化、校验和调度功能。 Spring面向切面编程(Spring AOP) 通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring框架中。所以，可以很容易地使 Spring框架管理的任何对象支持 AOP。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。 Spring DAO模块 DAO模式主要目的是将持久层相关问题与一般的的业务规则和工作流隔离开来。Spring 中的DAO提供一致的方式访问数据库，不管采用何种持久化技术，Spring都提供一直的编程模型。Spring还对不同的持久层技术提供一致的DAO方式的异常层次结构。 Spring ORM模块 Spring 与所有的主要的ORM映射框架都集成的很好，包括Hibernate、JDO实现、TopLink和IBatis SQL Map等。Spring为所有的这些框架提供了模板之类的辅助类，达成了一致的编程风格。 Spring Web模块 Web上下文模块建立在应用程序上下文模块之上，为基于Web的应用程序提供了上下文。Web层使用Web层框架，可选的，可以是Spring自己的MVC框架，或者提供的Web框架，如Struts、Webwork、tapestry和jsf。 Spring MVC框架(Spring WebMVC) MVC框架是一个全功能的构建Web应用程序的MVC实现。通过策略接口，MVC框架变成为高度可配置的。Spring的MVC框架提供清晰的角色划分：控制器、验证器、命令对象、表单对象和模型对象、分发器、处理器映射和视图解析器。Spring支持多种视图技术。 2.10 Hibernate内对象session的生命周期？ Hibernate 三种状态 临时状态（Transient）：在通过new关键字，实例化一个对象开始，该对象就进入了临时状态，但它还没有被持久化，没有保存在Session当中。 持久化状态（Persistent）：对象被加入到Session缓存当中，如通过session.save(entity)，Hibernate把实体保存到seesion当中，entity就处在持久化状态中。 游离状态（Detached）：对象脱离了session缓存，如通过session清理，将对象保存到数据库中，原来在session中的对象仍然与内存中，该对象就处于游离状态。 hibernate Session的生命周期受到其自身属性和方法的影响，简单的说： SessionFactory的openSession() 方法会开启一个session。 Session的flushMode会决定session何时进行flush。 Session的flush()方法会对session进行强制flush。 Session的close()方法会关闭session。 读取并解析配置文件 读取并解析映射信息，创建SessionFactory 打开Sesssion 创建事务Transation 持久化操作 提交事务 关闭Session 关闭SesstionFactory 当一个对象被实例化出来以后，该对象是临时状态，当调用方法session.save(entity)，后该对象被加入到session缓存中，进入持久化状态，这时数据库中还不存在相关的记录，当session提交数据库事务时，这里隐含做了两件事，一件事是隐式调用session.flush()，其作用先是清理缓存（相当于调用了session.clear()），再生成一条对应的insert语句，但该语句还没有提交，第二件事是对刚才生成的语句进行提交，从而在数据库中生成了对应的记录。至此原entity对象就在数据库中生成了一条对应的记录，而它本身也脱离了session缓存，处于游离状态，该对象经过垃圾回收机制处理被回收。一个hibernate的保存对象过程就此结束。 Hibernate一级缓存又称为“Session的缓存”。Session的缓存是事务范围的缓存（Session对象的生命周期通常对应一个数据库事务或者一个应用事务）。 Hibernate二级缓存又称为“SessionFactory的缓存”。由于SessionFactory对象的生命周期和应用程序的整个过程对应，因此Hibernate二级缓存是进程范围或者集群范围的缓存。适合1) 很少被修改的数据 2) 不是很重要的数据，允许出现偶尔并发的数据 3) 不会被并发访问的数据 4) 常量数据 。 当Hibernate根据ID访问数据对象的时候，首先从Session一级缓存中查；查不到，如果配置了二级缓存，那么从二级缓存中查；如果都查不到，再查询数据库，把结果按照ID放入到缓存删除、更新、增加数据的时候，同时更新缓存。 Session的延迟加载实现要解决两个问题：正常关闭连接和确保请求中访问的是同一个session。Hibernate session就是java.sql.Connection的一层高级封装，一个session对应了一个Connection。http请求结束后正确的关闭session（过滤器实现了session的正常关闭）；延迟加载必须保证是同一个session（session绑定在ThreadLocal）。 2.11 SSH，SSM请求过程？ SSH jsp页面，提交表单或者点击链接会触发一个action。 action交给struts2处理，读取struts.xml文件，在配置中找到对应的action。 根据struts.xml文件中找到的class=”XXXAction”交给spring，读取Spring容器的配置文件/WebRoot/WEB-INF/applicationContext.xml文件。根据applicationContext.xml配置文件找到xxx.xxx.action.xxxAction类，其中有一个属性xxxService,并提供setxxxService()方法，由applicationContext.xml文件得知该xxxService对象由Spring容器依赖注入，set注入。 取得xxxService对象(接口对接口实现类的一个引用)后，调用它的方法。后面的就是访问DAO了。 执行的结果会在action的对应的方法中以字符串的形式给出。然后根据配置文件中的result.找到下一步要执行的动作，是跳转到页面显示还是继续交给其他的action处理。 SSM 2.12 Spring如何实现AOP，那几种方式实现，内部如何实现？ Spring提供了4种实现AOP的方式： 经典的基于代理的AOP @AspectJ注解驱动的切面 纯POJO切面 注入式AspectJ切面 内部实现：动态代理就是说AOP框架不会去修改字节码，而是在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。 2.13 Struts2中的拦截器的实现运用哪一种设计模式？ 拦截器用到了代理模式，过滤器用到了责任链的设计模式。拦截器栈（Interceptor Stack）：将拦截器按一定的顺序联结成一条链，在访问被拦截的方法时，Struts2拦截器链中的拦截器就会按其之前定义的顺序依次调用。类似于FilterChain，但又有很大的不同，比如FilterChain需要在编写doFilter方法时自行实现返回时的过滤以及链的向下执行，并且是在Servlet容器执行的这些操作。相反Intercetor是通过反射，通过XWork容器调用，自动返回拦截。 3. 基础知识3.1 TCP如何保证消息传递的顺序？ 主机每次发送数据时，TCP就给每个数据包分配一个序列号并且在一个特定的时间内等待接收主机对分配的这个序列号进行确认， 如果发送主机在一个特定时间内没有收到接收主机的确认，则发送主机会重传此数据包。 接收主机利用序列号对接收的数据进行确认，以便检测对方发送的数据是否有丢失或者乱序等， 接收主机一旦收到已经顺序化的数据，它就将这些数据按正确的顺序重组成数据流并传递到高层进行处理。 3.2 出现拥塞如何解决？如何进行流量控制？ TCP的流量控制是利用滑动窗口机制实现的，流量的端到端控制的实现是接收方在返回的ACK中会包含自己的接收窗口的大小，以控制发送方的数据发送。TCP的窗口单位是字节，不是报文段，发送方的发送窗口不能超过接收方给出的接收窗口的数值。 网络中的链路容量和交换结点中的缓存和处理机都有着工作的极限，当网络的需求超过它们的工作极限时，就出现了拥塞。拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。 慢开始（Slow-Start）和拥塞避免（Congestion Avoidance）结合 （1）当 cwnd &lt; ssthresh 时，使用上述的慢开始算法。（2）当 cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。（3）当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法。 快重传（Fast Retransmit）和快恢复（Fast Recovery）结合 （1）当发送方在cwnd=24时连续收到三个重复确认，就把慢开始门限ssthresh减半，就是上图中的24修改为12。这是为了预防网络发生拥塞。（2）与慢开始不同之处是现在不执行慢开始算法，而是把cwnd值设置为慢开始门限ssthresh减半后的数值，即cwnd不是设置为1而是设置为12，然后开始执行拥塞避免算法，使拥塞窗口缓慢地线性增大。这里为什么替换掉了慢开始算法呢，这是因为收到重复的ACK不仅仅告诉我们一个分组丢失了，由于接收方只有在收到另一个报文段时才会产生重复的ACK，所以还告诉我们该报文段已经离开了网络并进入了接收方的缓存。也就是说，在收发两端之间仍然有流动的数据，而我们不想执行慢启动来突然减少数据流。无论在慢开始阶段还是在拥塞避免阶段，只要发送方没有收到确认通知判断网络出现拥塞，就要把慢开始门限ssthresh设置为出现拥塞时的发送方窗口值的一半，上例中是把发送方窗口值24修改为12。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。 参考文章：NetWork——TCP的流量控制和拥塞控制 3.3 HTTP状态码以3开头的含义？ 重定向。基本上会配合Location首部字段来使用。301、302、304是HTTP1.0定义的，303、307是HTTP1.1定义的。 301：永久性重定向，该状态码表示请求的资源已被分配了新的URI，以后应使用Location指定的URI。 302：表示资源是临时性移动，已移动资源对应的URI将来还可能发生改变。301和302标准是禁止将post请求改变为get请求的，即原先使用post的请求，收到301和302的响应时，不能使用get请求Location指定的URI，而是应该得到用户的确认，然后使用post请求Location指定的URI。但是很多浏览器为了方便，直接略过用户确认，并使用get请求Location指定的URI。 303：与302类似。标准明确规定客户端应使用get请求Location指定的URI。 304：304其实和重定向没有任何关系。当客户端缓存了目标资源但不确定该缓存资源是否是最新版本的时候,就会发送一个条件请求.在Fiddler中,你可以在Headers Inspector查找相关请求头,这样就可以辨别出一个请求是否是条件请求。服务器会读取到这两个请求头中的值,判断出客户端缓存的资源是否是最新的,如果是的话,服务器就会返回HTTP/304 Not Modified响应。 307：相当于302，由于浏览器对于302标准并不遵守，因此定义307来代替302。post请求不会改变为get请求。 3.3 进程之间的通信方式？ 无名管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 高级管道(popen)：将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程，这种方式我们成为高级管道方式。 有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。 套接字( socket ) ： 套接口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>meituan</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度运维一面]]></title>
    <url>%2Fbaidu_interview.html</url>
    <content type="text"><![CDATA[百度运维一面 问的最多的问题就是你为什么不在原来的部门留下来。这个问题回答的不是很自信。 面对自己的过去，明白自己想要的。 1. 实习经历梳理以及Sug项目介绍 还不错。 2. 关于正则表达式2.1 项目中正则表达式匹配效率 自己当时说自己主要是匹配一些策略类型，然后聊到匹配的效率，同阿伦如何一次匹配所有。因为自己之前是写了几个正则表达式去匹配各自的信息。 2.2 写出提取一行中所有满足格式的数据的正则表达式：例如提取a:3, b:2, c:1,… 如何提取？类似于匹配多个结果 1234567891011121314151617181920public void testRegex() &#123; String msg = &quot;Rect(x1=\&quot;0\&quot; y1=\&quot;0\&quot; x2=\&quot;416\&quot; y2=\&quot;416\&quot;)Rect(x1=\&quot;1\&quot; y1=\&quot;2\&quot; x2=\&quot;413\&quot; y2=\&quot;414\&quot;)&quot;; List&lt;String&gt; textList = new ArrayList&lt;String&gt;(); Pattern pattern = Pattern.compile(&quot;(x1=\&quot;[^\&quot;]*\&quot;\\s*y1=\&quot;[^\&quot;]*\&quot;\\s*x2=\&quot;[^\&quot;]*\&quot;\\s*y2=\&quot;[^\&quot;]*\&quot;)&quot;); Matcher matcher = pattern.matcher(msg); while (matcher.find()) &#123; textList.add(matcher.group(1)); &#125; for (String text : textList) &#123; try &#123; System.out.println(text); System.out.println(getValue(text, &quot;x1&quot;)); System.out.println(getValue(text, &quot;y1&quot;)); System.out.println(getValue(text, &quot;x2&quot;)); System.out.println(getValue(text, &quot;y2&quot;)); &#125; catch (Exception ex) &#123;&#125; &#125; &#125; 3. crontab实现原理以及使用格式 周期执行的任务一般由cron这个守护进程来处理。cron读取一个或多个配置文件，这些配置文件中包含了命令行及其调用时间。cron的配置文件称为“crontab”，是“cron table”的简写。 一旦cron进程启动，它就会读取配置文件，并将其保存在内存中，接着自己转入到休眠状态。以后每分钟会醒来一次检查配置文件，读取修改过的，并执行为这一刻安排的任务，然后再转入休眠。 实例5：每个星期一的上午8点到11点的第3和第15分钟执行命令： 3,15 8-11 1 command 4. 如何在日志中找到ip最大的10个，Linux命令以及设计算法实现 自己回答桶排序是错的，IP32位无法映射完所有的IP IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理。 按照IP地址的Hash(IP) % 1024的值，把海量IP日志分别存储到1024个小文件中，这样，每个小文件最多包含4MB个IP地址； 每一个文件中建立Top10的堆, 比较IP地址大小，遍历维护堆 12345678910111213141516171819202122232425262728293031 /** * 比较两个ip地址，如果两个都是合法地址，则1代表ip1大于ip2，-1代表ip1小于ip2,0代表相等； * 如果有其一不是合法地址，如ip2不是合法地址，则ip1大于ip2，返回1，反之返回-1；两个都是非法地址时，则返回0； * 注意此处的ip地址指的是如“192.168.1.1”地址，并不包括mask * @return */ public static int compareIpV4s(String ip1,String ip2) &#123; int result = 0; int ipValue1 = getIpV4Value(ip1); // 获取ip1的32bit值 int ipValue2 = getIpV4Value(ip2); // 获取ip2的32bit值 if(ipValue1 &gt; ipValue2) &#123; result = -1; &#125; else if(ipValue1 &lt;= ipValue2) &#123; result = 1; &#125; return result; &#125; public static int getIpV4Value(String ipOrMask) &#123; byte[] addr = getIpV4Bytes(ipOrMask); int address1 = addr[3] &amp; 0xFF; address1 |= ((addr[2] &lt;&lt; 8) &amp; 0xFF00); address1 |= ((addr[1] &lt;&lt; 16) &amp; 0xFF0000); address1 |= ((addr[0] &lt;&lt; 24) &amp; 0xFF000000); return address1; &#125; 最后遍历所有的堆取所有的Top10。 5. MySQL中联合索引（A，B，C）单独使用B会使用联合索引吗？联合索引为什么比单独的索引快？ SELECT * FROM TABLE_NAME WHERE COL1=’ABC’ AND COL2=123; 在2个列上单独创建索引，如果查询语句使用到，叫合并索引；如果是在2个列上创建组合索引，就叫组合索引。 单独的2个索引进行查找——索引合并，需要反复在2个索引表间进行跳转，这是造成速度慢的第一个影响。第2个影响是，假设满足COL1=’ABC’的数据有5行，满足COL2=123的数据有1000行。最坏的情况下（那5行在COL2的1000行最后面）需要扫描完COL2的1000行才能找到需要的数据，并不能达到快速查找的目的。 由于组合索引综合保存了COL1和COL2的数据，它不需要在2个索引表之间跳转，所以速度会更快。组合索引不需要像索引合并那样对索引的ROWID进行比较合并。 复合索引的优势只有查询复合索引的全部列，并且按索引的设置顺序查询，最重要的是一定要有首列的查询条件。 6. 手写堆排序维护最大的10个数 Key[i]&lt;=key[2i+1]&amp;&amp;Key[i]&lt;=key[2i+2]或者Key[i]&gt;=Key[2i+1]&amp;&amp;key[i]&gt;=key[2i+2]即任何一非叶节点的关键字不大于或者不小于其左右孩子节点的关键字。 step 1. 随意选出K个数，挑出这K个数的最小的数。这个过程可以用最小堆完成。 step 2. 在剩下的n – K个数中，挑出任意一个数m，和最小堆的堆顶进行比较，如果比最小堆的堆顶大，那么说明此数可以入围前K的队伍，于是将最小堆的堆顶置为当前的数m。 step 3. 调整最小堆。时间复杂度为Olg(K)，由于K是constant(常数级别)，所以时间复杂度可以认为是常数级别。 step 4. 重复进行step 2 ~ step 3，直到剩下的n – K个数完成。进行了n –constant次，时间复杂度为O(n lgK)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114/** * Created by jeff on 16/5/11. */public class MinHeap &#123; // 堆的存储结构 - 数组 private int[] data; /** * 初始化堆中的数据,以int的最小值作为初始值 * * @param k */ public MinHeap(int k) &#123; this.data = new int[k]; for(int i=0;i&lt;k;i++)&#123; data[i]=Integer.MIN_VALUE; &#125; &#125; private void adjustHeap(int i) &#123; //获取左右结点的数组下标 int l = left(i); int r = right(i); // 这是一个临时变量，表示 跟结点、左结点、右结点中最小的值的结点的下标 int min = i; // 存在左结点，且左结点的值小于根结点的值 if (l &lt; data.length &amp;&amp; data[l]&lt;data[i])&#123; min = l; &#125; // 存在右结点，且右结点的值小于以上比较的较小值 if (r &lt; data.length &amp;&amp; data[r]&lt;data[min])&#123; min = r; &#125; // 左右结点的值都大于根节点，直接return，不做任何操作 if (i == min) return; // 交换根节点和左右结点中最小的那个值，把根节点的值替换下去 swap(i, min); // 由于替换后左右子树会被影响，所以要对受影响的子树再进行adjustHeap adjustHeap(min); &#125; /** * 获取右结点的数组下标 * @param i * @return */ private int right(int i) &#123; return (i + 1) &lt;&lt; 1; &#125; /** * 获取左结点的数组下标 * @param i * @return */ private int left(int i) &#123; return ((i + 1) &lt;&lt; 1) - 1; &#125; /** * 交换元素位置 * * @param i * @param j */ private void swap(int i, int j) &#123; int tmp = data[i]; data[i] = data[j]; data[j] = tmp; &#125; /** * 将数值加入到堆中 * * @param element */ public void add(int element) &#123; if(element&gt;data[0]) &#123; data[0] = element; adjustHeap(0); &#125; &#125; public int[] getData()&#123; return data; &#125; @Override public String toString()&#123; StringBuilder builder = new StringBuilder(); builder.append(&quot;&#123;&quot;); for (int i=0;i&lt;data.length;i++)&#123; builder.append(data[i]); if(i!=data.length-1)&#123; builder.append(&quot;,&quot;); &#125; &#125; builder.append(&quot;&#125;&quot;); return builder.toString(); &#125;&#125; 参考文章：算法——TOP K问题最小堆实现]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>baidu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蘑菇街二面记录]]></title>
    <url>%2Fmogujie_interview_2.html</url>
    <content type="text"><![CDATA[蘑菇街二面 全是项目对于活动类问题问的非常详细 项目中面试询问计数点1. 项目流程图- 略。 2. 订单中心接口的时延是多少？- 接口文档说明。 - 自己的理解应该是如何降低接口的响应时间？ - 从接口垂直与水平拆分（MapReduce）：项目没用到。。。 - 接口缓存与本地缓存 ：对于订单的实时获取，将用户的最新订单缓存到本地，然后计算结果放入缓存？随之带来存储问题？感觉可以把用户信息缓存，省去一部分查询用户的时间比较实际？ - 非核心流程异步化：类似于发消息，写日志，更新缓存等不会影响接口准确性的非核心流程，可以采用异步方式进行处理，不阻塞主计算逻辑处理。 - 内部并发：项目没有考虑。 - 参考文章：[如何减少接口响应时间](http://blog.csdn.net/xiaoxuan2015/article/details/51240872) 3. 高QPS下如何保证接口访问的稳定性？除了加服务器之外？- QPS（TPS）：每秒钟request/事务 数量 - 并发数： 系统同时处理的request/事务数 - 响应时间： 一般取平均响应时间 - 自己的理解应该是业务的解耦分拆，提高并发量。具体没有实践所以需要好好学习。 - 服务做到多少QPS，多长的耗时，应结合手头有限的资源、业务需求来制定。 - 参考文章：[美团团购订单系统优化记](https://tech.meituan.com/meituan_tuangou_order.html) - 非常有价值的文章。 4. 高QPS下哪一个模块的压力最大？如何解决？- 数据库访问。引入连接池。 - 如果在代码中显式为每次操作分别建立并释放资源，无疑增大了业务代码的复杂度，并且建立和释放连接的开销变得不可忽略。 5. 对于超时如何处理？- 尝试多调用一次 - 使用待处理队列 - 回滚数据 - 使用异步机制 - 参考文章：[【干货篇】调用其他系统http接口超时了，如何处理，方案汇总](http://blog.csdn.net/linsongbin1/article/details/50393383) - 自己的理解：对于活动类需要实时返回查询结果的，增加尝试调用次数是最合适的选择，要有错误策略来包容所有的错误类别。 - 实际项目中对于API接口调用是通过RAL调用，通过配置RAL的相关超时信息设置。具体设置搞忘了。 6. 如何确定每一次接口数据查询范围？极端数据情况占比多少？如何考虑这部分的数据的处理？- 针对于活动类接口，数据查询范围是根据活动规则制定，那么如何制定活动规则保证尽可能所有多的用户满足活动？ - 另一方面，调用其他接口获取数据量也需要根据实际需求去保证接口响应时间的可控性。 - API接口设计的考虑因素：是否需要为一小部分数据的需求去增加所有的适配工作？还是为了保证大部分的使用体验，对于小部分进行抛出错误的设计？ - 如何根据现有资源和条件进行权衡? 7. 活动中有用到消息队列吗？- 消息队列解决的是将突发大量请求转换为后端能承受的队列请求。 - NMQ。 - 参考文章：[nmq消息队列解析](http://www.cnblogs.com/lushilin/p/6209976.html) 8. 自己的学习目标和计划？]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>mogujie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蘑菇街一面记录]]></title>
    <url>%2Fmogujie_interview.html</url>
    <content type="text"><![CDATA[蘑菇街应用平台开发工程师一面 面试官给自己的建议是，按部就班，面试偶然性很多，没有办法一应俱全，打好自己的基础才是最好的办法。 Java基础知识 Java集合中哪些是线程安全的？ vector：就比arraylist多了个同步化机制（线程安全），因为效率较低，现在已经不太建议使用。在web应用中，特别是前台页面，往往效率（页面响应速度）是优先考虑的。 statck：堆栈类，先进后出。 hashtable：内部的方法基本都是synchronized。 enumeration：枚举，相当于迭代器。 Java集合其他的呢？ 1234567891011121314151617java.util.Collection [I]+--java.util.List [I] +--java.util.ArrayList [C] +--java.util.LinkedList [C] +--java.util.Vector [C] +--java.util.Stack [C]+--java.util.Set [I] +--java.util.HashSet [C] +--java.util.SortedSet [I] +--java.util.TreeSet [C]java.util.Map [I]+--java.util.SortedMap [I] +--java.util.TreeMap [C]+--java.util.Hashtable [C]+--java.util.HashMap [C]+--java.util.LinkedHashMap [C]+--java.util.WeakHashMap [C] ArrayList与LinkedList区别，增删改查时间复杂度区别？ ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。 对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。 对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。 当操作是在一列数据的后面添加数据而不是在前面或中间,并且需要随机地访问其中的元素时,使用ArrayList会提供比较好的性能；当你的操作是在一列数据的前面或中间添加或删除数据,并且按照顺序访问其中的元素时,就应该使用LinkedList. ArrayList 是线性表（数组） get() 直接读取第几个下标，复杂度 O(1) add(E) 添加元素，直接在后面添加，复杂度O(1) add(index, E) 添加元素，在第几个元素后面插入，后面的元素需要向后移动，复杂度O(n) remove() 删除元素，后面的元素需要逐个移动，复杂度O(n) LinkedList 是链表的操作 get() 获取第几个元素，依次遍历，复杂度O(n) add(E) 添加到末尾，复杂度O(1) add(index, E) 添加第几个元素后，需要先查找到第几个元素，直接指针指向操作，复杂度O(n) remove() 删除元素，直接指针指向操作，复杂度O(1) 如何描述链表的数据结构 链表种类：单向链表、双向链表、单向循环链表和双向循环链表 单向链表：每个元素包含两部分：数据域和一个称为next的指针域。通过采用这种结构，next的指针域将按某一个方向指向其下一个元素。最后一个元素的next指针域指向NULL，即为空，它表示链表的末尾元素，链表的第一个元素称为“头”元素。 项目详细状况 报表日志项目流程 遇到哪些问题，如何解决？ MySQL手写语句1.三个表（school、class、student）连接查询，查找制定学校、学院、姓名的学号 1234SELECT 学生.id,学生.name FROM 学校,院系,学生WHERE 学校.id=院系.sid AND 学生.cid=院系.idAND 学校.name = &quot;大学&quot; and 院系.name = &quot;信息&quot; and 学生.name = &quot;小王&quot; 12345select 学生.id,学生.name from 学校 inner join 院系 on 学校.id=院系.sid inner join 学生 on 学生.cid=院系.id where 学校.name = &quot;大学&quot; and 院系.name = &quot;信息&quot; and 学生.name = &quot;小王&quot; 1子查询嵌套 from多表加上where和多个inner join表加上on条件查询结果一样的，都是做完笛卡尔积在从里面根据条件筛选数据。 优化核心在索引。 Linux手写语句 看过不如写过的感受 在123.txt中找到abc所在的行 grep –i “被查找的字符串” 文件名 cat 123.txt | grep -n ‘wang’ 输出所有找到的行数 grep -c “被查找的字符串” 文件名 cat 123.txt | grep -c ‘wang’ 输出所有匹配的行号 cat 123.txt | grep -n ‘wang’ | awk -F: ‘{print $1}’ grep命令原理 find命令是根据文件的属性进行查找。 grep是根据文件的内容进行查找，会对文件的每一行按照给定的模式(patter)进行匹配查找。 －c：只输出匹配行的计数。 －i：不区分大小写 －h：查询多文件时不显示文件名。 －l：查询多文件时只输出包含匹配字符的文件名。 －n：显示匹配行及行号。 －s：不显示不存在或无匹配文本的错误信息。 －v：显示不包含匹配文本的所有行。 grep是基于行的文本搜索 工具， 按匹配模式打印出符合条件的所有行。sed是流式文本编辑器，每次读入一行，处理一行； 开放性试题 如何设计斗地主的发牌过程 int CardValueArray[54]。我们将54个元素用来代表不同的牌。 CardValueArray[0——–12]: 方块A———方块K CardValueArray[13——-25]: 梅花A———梅花K CardValueArray[26——-38]: 红心A———红心K CardValueArray[39——-51]: 黑桃A———黑桃K CardValueArray[52] 小鬼 CardValueArray[53] 大鬼 规则：将数组分为4份。3份17张，1份3张（抢地主牌）。将3份17张牌，依次分发给3个不同的玩家。 方法一：Collections.shuffle(CardValueArray); 该方法方法用于随机排列随机使用一个默认的源指定的列表。然后依次取出17张牌即可。 方法二：顺序写入，随机生成0-53随机数，依次遍历交换，达到洗牌的目的。最后依次取出17张牌。 顺序遍历，每次生成一个随机位置，和当前位置的元素互换。运行时间是线性的。 面试官问的思路应该是如何把一个数组随机排列，并且减少交换次数？ 未来待续 参考文章：QQ 斗地主发牌是完全随机吗？]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>mogujie</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为成渝地区面试]]></title>
    <url>%2Fhuawei_interview.html</url>
    <content type="text"><![CDATA[华为成渝地区面试 每一家公司的特点的不同就会决定了不同的面试风格以及不同的侧重点记录每一个不同，完善自己 一面业务面 重点在于项目细节 手绘项目流程图 项目实现细节 自己负责的部分详细说明 Java基础知识 try-catch-finally 执行顺序 不管有木有出现异常，finally块中代码都会执行； 当try和catch中有return时，finally仍然会执行； finally是在return后面的表达式运算后执行的（此时并没有返回运算后的值，而是先把要返回的值保存起来，管finally中的代码怎么样，返回的值都不会改变，任然是之前保存的值），所以函数返回值是在finally执行前确定的； finally中最好不要包含return，否则程序会提前退出，返回值不是try或catch中保存的返回值。 任何执行try 或者catch中的return语句之前，都会先执行finally语句，如果finally存在的话。如果finally中有return语句，那么程序就return了，所以finally中的return是一定会被return的，编译器把finally中的return实现为一个warning。 参考文章：有return的情况下try catch finally的执行顺序（最有说服力的总结）http://blog.csdn.net/kavensu/article/details/8067850 描述线程安全： 线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。 线程不安全就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。 Java中对于线程安全的机制。 加锁。(1) 锁能使其保护的代码以串行的形式来访问，当给一个复合操作加锁后，能使其成为原子操作。一种错误的思想是只要对写数据的方法加锁，其实这是错的，对数据进行操作的所有方法都需加锁，不管是读还是写。(2) 加锁时需要考虑性能问题，不能总是一味地给整个方法加锁synchronized就了事了，应该将方法中不影响共享状态且执行时间比较长的代码分离出去。(3) 加锁的含义不仅仅局限于互斥，还包括可见性。为了确保所有线程都能看见最新值，读操作和写操作必须使用同样的锁对象。 不共享状态。(1) 无状态对象： 无状态对象一定是线程安全的，因为不会影响到其他线程。(2) 线程关闭： 仅在单线程环境下使用。 不可变对象。可以使用final修饰的对象保证线程安全，由于final修饰的引用型变量(除String外)不可变是指引用不可变，但其指向的对象是可变的，所以此类必须安全发布，也即不能对外提供可以修改final对象的接口。 参考文章：Java并发编程——线程安全及解决机制简介 二面综合面 综合面最大的感觉是类似压力面的形式，多个综合性问题，反应时间短. 自我介绍 自己的优缺点。紧接着用实例描述自己的缺点。 你认为自己的编程水平在你的专业是什么样的水平？ 自己认为目前行业前沿的变成设计趋势是哪些？如何理解微服务？ 你认为优秀的编程人员和普通的编程人员的区别在哪里？ 你认为代码能力的提高你会从哪些方面着手？ 在你的眼中华为是一个什么样的公司，为什么选择来到华为？ 你认为你来到华为自己具备什么样的实力？ 如果在华为你的付出与你的回报不成正比，或者说不公平的现象在你身上发生，你该怎么办？ 你在成都学习生活，你对于成都的印象，你的第一意向城市是哪里？ 未完待续 等待结果]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好未来面试记录]]></title>
    <url>%2Fhaoweilai_interview.html</url>
    <content type="text"><![CDATA[好未来面试记录 通过提前批筛选之后就是二次现场面试。 相对于机器学习的面试来说，自己的难度确实简单，需要查漏补缺的地方还是很多。 一面1.如何处理hadoop的数据倾斜问题针对问题是，请描述一个自己印象最深刻的项目 数据倾斜表现：ruduce阶段卡在99.99%，一直99.99%不能结束。 数据分布不均匀，导致大量的数据分配到了一个节点。 解决思路： 业务逻辑：我们从业务逻辑的层面上来优化数据倾斜。对于数据同一类型数量巨大，单独来做count，最后和其他数据做整合。这也是面试官想要引导我说出的方案 调参方面，Hadoop和Spark都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。 mapjoin方式 count distinct的操作，先转成group，再count 万能膏药：hive.groupby.skewindata=true left semi jioin的使用 设置map端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了IO读写和网络传输，能提高很多效率） 通过randint等函数把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。这是自己的回答。 参考文章： 漫谈千亿级数据优化实践：数据倾斜（纯干货） [Hive数据倾斜]http://www.cnblogs.com/skyl/p/4855099.html() 2.Java多线程线程池的参数意义- 线程池解决的两个问题： - 1）线程池通过减少每次做任务的时候产生的性能消耗来优化执行大量的异步任务的时候的系统性能。 - 2）线程池还提供了限制和管理批量任务被执行的时候消耗的资源、线程的方法。 Executors提供的一些工厂方法来快速创建ThreadPoolExecutor实例: 1.newCachedThreadPool创建一个可缓存线程池，如果线程池长度超处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 2.newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 3.newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 4.newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 5.newWorkStealingPool JDK8引入。返回一个ForkJoinPool类型的 executor，它的工作方法与其他常见的execuotr稍有不同。与使用一个固定大小的线程池不同，ForkJoinPools使用一个并行因子数来创建，默认值为主机CPU的可用核心数。 - 线程池构造器参数： - corePoolSize：核心池的大小，这个参数与后面讲述的线程池的实现原理有非常大的关系。在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，除非调用了prestartAllCoreThreads()或者prestartCoreThread()方法，从这2个方法的名字就可以看出，是预创建线程的意思，即在没有任务到来之前就创建corePoolSize个线程或者一个线程。默认情况下，在创建了线程池后，线程池中的线程数为0，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； - maximumPoolSize：线程池最大线程数，它表示在线程池中最多能创建多少个线程； - keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止。默认情况下，只有当线程池中的线程数大于corePoolSize时，keepAliveTime才会起作用，直到线程池中的线程数不大于corePoolSize：即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize；但是如果调用了allowCoreThreadTimeOut(boolean)方法，在线程池中的线程数不大于corePoolSize时，keepAliveTime参数也会起作用，直到线程池中的线程数为0； - unit：参数keepAliveTime的时间单位，有7种取值，在TimeUnit类中有7种静态属性 - 参考文章： - [Java线程池（ThreadPool）详解](http://www.cnblogs.com/kuoAT/p/6714762.html) 3.如何设置Java多线程个数最合理- 对于计算密集型的任务，在拥有N个处理器的系统上，当线程池的大小为N+1时，通常能实现最优的效率。(即使当计算密集型的线程偶尔由于缺失故障或者其他原因而暂停时，这个额外的线程也能确保CPU的时钟周期不会被浪费。) - 如果是IO密集型应用，则线程池大小设置为2N+1 - 参考文章： - [java线程池大小为何会大多被设置成CPU核心数+1？](https://www.zhihu.com/question/38128980) 4.用尽可能多的方法去实现判断数组回文1. 处理小数字。使用数学方法。取每一位数字然后从后往前计算值，与原来数进行比较是否相等。 2. 处理大数字。使用字符串处理方式。因为回文数关于中心对称，只要比较对称的数即可。 3. 使用栈的思想。入栈一半元素，比较出栈元素与串中字符。 4. 回文判断不能使用正则表达式。正则表达式对应的是有限状态自动机，要达到你所说的需求，至少要图灵机。[正则表达式能否解决所有的字符串的模式匹配问题?](https://www.zhihu.com/question/32327623) - 参考文章： - [回文数的判断(三种方法)](http://blog.csdn.net/deaidai/article/details/71820164) 5.reverse函数源码实现- StringBuffer和StringBuilder常用到的方法，而String并没有这个牛逼的功能~~ 12345678910111213141516171819202122 public AbstractStringBuilder reverse() &#123; boolean hasSurrogates = false; int n = count - 1; //j初始化，长度-2再算术右移一位 j = (count-2)/2 //偶数长度，遍历一半次数，对调替换 //奇数长度，遍历一半-1次数，对调替换，中间值不用替换 for (int j = (n-1) &gt;&gt; 1; j &gt;= 0; j--) &#123; int k = n - j; char cj = value[j]; char ck = value[k]; value[j] = ck; value[k] = cj; if (Character.isSurrogate(cj) || Character.isSurrogate(ck)) &#123; hasSurrogates = true; &#125; &#125; if (hasSurrogates) &#123; reverseAllValidSurrogatePairs(); &#125; return this;&#125; 二面1.如何让接口发生改变时，使得调用方调用无感变化情况- 百度了下：采用调用中间代理方法 - 设计模式：Adpater。接口调用者不变。协议改变时由Adapter做转换。 1.session和cookie- Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中； - Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。]]></content>
      <categories>
        <category>面试记录</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度运维部交流会学习笔记]]></title>
    <url>%2FbaiduOPCoumminate.html</url>
    <content type="text"><![CDATA[百度运维部交流会学习笔记地点：电子科技大学时间：2017.08.20 最大的感受：随着AI的不断发展，自己未来工作中会遇到越来越多AI场景，要去拥抱这样的变化。自己希望工作之后也可以回到母校为学弟学妹们带来这样一场业界前沿、职场相关的交流会。 1. AIOps AIOps 定义：按照 Gartner 的定义，AIOps 是 Algorithmic IT Operations，但是在人工智能时代，可能很多的人会把 AI 理解成 Artificial Intelligence，不去纠结定义，我觉得本质上，想要表达的意思是一样的，就是让运维具备机器学习和算法的能力。 特征： 高质量 可靠用户访问体验 低成本 高效率 发展过程：最开始的人工填单、到Web自动化。由于计算成本的下降，导致机器的数量快速增长，这就加剧了运维成本上升，智能运维代替人工，降低成本就应运而生。 讲解单机房故障的止损以及排查方案 BGW与BFE之间的故障 后端服务故障 自己的感受：运维中的海量日志信息蕴含的故障问题原因以及其他相关信息都是可以数据挖掘机器学习去实现。所有的大数据行业都存在这样的过程。关键在与运维知识库：将传统运维基于人的经验，操作行为、规则固化至知识库中。如何将问题的特征提取出来，标签尽可能细化，比如地点标签精确到大学，那么检测到该地点流量下降就可以推测该地点存在故障，此外还有日期特征，等等，如何将尽可能多地相关特征提取出来，匹配最佳的知识库中的运维方案，这也是机器学习的重点吧，自己在这方面的技术知识太少，需要制定学习计划完善自己。 2. BFE BFE: 百度统一前端，Baidu Front End 对标Google的GFE 它相对于整个应用是处于最前面直接处理用户的http请求。 采用Go语言编写 基于机器学习的动态优化，识别机器访问与真实访问 全站HTTPS，有效避免网络信息劫持 3. 职场建议 小事做好，自然轮到做大事 自我驱动去完成项目比分配任务去完成更加提升自己 运维是架构的学堂。听完了分享，我是认可这句话。单纯的从业务方面是专注于业务一点，运维是需要掌握上下游所有的流程，从这点来看，运维入门难，最后深度与广度是更优的。]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>AIOps</tag>
        <tag>BFE</tag>
        <tag>Baidu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[菜鸟物流简历评估面试]]></title>
    <url>%2Falibaba_interview_0820.html</url>
    <content type="text"><![CDATA[菜鸟物流简历评估面试记录 刚睡醒的面试一塌糊涂，不打没有准备的仗。总结经验，扎实基础 问题记录1. 自我介绍两方面简单自我介绍：公司实习经历和学校项目。最后总结与申请职位的契合程度以及表明下自己的意愿。PS：感觉并没有什么用。。。 2. 你的简历中关于Java的很少，为什么还要选择投递研发工程师Java呢？ 当自己听到这个问题的时候，自己就懵了，和自己之前复习准备，参考其他菜鸟网络面经有很大的不一样。 每一位面试官都有自己的风格。 自己还是主动把学校项目中的Java部分阐述，以及在公司实习中采用PHP以及Python同时可以表现出自己的自学能力。 但是面试官的反馈是，从你的简历上无法看到Java相关的内容，无法谈论Java技术方面的问题。自己在项目中的SSH框架以及Java与Hadoop的项目经历放在实习经历后面，对于Java的具体实现功能自己之前因为一页简历的限制并没有详细叙述，这是自己的一个缺点。 自己面试完成后总结自己应该向面试官表达自己对于Java研发工程师岗位的浓厚兴趣以及自己在项目实践，博客记录等自己所做的工作。 自己对于压力面试的准备不是很充分，在面对压力以及工作之后的困难应该表现出进取向上的态度吧。 3. 那么你用过Java、PHP、Python这些不同的编程语言，你认为它们最大的区别在哪里？ 听到这个问题，我又懵了。自己的横向广度可以，但是纵向深度缺乏。 百度整理答案以及自己实践理解如下： Java：Java 编程语言是个简单、面向对象、分布式、解释性、健壮、安全与系统无关、可移植、高性能、多线程和动态的语言。 PHP：Hypertext Preprocessor的缩写，中文名：“PHP：超文本预处理器”）是一种通用开源脚本语言。语法吸收了C语言、Java和Perl的特点，入门门槛较低，易于学习，使用广泛，主要适用于Web开发领域。 Python：是一种面向对象、直译式计算机程序设计语言，Python语法简洁而清晰，具有丰富和强大的类库。它常被昵称为胶水语言，它能够很轻松的把用其他语言制作的各种模块（尤其是C/C++）轻松地联结在一起。 参考文章：C、C++、C#、Java、php、python语言的内在特性及区别 通过不同语言与MySQL数据库连接过程，发现不同语言之间的特点差异：六种流行的语言—C、C++、python、Java、php、C#比较 总结：自己在使用编程语言的时候，会用是基础，为什么这么用，这么用的好处，设计的原则这些都要好好理解。 4. Java基础知识 Object对象的方法有哪些？记得但是没有答全。重新阅读Java.lang.Object.java文件源码，温故知新。 Java中，用native关键字修饰的函数表明该方法的实现并不是在Java中去完成，而是由C/C++去完成，并被编译成了.dll，由Java去调用。 private static native void registerNatives();//返回此Object运行时类型 public final native Class&lt;?&gt; getClass();//返回对象的哈希值 public native int hashCode();//判断其他对象是否与此对象”相等” public boolean equals(Object obj);//创建并返回此对象的一个副本 protected native Object clone() throws CloneNotSupportedException;//返回此对象的字符串表示 public String toString();//唤醒在此对象监视器上等待的单个线程 public final native void notify();//唤醒在此对象监视器等待的所有线程 public final native void notifyAll();//在其他线程调用此对象的notify()方法或notifyAll()方法前，或者超过指定的时间量前，让当前线程等待 public final void wait() throws InterruptedException; public final native void wait(long timeout) throws InterruptedException; public final void wait(long timeout, int nanos) throws InterruptedException//当垃圾回收器确定不存在对该对象的更多引用时，由对象的垃圾回收器调用此方法 protected void finalize() throws Throwable; 参考文章：Java源码解读：顶级父类Object hashCode()方法重写与equals()方法的关系。这个答上来了。 equals方法和hashCode方法的关系 如果重写了equals方法，则必须重写hashCode防止两个等价的对象的hashCode值不同，这在集合中将造成严重后果。 如果equals成立，则hashCode必须相同，如果hashCode不同，则equals则一定不成立。 如果两个对象各自调用hashCode方法产生的不同结果，对这两个对象进行equals方法的比较一定要返回false。 如果根据equals方法比较。两个对象时等价的，那么在两个对象中各自调用hashCode方法必须产生相同的整数结果。 如果根据equals方法比较，两个对象不等价，那么在两个对象中各自调用hashCode方法不一定会产生不同的整数结果。 final，finally，finalize区别。这个答上来了。现在回想自己回答的不够简练。 final: 常量声明。 finally: 处理异常。 finalize: 帮助进行垃圾回收。 接口里声明的变量默认是final的。final类无法继承，也就是没有子类。这么做是出于基础类型的安全考虑，比如String和Integer。这样也使得编译器进行一些优化，更容易保证线程的安全性。final方法无法重写。final变量的值不能改变。finalize()方法在一个对象被销毁和回收前会被调用。finally,通常用于异常处理，不管有没有异常被抛出都会执行到。比如，关闭连接通常放到finally块中完成。 参考文章：Java面试题与解答(一)——基础部分 介绍序列化。并提出当一个对象序列化之后又添加一个字段，当其被反序列化时会发生什么？这个问题后半部分答得不好，说在类加载的验证过程中不通过报错。感觉不对。 序列化的思想是 “冻结” 对象状态，传输对象状态（写到磁盘、通过网络传输等等），然后 “解冻” 状态，重新获得可用的 Java 对象。 序列化允许重构：序列化允许一定数量的类变种，甚至重构之后也是如此，ObjectInputStream 仍可以很好地将其读出来。 Java Object Serialization 规范可以自动管理的关键任务是：将新字段添加到类中；将字段从 static 改为非 static；将字段从 transient 改为非 transient。 序列化新加字段，前一个类与后一个类必须有有相同的序列化版本 hash（存储为 private static final serialVersionUID 字段）。 一旦有了serialVersionUID，不仅可以从原始对象的序列化数据创建新加字段对象（当出现新字段时，新字段被设为缺省值，最常见的是“null”），还可以反过来做：即从新加字段对象的数据通过反序列化得到原始对象。 序列化文件格式为二进制文件流： 0xACED:根据协议文档的约定,由于所有二进制流(文件)都需要在流的头部约定魔数(magic number=STREAM_MAGIC),既java object 序列化后的流的魔数约定为ACED; 0x0005:然后是流协议版本是short类型(2字节)并且值为5,则十六进制值为0005; 0x7372:java byte型长度为1字节,所以0x73 0x72直接对应到字节流上,0x73(TC_OBJECT)代表这是一个对象,0x72(TC_CLASSDESC)代表这个之后开始是对类的描述. 0x0003:类名的长度,这个类名是Pet,是三个字符,所以长度是3,对应16进制中就是0x0003. 0x506574:这三个字节转为ASCII码就是类名Pet。 序列化存在安全风险，信任，但要验证。可以实现 ObjectInputValidation接口，并覆盖 validateObject() 方法。如果调用该方法时发现某处有错误，则抛出一个 InvalidObjectException。 参考文章： 关于 Java 对象序列化您不知道的 5 件事 Java序列化格式详解 接下来转为数据结构，问的基础问题。但是越是基础，越容易犯眼高手低的毛病。什么是二叉树？什么是平衡二叉树？ 自己从数据结构的实现方面阐述被打断。要求用自己的理解来阐述二叉树的结构。 二叉树是每个节点最多有两个子树的树结构。 二叉树的每个结点至多只有二棵子树(不存在度大于2的结点)，二叉树的子树有左右之分，次序不能颠倒。二叉树的第i层至多有2^{i-1}个结点；深度为k的二叉树至多有2^k-1个结点；对任何一棵二叉树T，如果其终端结点数为n_0，度为2的结点数为n_2，则n_0=n_2+1。 平衡二叉树（Self-balancing binary search tree）又被称为AVL树（有别于AVL算法），且具有以下性质：它是一 棵空树或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。常用算法有红黑树、AVL、Treap、伸展树等。在平衡二叉搜索树中，我们可以看到，其高度一般都良好地维持在O（log（n）），大大降低了操作的时间复杂度。 自己把平衡二叉树和二叉查找树弄反了，汗颜。 二叉排序树或者是一棵空树，或者是具有下列性质的二叉树： 若左子树不空，则左子树上所有结点的值均小于或等于它的根结点的值； 若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值； 左、右子树也分别为二叉排序树； 参考文章：平衡二叉树 算法题：给定一个数组A，一个数组B，两个数组元素均是数字，只允许分别循环遍历每个数组1次，求出： 1. A中有B中没有的元素集合； 2. A中没有B中有的元素集合； 3. A中有B中也有的元素集合。 思路：桶排序（位图）来解决，但是还是要多一次桶的循环。HashMap也是一样的思路。 升级版思路：TreeSet来解决,遍历A构建TreeSet，遍历B通过add返回值来判断，同时再remove该元素。 桶排序思路改进：遍历A数组，添加时+1；遍历B数组，添加时+2；对应三种情况值分别为1,2,3。最后遍历输出即可。设置不同的权值来进行区分，重复元素不再重复相加即可。 当数组数据有序和无序两种情况下，算法有哪些不同？ 无序最终也要归结为有序来处理吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148package com.alibaba.interview;import java.util.*;/** * Created by ml on 2017/8/20. * 给定一个数组A，一个数组B，两个数组元素均是数字，只允许分别循环遍历每个数组1次，求出： 1. A中有B中没有的元素集合； 2. A中没有B中有的元素集合； 3. A中有B中也有的元素集合。 */public class findNumber &#123; public static void main(String[] args) &#123; int[] testA = &#123; 5, 9, -4, 10, 21, 16&#125;; int[] testB = &#123; 10, 8, 4, 5, -3 &#125;; int[] test2A = &#123;&#125;; int[] test2B = &#123;&#125;; calSorted(testA, testB); calSorted2(testA, testB); calRandom(test2A,test2B); &#125; //自己当时的思路，桶排序但是只能实现两种 private static void calSorted(int[] testA, int[] testB) &#123; //数组有序 Arrays.sort(testA); Arrays.sort(testB); //确定最大值和最小值 int len = testA.length &lt;= testB.length ? testB.length : testA.length; int min = testA[0] &lt;= testB[0] ? testA[0] : testB[0]; int max = testA[testA.length - 1] &lt;= testB[testB.length - 1] ? testB[testB.length - 1] : testA[testA.length - 1]; int[] tong = new int[max - min + 1]; for(int i = 0; i &lt; testA.length;i++) &#123; tong[testA[i] - min]++; &#125; //A中有B中没有的元素集合 List&lt;Integer&gt; A1B0 = new ArrayList&lt;&gt;(); //A中没有B中有的元素集合 List&lt;Integer&gt; A0B1 = new ArrayList&lt;&gt;(); //A中有B中也有的元素集合 List&lt;Integer&gt; A1B1 = new ArrayList&lt;&gt;(); for(int i = 0; i &lt; testB.length;i++) &#123; //A中没有B中有的元素 if(tong[testB[i] - min] == 0) &#123; A0B1.add(testB[i]); &#125; //A中有B中也有的元素 if(tong[testB[i] - min] &gt; 0) &#123; A1B1.add(testB[i]); tong[testB[i] - min]--; &#125; //A中有B中没有的元素无法搞定 &#125; //必须要添加一层循环 for(int i = 0; i &lt; tong.length;i++) &#123; if(tong[i] &gt; 0) &#123; A1B0.add(i + min); &#125; &#125; System.out.println("A中没有B中有的元素:"); printList(A0B1); System.out.println("A中有B中没有的元素:"); printList(A1B0); System.out.println("A中有B中也有的元素:"); printList(A1B1); &#125; //改进版 private static void calSorted2(int[] testA, int[] testB) &#123; TreeSet&lt;Integer&gt; treeA = new TreeSet&lt;&gt;(); for(int i = 0; i &lt; testA.length; i++) &#123; treeA.add(testA[i]); &#125; //A中有B中没有的元素集合 List&lt;Integer&gt; A1B0 = new ArrayList&lt;&gt;(); //A中没有B中有的元素集合 List&lt;Integer&gt; A0B1 = new ArrayList&lt;&gt;(); //A中有B中也有的元素集合 List&lt;Integer&gt; A1B1 = new ArrayList&lt;&gt;(); for(int j = 0; j &lt; testB.length; j++) &#123; //A中没有B中有的元素 if(treeA.add(testB[j])) &#123; A0B1.add(testB[j]); treeA.remove(testB[j]); &#125; else &#123; A1B1.add(testB[j]); treeA.remove(testB[j]); &#125; &#125; System.out.println("A中没有B中有的元素:"); printList(A0B1); System.out.println("A中有B中没有的元素:"); printSet(treeA); System.out.println("A中有B中也有的元素:"); printList(A1B1); &#125; private static void calRandom(int[] test2A, int[] test2B) &#123; &#125; private static void printArr(int[] arr) &#123; for(int i = 0; i &lt; arr.length - 1; i++) &#123; System.out.print(arr[i] + " "); &#125; System.out.print(arr[arr.length - 1]); System.out.println(); &#125; private static void printList(List&lt;Integer&gt; list) &#123; for(int i = 0; i &lt; list.size() - 1; i++) &#123; System.out.print(list.get(i) + " "); &#125; System.out.print(list.get(list.size() - 1)); System.out.println(); &#125; private static void printSet(TreeSet&lt;Integer&gt; tree) &#123; Iterator&lt;Integer&gt; it=tree.iterator(); int i=0; while(it.hasNext())&#123; i++; if(i&lt;tree.size())&#123; System.out.print(it.next()+" "); &#125;else&#123; System.out.print(it.next()); &#125; &#125; System.out.println(); &#125;&#125; 验证结果：123456789101112A中没有B中有的元素:-3 4 8A中有B中没有的元素:-4 9 16 21A中有B中也有的元素:5 10A中没有B中有的元素:-3 4 8A中有B中没有的元素:-4 9 16 21A中有B中也有的元素:5 10]]></content>
      <categories>
        <category>alibaba</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习《阿里巴巴Java开发手册》]]></title>
    <url>%2Flearn_alibaba_java.html</url>
    <content type="text"><![CDATA[经验，是提高自己的重要途径之一。 学习笔记：编程规约命名风格 “骆驼拼写法”分为两种。第一个词的首字母小写，后面每个词的首字母大写，叫做“小骆驼拼写法”（lowerCamelCase）；第一个词的首字母，以及后面每个词的首字母都大写，叫做“大骆驼拼写法”（UpperCamelCase），又称“帕斯卡拼写法”。类名大骆驼，方法名、参数名、成员变量、局部变量都是小骆驼。 POJO类（只有getter和setter方法的简单类）：布尔类型变量不要加is，否则会引起部分框架解析引起序列化错误。 接口类的方法和属性不要加任何修饰符号，包括public。JDK8允许接口有默认实现，default方法。 基于SOA理念，暴露出来的一定是接口，内部实现类加Impl后缀。 常量定义 常量的复用层次： 跨应用共享常量 应用内共享常量 子工程内共享常量 包内共享常量 类内共享常量： private static final 变量值在范围内变化，设置为枚举类。成员名称全部大写。 OOP规约 直接通过类名访问类的静态变量和静态方法，而不是通过类的对象，增加编译器的解析成本。 所有覆写方法都必须加上@override注解。 所有相同类型的包装对象之间的值的比较，通过equals方法。 原因：Integer在128 127 分为内的复制，Integer对象是在IntegerCache.cache产生，会复用已有的对象，区间之外的所有数据都在堆上产生，并不会复用已有对象。 == 不仅比较值的大小，还比较对象的地址。 POJO类属性必须使用包装类型，返回值和参数也必须使用包装数据类型。预防NPE现象。 基本类型 包装器类型 boolean Boolean char Character int Integer byte Byte short Short long Long float Float double Double 使用索引访问String的split方法，需要对最后一个分隔符后面有无内容进行检查，否则会报IndexOutOfBoundsException。 public String[] split(String regex,int limit)方法：split(String regex) 方法，其实也就等同于split(String regex，0)方法，把结尾的空字符串丢弃！ 可以使用split(“分隔符”,1)或者是org.apache.commons.lang.StringUtils提供的split 参考文章：java 字符串split有很多坑，使用时请小心！！ 循环体内字符串的链接方式： 用String和“+”：因为“+”拼接字符串，每拼接一次都是再内存重新开辟一个新的内存区域（堆里边）,然后把得到的新的字符串存在这块内存，很容易引起内存溢出。 使用StringBuilder的append方法进行扩展。是在已有的内存空间追加的字符串。 commonlang工具包的StringUtils.join(list,”,”);来一步实现这个拼接而且还能指定分隔的符号。 对象的clone方法默认是前拷贝，实现深拷贝需要重写clone方法。 基本数据类型的拷贝是没有意义的，String类型这样的引用的拷贝才是有意义的。 需要注意的是，如果在拷贝一个对象时，要想让这个拷贝的对象和源对象完全彼此独立，那么在引用链上的每一级对象都要被显式的拷贝。所以创建彻底的深拷贝是非常麻烦的，尤其是在引用关系非常复杂的情况下， 或者在引用链的某一级上引用了一个第三方的对象， 而这个对象没有实现clone方法， 那么在它之后的所有引用的对象都是被共享的。 参考文章：详解Java中的clone方法 — 原型模式 集合处理 只要重写equals方法，就必须重写hashCode。 为了保证同一个对象，保证在equals相同的情况下hashcode值必定相同，如果重写了equals而未重写hashcode方法，可能就会出现两个没有关系的对象equals相同的（因为equal都是根据对象的特征进行重写的），但hashcode确实不相同的。 Set存放不重复队形，先比较hashCode，再用equals比较，提高效率。 String两个方法都重写了，放心使用。 参考文章：为什么重写equals时必须重写hashCode方法？ ArrayList之subList： Java.util.List中有一个subList方法，用来返回一个list的一部分的视图。 1List&lt;E&gt; subList(int fromIndex, int toIndex); 它返回原来list的从[fromIndex, toIndex)之间这一部分的List(下面称之为sublist)，但是这个sublist是依赖于原来的List集合。 在subList中进行了结构性修改（list大小修改），原来的list的大小也会发生变化，抛出一个ConcurrentModificationException。 集合转为数组的方法，必须使用集合的toArray(T[] array)，类型与大小完全一致。 不带参数的toArray方法，是构造的一个Object数组，然后进行数据拷贝，此时进行转型就会产生ClassCastException。 Arrays.asList()数组转为集合方法，不能使用挂起修改集合相关的方法，如add、remove、clear等，会抛出UnsupportedOperationException异常。 设计模式：适配器模式。只是转换接口，后台的数据仍是数组。 Arrays.asList方法返回的ArrayList是继承自AbstractList同时实现了RandomAccess和Serializable接口，AbstractList定义add等方法抛出异常。 解决方法： 12345//1List&lt;Integer&gt; list = new ArrayList&lt;&gt;(Arrays.asList(1,2,3));//2int i[]=&#123;11,22,33&#125;; Arrays.asList(ArrayUtils.toObject(i)); Java泛型通配符PECS原则： 如果要从集合中读取类型T的数据，并且不能写入，可以使用 ? extends 通配符；(Producer Extends) 如果要从集合中写入类型T的数据，并且不需要读取，可以使用 ? super 通配符；(Consumer Super) 如果既要存又要取，那么就不要使用任何通配符。 不要在foreach循环中进行元素的remove/add操作。如果要remove，需在Iterator中。如果并发，需给Iterator加锁。 List类会在内部维护一个modCount的变量，用来记录修改次数。 每生成一个Iterator，Iterator就会记录该modCount，每次调用next()方法就会将该记录与外部类List的modCount进行对比，发现不相等就会抛出多线程编辑异常。 1234//foreach和迭代器的hasNext()方法，foreach这个语法糖，实际上就是while(itr.hasNext())&#123; itr.next()&#125; 123public boolean hasNext() &#123; return cursor != size;&#125; cursor是用于标记迭代器位置的变量，该变量由0开始，每次调用next执行+1操作。 你的代码在执行删除“1”后，size=1，cursor=1，此时hasNext()返回false，结束循环，因此你的迭代器并没有调用next查找第二个元素，也就无从检测modCount了，因此也不会出现多线程修改异常但当你删除“2”时，迭代器调用了两次next，此时size=1，cursor=2，hasNext()返回true，于是迭代器傻乎乎的就又去调用了一次next()，因此也引发了modCount不相等，抛出多线程修改的异常。 当你的集合有三个元素的时候，你就会神奇的发现，删除“1”是会抛出异常的，但删除“2”就没有问题了，究其原因，和上面的程序执行顺序是一致的。 参考文章：为什么java不要在foreach循环里进行元素的remove/add操作 在 JDK 7 版本以上， Comparator 要满足自反性，传递性，对称性，不然 Arrays.sort ，Collections.sort 会报 IllegalArgumentException 异常。 保证等于和大小与分开，要严格有序。 Comparable 是排序接口。若一个类实现了Comparable接口，就意味着“该类支持排序”。 Comparable 接口仅仅只包括一个函数： Comparator 是比较器接口。我们若需要控制某个类的次序，而该类本身不支持排序(即没有实现Comparable接口)，可以通过“实现Comparator类来新建一个比较器”，然后通过该比较器对类进行排序。 Comparable相当于“内部比较器”，而Comparator相当于“外部比较器”。 1234567891011package java.lang;import java.util.*;public interface Comparable&lt;T&gt; &#123; public int compareTo(T o);&#125;public interface Comparator&lt;T&gt; &#123; int compare(T o1, T o2); boolean equals(Object obj);&#125; 集合初始化，指定集合的初始值大小。 Collection的初始容量也显得异常重要。所以：对于已知的情景，请为集合指定初始容量。 HashMap初始化容量计算 = （需要存储元素的个数 / 负载因子） + 1 12345678910111213141516171819202122// ArrayList新容量扩大到原容量的1.5倍，右移一位相关于原数值除以2。int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);//Vector线程安全，速度慢。默认初始容量为10，加载因子为1：即当 元素个数 超过 容量长度 时，进行扩容扩容增量：原容量的1倍int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity);//HashMapstatic final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;static final float DEFAULT_LOAD_FACTOR = 0.75f;if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1;//HashTableint newCapacity = oldCapacity * 2 + 1;//StringBuildervoid expandCapacity(int minimumCapacity) &#123; int newCapacity = value.length * 2 + 2; if (newCapacity minimumCapacity &lt; 0) newCapacity = minimumCapacity; if (newCapacity &lt; 0) &#123; if (minimumCapacity &lt; 0) // overflow throw new OutOfMemoryError(); newCapacity = Integer.MAX_VALUE; &#125; value = Arrays.copyOf(value, newCapacity); &#125; 集合名称 默认容量 加载因子 扩容容量(扩大到原来的) ArrayList 10 1 1.5 Vector 10 1 2 HashSet 16 0.75 2 HashMap 16 0.75 2 HashTable 11 0.75 *2 +1 StringBuilder（StringBuffer） 16 条件判断 *2+2 遍历Map类，使用entrySet。 同时遍历key和value时，keySet与entrySet方法的性能差异取决于key的具体情况，如复杂度（复杂对象）、离散度、冲突率等。换言之，取决于HashMap查找value的开销。entrySet一次性取出所有key和value的操作是有性能开销的，当这个损失小于HashMap查找value的开销时，entrySet的性能优势就会体现出来。 同时遍历key和value时，与HashMap不同，entrySet的性能远远高于keySet。这是由TreeMap的查询效率决定的，也就是说，TreeMap查找value的开销较大，明显高于entrySet一次性取出所有key和value的开销。因此，遍历TreeMap时强烈推荐使用entrySet方法。 123456789101112131415//keySet遍历2次，一次转为Iterator对象，一次从HashMap中取出对应的valuefor (String key : map.keySet()) &#123; value = map.get(key);&#125;//entrySetfor (Entry&lt;String, String&gt; entry: map.entrySet()) &#123; key = entry.getKey(); value = entry.getValue();&#125;//for循环for (String value : map.values()) &#123;&#125;//JDK8 增强for循环Map&lt;String, Integer&gt; items = new HashMap&lt;&gt;();items.forEach((k,v)&gt;System.out.println("key : " + k + "; value : " + v)); 参考文章：Java Map遍历方式的选择 Map类集合K/V为null需要注意的地方： 集合类 Key Value Super 说明 Hashtable 不允许为 null 不允许为 null Dictionary 线程安全 ConcurrentHashMap 不允许为 null 不允许为 null AbstractMap 分段所锁技术 TreeMap 不允许为 null 允许为 null AbstractMap 线程不安全 HashMap 允许为 null 允许为 null AbstractMap 线程不安全 由于 HashMap的干扰，很多人认为 ConcurrentHashMap是可以置入 null值，注意存储null值时会抛出 NPE异常。 集合的有序性和稳定性：ArrayList是order/unsort，HashMap是unorder/unsort，TreeSet是order/sort 利用Set唯一性去重，避免使用List的contains方法遍历去重。Collection的contains()和remove()操作都是线性时间复杂度，用set也会隐式的调用contains()方法，不过你用的是HashSet,这个contains()应该只会用常数时间，所以如果考虑平均时间复杂度，用set可能会占优；最坏情况下，两者可能差不多 。 并发处理 获取单例对象，保证线程安全，以及方法的线程安全。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//立即加载（饿汉模式），在调用getInstance()方法前，实例就被创建了，getInstance()方法没有同步，所以可能出现非线程安全问题。public class Singleton &#123; private final static Singleton INSTANCE = new Singleton(); private Singleton() &#123; &#125; public static Singleton getInstance() &#123; return INSTANCE; &#125;&#125;//延迟加载（懒汉模式），延迟加载就是在getInstance()方法中创建实例。在多线程的环境中，延迟加载中使用同步代码块，对类加锁。虽然做到了线程安全，并且解决了多实例的问题，但是它并不高效。因为在任何时候只能有一个线程调用 getInstance() 方法。public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125;//DCL双检查锁机制。 DCL双检查锁机制即使用volatile关键字（使变量在多个线程中可见）修改对象和synchronized代码块//两次检查 instance == null，一次是在同步块外，一次是在同步块内。为什么在同步块内还要再检验一次？因为可能会有多个线程一起进入同步块外的 if，如果在同步块内不进行二次检验的话就会生成多个实例了。//instance = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。//给 instance 分配内存//调用 Singleton 的构造函数来初始化成员变量//将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了）//JVM 的即时编译器中存在指令重排序的优化，只需要将 instance 变量声明成 volatile 来避免指令重排序。public class Singleton &#123; private static volatile Singleton singleton = null; private Singleton()&#123;&#125; public static Singleton getSingleton()&#123; if(singleton == null)&#123; synchronized (Singleton.class)&#123; if(singleton == null)&#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125;//【推荐】静态内部类//使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。//但是如果对象是序列化的就无法达到效果了。public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125;//枚举//枚举的缺点是它无法从另一个基类继承，因为它已经继承自java.lang.Enum。public enum FooEnumSingleton &#123; INSTANCE; public static FooEnumSingleton getInstance() &#123; return INSTANCE; &#125; public void bar() &#123; &#125;&#125; 线程资源通过线城池提供，通过ThreadPoolExecutor方式创建。 ThreadPoolExecutor作为java.util.concurrent包对外提供基础实现，以内部线程池的形式对外提供管理任务执行，线程调度，线程池管理等等服务 Executors方法提供的线程服务，都是通过参数设置来实现不同的线程池机制。 关系：Executors可以认为是封装好的线城池服务，ThreadPoolExecutor更加明确线程池的运行机制。 Executors.newCachedThreadPool(); ``//创建一个缓冲池，缓冲池容量大小为Integer.MAX_VALUE，允许创建线程为Integer.MAX_VALUE,容易OOM Executors.newSingleThreadExecutor(); ``//创建容量为1的缓冲池，请求队列为长度为Integer.MAX_VALUE,容易OOM， Executors.newFixedThreadPool(``int``); ``//创建固定容量大小的缓冲池请，求队列为长度为Integer.MAX_VALUE,容易OOM 123456789101112131415161718192021222324252627282930public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; /*corePoolSize 核心线程池大小maximumPoolSize 线程池最大容量大小keepAliveTime 线程池空闲时，线程存活的时间TimeUnit 时间单位ThreadFactory 线程工厂BlockingQueue任务队列RejectedExecutionHandler 线程拒绝策略*/ 123456789101112131415161718192021222324252627282930313233public class Test &#123; public static void main(String[] args) &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); for(int i=0;i&lt;15;i++)&#123; MyTask myTask = new MyTask(i); executor.execute(myTask); System.out.println("线程池中线程数目："+executor.getPoolSize()+"，队列中等待执行的任务数目："+ executor.getQueue().size()+"，已执行玩别的任务数目："+executor.getCompletedTaskCount()); &#125; executor.shutdown(); &#125;&#125; class MyTask implements Runnable &#123; private int taskNum; public MyTask(int num) &#123; this.taskNum = num; &#125; @Override public void run() &#123; System.out.println("正在执行task "+taskNum); try &#123; Thread.currentThread().sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("task "+taskNum+"执行完毕"); &#125;&#125; 参考文章：[深入理解Java之线程池](http://www.importnew.com/19011.html) SimpleDateFormat类线程不安全 线程不安全原因： SimpleDateFormat(下面简称sdf)类内部有一个Calendar对象引用,它用来储存和这个sdf相关的日期信息,例如sdf.parse(dateStr), sdf.format(date) 。 calendar这个共享变量的访问没有做到线程安全 解决方法： 将SimpleDateFormat定义成局部变量： 加一把线程同步锁：synchronized(lock)； 【推荐】使用ThreadLocal: 每个线程都将拥有自己的SimpleDateFormat对象副本。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import java.util.HashMap;import java.util.Map;public class DateUtil &#123; /** 锁对象 */ private static final Object lockObj = new Object(); /** 存放不同的日期模板格式的sdf的Map */ private static Map&lt;String, ThreadLocal&lt;SimpleDateFormat&gt;&gt; sdfMap = new HashMap&lt;String, ThreadLocal&lt;SimpleDateFormat&gt;&gt;(); /** * 返回一个ThreadLocal的sdf,每个线程只会new一次sdf * * @param pattern * @return */ private static SimpleDateFormat getSdf(final String pattern) &#123; ThreadLocal&lt;SimpleDateFormat&gt; tl = sdfMap.get(pattern); // 此处的双重判断和同步是为了防止sdfMap这个单例被多次put重复的sdf if (tl == null) &#123; synchronized (lockObj) &#123; tl = sdfMap.get(pattern); if (tl == null) &#123; // 只有Map中还没有这个pattern的sdf才会生成新的sdf并放入map System.out.println("put new sdf of pattern " + pattern + " to map"); // 这里是关键,使用ThreadLocal&lt;SimpleDateFormat&gt;替代原来直接new SimpleDateFormat tl = new ThreadLocal&lt;SimpleDateFormat&gt;() &#123; @Override protected SimpleDateFormat initialValue() &#123; System.out.println("thread: " + Thread.currentThread() + " init pattern: " + pattern); return new SimpleDateFormat(pattern); &#125; &#125;; sdfMap.put(pattern, tl); &#125; &#125; &#125; return tl.get(); &#125; /** * 是用ThreadLocal&lt;SimpleDateFormat&gt;来获取SimpleDateFormat,这样每个线程只会有一个SimpleDateFormat * * @param date * @param pattern * @return */ public static String format(Date date, String pattern) &#123; return getSdf(pattern).format(date); &#125; public static Date parse(String dateStr, String pattern) throws ParseException &#123; return getSdf(pattern).parse(dateStr); &#125;&#125; 高并发的同步调用考虑锁的性能消耗，锁的粒度尽可能小。 在获得锁之前做完所有需要做的事，只把锁用在需要同步的资源上，用完之后立即释放它。减少锁持有时间。 减小锁粒度：ConcurrentHashMap。 锁粗化：如果对同一个锁不停的进行请求、同步和释放，其本身也会消耗系统宝贵的资源，反而不利于性能的优化 。 锁消除：即时编译器时，如果发现不可能被共享的对象，则可以消除这些对象的锁操作。 分别用不同的锁来保护同一个类中多个独立的状态变量，而不是对整个类域只使用一个锁。锁分离。ReadWriteLock。 参考文章：Java 高并发九：锁的优化和注意事项详解 对多个资源、库表、对象加锁，需要保持一致的加锁顺序。 当多个线程需要相同的一些锁，但是按照不同的顺序加锁，死锁就很容易发生。如果能确保所有的线程都是按照相同的顺序获得锁，那么死锁就不会发生。 此外还有加锁时限、死锁检测等方法预防死锁。 并发修改同一记录必须加锁。 乐观锁，大多是基于数据版本(Version)记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。 读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。 提交版本必须大于记录当前版本才能执行更新。 悲观锁（Pessimistic Lock），正如其名，具有强烈的独占和排他特性。它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 多线程并行之Timer 定时任务用Timer实现有可能出现异常，因为它是基于绝对时间而不是相对时间进行调度的。当环境的系统时间被修改后，原来的定时任务可能就不跑了。另外需要注意一点，捕获并处理定时任务的异常。如果在TimerTask里抛出了异常，那么Timer认为定时任务被取消并终止执行线程。 异步转同步操作CountDownLatch CountDownLatch这个类能够使一个线程等待其他线程完成各自的工作后再执行。例如，应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行。 CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。 构造器中的**计数值（count）实际上就是闭锁需要等待的线程数量**。这个值只能被设置一次，而且CountDownLatch**没有提供任何机制去重新设置这个计数值**。这种通知机制是通过 **CountDownLatch.countDown()**方法来完成的；每调用一次这个方法，在构造函数中初始化的count值就减1。所以当N个线程都调 用了这个方法，count的值等于0，然后主线程就能通过await()方法，恢复执行自己的任务。 参考文章：[什么时候使用CountDownLatch](http://www.importnew.com/15731.html) 多线程之Random 任何情况下都不要在多个线程间共享一个java.util.Random实例，而该把它放入ThreadLocal之中。 Java7在所有情形下都更推荐使用java.util.concurrent.ThreadLocalRandom——它向下兼容已有的代码且运营成本更低。 1234567891011121314151617181920private static void testTL_Random( final int threads, final long cnt )&#123; final CountDownLatch latch = new CountDownLatch( threads ); final ThreadLocal&lt;Random&gt; rnd = new ThreadLocal&lt;Random&gt;() &#123; @Override protected Random initialValue() &#123; return new Random( 100 ); &#125; &#125;; for ( int i = 0; i &lt; threads; ++i ) &#123; final Thread thread = new Thread( new RandomTask( null, i, cnt, latch ) &#123; @Override protected Random getRandom() &#123; return rnd.get(); &#125; &#125; ); thread.start(); &#125;&#125; 参考文章：多线程环境下生成随机数 volatile 可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 volatile不能确保原子性 可以通过synchronized或lock，进行加锁，来保证操作的原子性。也可以通过AtomicInteger。 java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。atomic是利用CAS来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。 好的一面是它通过一个直接机器码指令设置值时，能够最小程度地影响其他线程的执行。坏的一面是如果它在与其他线程竞争设置值时失败了，它不得不再次尝试。在高竞争下，这将转化为一个自旋锁，线程不得不持续尝试设置值，无限循环直到成功。 JDK8中LongAdder实例，并使用intValue()和add()来获取和设置值。神奇的地方发生在幕后。这个类所做的事情是当一个直接CAS由于竞争失败时，它将delta保存在为该线程分配的一个内部单元对象中，然后当intValue()被调用时，它会将这些临时单元的值再相加到结果和中。这就减少了返回重新CAS或者阻塞其他线程的必要。 参考文章： 你真的了解volatile关键字吗？ Java 8 LongAdders：管理并发计数器的正确方式 HashMap在resize可能发生死链，加锁解决。 当多个线程同时检测到总数量超过门限值的时候就会同时调用resize操作，各自生成新的数组并rehash后赋给该map底层的数组table，结果最终只有最后一个线程生成的新数组被赋给table变量，其他线程的均会丢失。而且当某些线程已经完成赋值而其他线程刚开始的时候，就会用已经被赋值的table作为原始数组，这样也会有问题。 首先如果多个线程同时使用put方法添加元素，而且假设正好存在两个put的key发生了碰撞(hash值一样)，那么根据HashMap的实现，这两个key会添加到数组的同一个位置，这样最终就会发生其中一个线程的put的数据被覆盖。 线程安全： Hashtable ConcurrentHashMap(性能优势) Synchronized Map 参考文章：如何线程安全的使用HashMap ThreadLocal 参考文章：彻底理解ThreadLocal 参考文章： 【Java编码规范】《阿里巴巴Java开发手册（正式版）》更新（v1.2.0版）——迄今最完善版本 白话阿里巴巴Java开发手册(编程规约) 白话阿里巴巴Java开发手册（异常日志） 白话阿里巴巴Java开发手册（安全规约）]]></content>
      <categories>
        <category>学习笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>alibaba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSM框架整合理解]]></title>
    <url>%2Fframe_ssm.html</url>
    <content type="text"><![CDATA[SSM框架整合理解 把IntelliJ IDEA+Maven+Spring + SpringMVC + MyBatis项目部署，框架流程梳理调试了一遍，加深自己的理解。 回顾SSM框架SpringSpring就像是整个项目中装配bean的大工厂，在配置文件中可以指定使用特定的参数去调用实体类的构造方法来实例化对象。Spring的核心思想是IoC（控制反转），即不再需要程序员去显式地new一个对象，而是让Spring框架帮你来完成这一切。 SpringMVCSpringMVC在项目中拦截用户请求，它的核心Servlet即DispatcherServlet承担中介或是前台这样的职责，将用户请求通过HandlerMapping去匹配Controller，Controller就是具体对应请求所执行的操作。SpringMVC相当于SSH框架中struts。 mybatismybatis是对jdbc的封装，它让数据库底层操作变的透明。mybatis的操作都是围绕一个sqlSessionFactory实例展开的。mybatis通过配置文件关联到各实体类的Mapper文件，Mapper文件中配置了每个类对数据库所需进行的sql语句映射。在每次与数据库交互时，通过sqlSessionFactory拿到一个sqlSession，再执行sql命令。 SSM框架流程 SSM框架搭建创建Maven的Web项目 通过IntelliJ IDEA创建maven项目： 选中Createfrom archetype，选择maven-archetype-webapp 在Properties中添加一个参数 archetypeCatalog=internal，提高maven项目构建速度 SSH框架Web项目框架 main： 创建java文件夹：项目代码 resources文件夹： mapping文件夹：数据库表xml xml配置文件 webapp： WEB-INF： 创建jsp文件夹：不同显示页面 web.xml:配置文件 Tomcat启动项目 为项目配置Tomcat 配置各种XML pom.xml——引入项目所需要的jar包 spring核心依赖 mybatis依赖 mybatis-spring整合包依赖 mysql驱动依赖 其他依赖： 日志相关：log4j、slf4j 连接池相关：commons-dbcp、c3p0、Druid Json相关：fastjson 其他：jstl PS：此外还有SpringBoot可以简化xml中的配置项数量。SpringBoot完全抛弃了繁琐的XML文件配置方式，而是替代性地用注解方式来实现。 参考文章：IDEA下从零开始搭建SpringBoot工程 调试过程中的错误有很大一部分是所引的jar没有在pom.xml配置，这部分需要仔细细致。 关于jar包的版本号的修改，可以在标签中用变量保存版本号，中具体的jar包的版本用变量代替，方便后续修改。 web.xml 这是整个web项目的配置文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd" version="3.0"&gt; &lt;display-name&gt;cloudmusic_ssm_demo&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mybatis.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;context-param&gt; &lt;param-name&gt;log4jConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:log4j.properties&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 编码过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!-- spring监听器 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- 防止spring内存溢出监听器，比如quartz --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.util.IntrospectorCleanupListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- spring mvc servlet--&gt; &lt;servlet&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;SpringMVC&lt;/servlet-name&gt; &lt;!-- 此处也可以配置成 *.do 形式 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;/index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;!-- session配置 --&gt; &lt;session-config&gt; &lt;session-timeout&gt;15&lt;/session-timeout&gt; &lt;/session-config&gt;&lt;/web-app&gt; 中的配置，加载SpringMVC的配置文件。 SpringMVC具有统一的入口DispatcherServlet，所有的请求都通过DispatcherServlet。DispatcherServlet是前置控制器，配置在web.xml文件中的。拦截匹配的请求，Servlet拦截匹配规则要自已定义，把拦截下来的请求，依据某某规则分发到目标Controller来处理。 拦截所有的请求，并加载所有的ssm配置文件（路径为classpath:spring-mvc.xml） 在web.xml中使用contextConfigLocation参数定义要装入的Spring配置文件。 加载路径为classpath:spring-mybatis.xml文件 参考文章： SSM:spring+springmvc+mybatis框架中的XML配置文件功能详细解释 spring-mvc.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd"&gt; &lt;!-- 自动扫描 @Controller--&gt; &lt;context:component-scan base-package="com.ssm.demo.controller"/&gt; &lt;!--避免IE执行AJAX时，返回JSON出现下载文件 --&gt; &lt;bean id="mappingJacksonHttpMessageConverter" class="org.springframework.http.converter.json.MappingJackson2HttpMessageConverter"&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 启动SpringMVC的注解功能，完成请求和注解POJO的映射 --&gt; &lt;bean class="org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter"&gt; &lt;property name="messageConverters"&gt; &lt;list&gt; &lt;ref bean="mappingJacksonHttpMessageConverter"/&gt; &lt;!-- JSON转换器 --&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 定义跳转的文件的前后缀 ，视图模式配置 --&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;property name="suffix" value=".jsp"/&gt; &lt;/bean&gt; &lt;!-- 文件上传配置 --&gt; &lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;!-- 默认编码 --&gt; &lt;property name="defaultEncoding" value="UTF-8"/&gt; &lt;!-- 上传文件大小限制为31M，31*1024*1024 --&gt; &lt;property name="maxUploadSize" value="32505856"/&gt; &lt;!-- 内存中的最大值 --&gt; &lt;property name="maxInMemorySize" value="4096"/&gt; &lt;/bean&gt;&lt;/beans&gt; controller注入：使用组件扫描方式，扫描包下面所有的Controller，可以使用注解来指定访问路径。 Spring 所有功能都在 Bean 的基础上演化而来，所以必须事先将 Controller 变成 Bean。配置了一个 AnnotationMethodHandlerAdapter，它负责根据 Bean 中的 Spring MVC 注解对 Bean 进行加工处理，使这些 Bean 变成控制器并映射特定的 URL 请求。 视图解析：在Controller中设置视图名的时候会自动加上前缀和后缀。 spring-mybatis.xml：Spring与MyBatis的整合配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;!-- 自动扫描 --&gt; &lt;context:component-scan base-package="com.ssm.demo"/&gt; &lt;!-- 第一种方式：加载一个properties文件 --&gt; &lt;bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="location" value="classpath:jdbc.properties"/&gt; &lt;/bean&gt; &lt;!-- 第二种方式：加载多个properties文件 &lt;bean id="configProperties" class="org.springframework.beans.factory.config.PropertiesFactoryBean"&gt; &lt;property name="locations"&gt; &lt;list&gt; &lt;value&gt;classpath:jdbc.properties&lt;/value&gt; &lt;value&gt;classpath:common.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="fileEncoding" value="UTF-8"/&gt; &lt;/bean&gt; &lt;bean id="propertyConfigurer" class="org.springframework.beans.factory.config.PreferencesPlaceholderConfigurer"&gt; &lt;property name="properties" ref="configProperties"/&gt; &lt;/bean&gt; --&gt; &lt;!-- 配置数据源 --&gt; &lt;bean id="dataSource" class="org.apache.commons.dbcp.BasicDataSource" destroy-method="close"&gt; &lt;property name="driverClassName" value="$&#123;driverClass&#125;"/&gt; &lt;property name="url" value="$&#123;jdbcUrl&#125;"/&gt; &lt;property name="username" value="$&#123;username&#125;"/&gt; &lt;property name="password" value="$&#123;password&#125;"/&gt; &lt;!-- 初始化连接大小 --&gt; &lt;property name="initialSize" value="$&#123;initialSize&#125;"&gt;&lt;/property&gt; &lt;!-- 连接池最大数量 --&gt; &lt;property name="maxActive" value="$&#123;maxActive&#125;"&gt;&lt;/property&gt; &lt;!-- 连接池最大空闲 --&gt; &lt;property name="maxIdle" value="$&#123;maxIdle&#125;"&gt;&lt;/property&gt; &lt;!-- 连接池最小空闲 --&gt; &lt;property name="minIdle" value="$&#123;minIdle&#125;"&gt;&lt;/property&gt; &lt;!-- 获取连接最大等待时间 --&gt; &lt;property name="maxWait" value="$&#123;maxWait&#125;"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- mybatis和spring完美整合，不需要mybatis的配置映射文件 --&gt; &lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;!-- 自动扫描mapping.xml文件 --&gt; &lt;property name="mapperLocations" value="classpath:mapping/*.xml"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- DAO接口所在包名，Spring会自动查找其下的类 --&gt; &lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;property name="basePackage" value="com.ssm.demo.dao"/&gt; &lt;property name="sqlSessionFactoryBeanName" value="sqlSessionFactory"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;!-- (事务管理)transaction manager, use JtaTransactionManager for global tx --&gt; &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;/bean&gt; &lt;!-- (事务管理)transaction manager, use JtaTransactionManager for global tx --&gt; &lt;tx:annotation-driven transaction-manager="transactionManager"/&gt;&lt;/beans&gt; 自动扫描，自动注入，配置数据库 自动扫描,将标注Spring注解的类自动转化Bean，同时完成Bean的注入 加载数据资源属性文件 配置数据源（三种方式，采用DBCP） 配置sessionfactory 装配Dao接口 声明式事务管理 注解事务切面 Mapper.xml映射文件中定义了操作数据库的sql，每一个sql是一个statement，映射文件是myBatis的核心。 jdbc.properties：JDBC属性文件 123456789101112131415driverClass=com.mysql.jdbc.DriverjdbcUrl=jdbc:mysql://localhost:3306/db_ssm?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNullusername=rootpassword=147789#定义初始连接数initialSize=0#定义最大连接数maxActive=20#定义最大空闲maxIdle=20#定义最小空闲minIdle=1#定义最长等待时间maxWait=60000 创建业务流程 以数据库查询表内容为例 持久层：DAO层（mapper）做数据持久层的工作，负责与数据库进行联络的一些任务都封装在此， DAO层的设计首先是设计DAO的接口， 然后在Spring的配置文件中定义此接口的实现类， 然后就可在模块中调用此接口来进行数据业务的处理，而不用关心此接口的具体实现类是哪个类，显得结构非常清晰， DAO层的数据源配置，以及有关数据库连接的参数都在Spring的配置文件中进行配置。 业务层：Service层 主要负责业务模块的逻辑应用设计。 首先设计接口，再设计其实现的类 接着再在Spring的配置文件中配置其实现的关联。这样我们就可以在应用中调用Service接口来进行业务处理。 Service层的业务实现，具体要调用到已定义的DAO层的接口， 封装Service层的业务逻辑有利于通用的业务逻辑的独立性和重复利用性，程序显得非常简洁。 表现层：Controller层（Handler层）负责具体的业务模块流程的控制 在此层里面要调用Service层的接口来控制业务流程， 控制的配置也同样是在Spring的配置文件里面进行，针对具体的业务流程，会有不同的控制器，我们具体的设计过程中可以将流程进行抽象归纳，设计出可以重复利用的子单元流程模块，这样不仅使程序结构变得清晰，也大大减少了代码量。 模型层：Model层 主要存放实体类 项目代码结构： controller： “@RequestMapping”请求路径映射，如果标注在某个controller的类级别上，则表明访问此类路径下的方法都要加上其配置的路径；最常用是标注在方法上，表明哪个具体的方法来接受处理某次请求。 调用service层方法 spring mvc 支持如下的返回方式：ModelAndView, Model, ModelMap, Map,View, String, void。本文返回的是String，通过model进行使用。 参考文章：SpringMVC返回（return）方式详解 service：建立service接口和实现类 impl:接口对应实现类： 调用Dao层的数据库操作以及model层的实体类 dao 定义接口中的方法 一个Dao对应一个对应的mapper文件，实现Dao对应的定义的接口方法 mapping： mapper.xml：实现dao中接口定义的方法 1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt;mapper namespace="com.ssm.demo.dao.UserDao"&gt; &lt;resultMap id="UserBaseMap" type="com.ssm.demo.model.User"&gt; &lt;id column="id" property="id" jdbcType="BIGINT"/&gt; &lt;result column="user_name" property="userName" jdbcType="VARCHAR"/&gt; &lt;result column="user_phone" property="userPhone" jdbcType="VARCHAR"/&gt; &lt;result column="user_email" property="userEmail" jdbcType="VARCHAR"/&gt; &lt;result column="user_pwd" property="userPwd" jdbcType="VARCHAR"/&gt; &lt;result column="pwd_salt" property="pwdSalt" jdbcType="VARCHAR"/&gt; &lt;result column="create_time" property="createTime" jdbcType="DATE"/&gt; &lt;result column="modify_time" property="modifyTime" jdbcType="DATE"/&gt; &lt;result column="is_delete" property="isDelete" jdbcType="SMALLINT"&gt;&lt;/result&gt; &lt;/resultMap&gt; &lt;select id="selectUserById" parameterType="java.lang.Long" resultMap="UserBaseMap"&gt; SELECT * FROM t_user WHERE id = #&#123;userId&#125; &lt;/select&gt; &lt;select id="selectUserByPhoneOrEmail" resultMap="UserBaseMap"&gt; SELECT * FROM t_user WHERE user_email = #&#123;emailOrPhone&#125; OR user_phone = #&#123;emailOrPhone&#125; AND user_state = #&#123;state&#125; &lt;/select&gt; &lt;select id="selectAllUser" resultMap="UserBaseMap"&gt; SELECT * FROM t_user &lt;/select&gt;&lt;/mapper&gt; namespace:当前库表映射文件的命名空间，唯一的不能重复 映射实体类的数据类型 id：resultMap的唯一标识 column:库表的字段名 property:实体类里的属性名 id：当前sql的唯一标识 parameterType：输入参数的数据类型 返回值的数据类型：resultMap适合使用返回值是自定义实体类的情况 ； resultType适合使用返回值的数据类型是非自定义的，即jdk的提供的类型。 {}:用来接受参数的，如果是传递一个参数#{id}内容任意，如果是多个参数就有一定的规则,采用的是预编译的形式select model 实体属性——对应表中的元组的属性 getter和setter方法 DataBase ===&gt; Entity ===&gt; Mapper.xml ===&gt; Mapper.Java ===&gt; Service.java ===&gt; Controller.java ===&gt; Jsp. 参考文章SSM框架整合（IntelliJ IDEA + maven + Spring + SpringMVC + MyBatis） SSM框架感受 本质上的MVC，xml配置、注解，以及mapper的映射，让开发更加简洁和思路清晰 ##]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>SSM</tag>
        <tag>SpringMVC</tag>
        <tag>Spring</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【面试持续更新】2018校招记录]]></title>
    <url>%2Finterview_record.html</url>
    <content type="text"><![CDATA[【面试持续更新】2018校招记录工商银行一句话，谈人生谈理想，不谈技术 中兴提前批一面 简单自我介绍 实习经历详细介绍 前端请求与后端处理的异常情况如何处理 SQL模糊查询语句，如何提高查询效率 实习所用PHP框架是如何与数据库相连接的 常用Shell命令 Git开发流程 二面 再次自我介绍 工作地点意向询问 网络请求如何加速（不理解是询问那一方面的加速，自己回答分机房说不对） 实习相关问题 然后就陷入了尴尬的安静 海康威视提前批一面 简单自我介绍 Spark特性有哪些？ 除了Hadoop之外有哪些其他的大数据处理框架？ MapReduce处理过程 Hadoop部署节点？如何部署？ Java与python的交互了解多少？如何进行交互？ Java内存模型介绍？ Java多线程介绍？ JVM调优经验 使用过Redis等NoSQL数据库吗，Redis如何部署？Redis中的数据最终存储在哪里？ 自己询问的问题： 部门业务应用场景：大数据构建多特征模型进行行为安全预测判断 技术能力要求：部门有不同方向，不必太在意招聘中的所有要求都需要符合 什么时候有下一步的结果：他也不知道。。。。]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己用过的框架]]></title>
    <url>%2Fused_frames.html</url>
    <content type="text"><![CDATA[自己用过的框架Java之SSH SSH不是一个框架，而是多个框架的集成，是目前较流行的一种Web应用程序开源集成框架，用于构建灵活、易于扩展的多层Web应用程序。 系统从职责上分为四层：表示层、业务逻辑层、数据持久层和域模块层。 Spring+Struts+Hibernate（SSH） 其中使用Struts作为系统的整体基础架构，负责MVC的分离，在Struts框架的模型部分，利用hibernate框架对持久层提供支持，业务层用spring支持。 系统的基本业务流程是： 在表示层中，首先通过JSP页面实现交互界面，负责传送请求(Request)和接收响应(Response)，然后Struts根据配置文件(struts-config.xml)将ActionServlet接收到的Request委派给相应的Action处理。 在业务层中，管理服务组件的 Spring IoC容器负责向Action提供业务模型(Model)组件和该组件的协作对象数据处理(DAO)组件完成业务逻辑，并提供事务处理、缓冲池等容器组件以提升系统性能和保证数据的完整性。 在持久层中，则依赖于Hibernate的对象化映射和数据库交互，处理DAO组件请求的数据，并返回处理结果。 SSH配置流程： 创建web项目 配置struts： 添加Struts2所需要的基本jar包到 lib目录 在web.xml 文件里添加struts的过滤器配置 在src目录下创建struts配置文件struts.xml 配置spring： 在lib目录下导入spring相关的jar包（2个spring跟struts结合的jar包） 在web.xml文件下配置监听器 配置hibernate: 在lib目录里导入hibernate相关的jar包 创建实体类 创建实体类对应的xxx..hbm.xml映射文件 应用IOC实现DAO接口 编写Action类 编写Service(接口类)和ServiceImpl(实现类) Spring是一个轻量级的控制反转（IoC）和面向切面（AOP）的容器框架。 Struts它通过采用 Java Servlet/JSP 技术，实现了基于JavaEE Web应用的MVC设计模式的应用框架，是MVC经典设计模式中的一个经典产品。 Struts 2以WebWork为核心，采用拦截器的机制来处理用户的请求，这样的设计也使得业务逻辑控制器能够与ServletAPI完全脱离开，所以Struts 2可以理解为WebWork的更新产品。 Hibernate是一个开放源代码的对象关系映射框架，它对JDBC进行了非常轻量级的对象封装，使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库。 Spring+SpringMVC+Mybatis（SSM） SpringMVC 做控制器(controller)，Spring 管理各层的组件，MyBatis 负责持久化层。 Struts2与SpringMVC MyBatis与Hibernate MyBatis可以进行更为细致的SQL优化，可以减少查询字段。 MyBatis容易掌握，而Hibernate门槛较高。 Hibernate的DAO层开发比MyBatis简单，Mybatis需要维护SQL和结果映射。 Hibernate对对象的维护和缓存要比MyBatis好，对增删改查的对象的维护要方便。 Hibernate数据库移植性很好，MyBatis的数据库移植性不好，不同的数据库需要写不同SQL。 Hibernate有更好的二级缓存机制，可以使用第三方缓存。MyBatis本身提供的缓存机制不佳，更新操作不能指定刷新指定记录，会清空整个表，但是也可以使用第三方缓存。 Hibernate 封装性好，屏蔽了数据库差异，自动生成SQL语句，应对数据库变化能力较弱，SQL语句优化困难。 MyBatis仅实现了SQL语句和对象的映射，需要针对具体的数据库写SQL语句，应对数据库变化能力较强，SQL语句优化较为方便。 SSM配置流程： 创建web项目 在WEB-INF/lib导入jar包（亦可以根目录下用maven配置文件poom.xml进行配置管理jar包） 配置MyBatis:dao层编写dao类以及对应的mapper和xml（为dao接口方法提供sql语句配置） 配置spring：在applicationContext.xml.xml文件下配置 配置springmvc：配置springMVC.xml 编写Service以及ServiceImpl 编写Controller（相当于struts中的action） SSM和SSH不同主要在MVC实现方式，以及ORM持久化方面不同（Hiibernate与Mybatis）。SSM越来越轻量级配置，将注解开发发挥到极致，且ORM实现更加灵活，SQL优化更简便；而SSH较注重配置开发，其中的Hiibernate对JDBC的完整封装更面向对象，对增删改查的数据维护更自动化，但SQL优化方面较弱，且入门门槛稍高。 参考文章 SSH框架的底层机制及原理 SSH的框架整合 SSH框架总结（框架分析+环境搭建+实例源码下载） SSH和SSM对比总结 手把手教你整合最优雅SSM框架：SpringMVC + Spring + MyBatis SSM SPRING+SPING MVC + MYBATIS 三大框架整合详细步骤 PHP之ODP ODP是公司发布的在线业务开发平台，面向全百度的在线业务支撑平台，专注于总结大社区类业务模式，其提供了标准的webserver环境、标准php环境、AP框架、SAF社区业务框架、基础库、RAL资源访问层、KSARCH通用服务等组件，统一业务的逻辑和部署结构，为测试、运维等提供一致的视图。 这也是自己实习时候一直在用的框架。 Online Develop Platform = Linux+Lightted/nginx+mysql+PHP ODP核心包含了ODP的核心功能组件，包括运行环境、核心基础库、数据交互层、框架等。 横向看，ODP核心通过库、框架、工具等集成支持了各类规范和模式，也为全流程支持提供接口。 向上看，ODP核心直接为产品线业务提供运行环境和研发支持。 向下看，ODP核心通过数据交互层将底层的通用服务提供给业务。 AP框架目录： 1234567891011模板层： odp/template/appname/Actiono: odp/app/appname/actions/PageService: odp/app/appname/models/service/page/DataService: odp/app/appname/models/service/data/Dao: odp/app/appname/models/dao/Controller: odp/app/appname/controller/ ODP应用程序结构： 123456789101112131415161718192021222324252627282930313233343536newapp // 应用名称+--action // 动作类目录| +--api // 子系统交互api目录| | --Sample.php // 示例api服务端action| --Sample.php // 示例普通action+--api // saf api接口和服务类| --Interface.php // 接口类| --Service.php // 实现类+--conf // 配置目录| +--newapp // 配置目录，配置文件可以拆分| --global.conf // app全局配置文件| --log.conf // log示例配置文件+--controllers // 控制类目录| --Main.php // 主控制类| --Api.php // api控制类+--doc // 文档目录+--library // 本地类根目录| +--newapp // app本地类目录| --Util.php // 示例本地类+--models // 数据目录| +--dao // 数据获取目录| | --Sample.php // 示例| +--service // 页面数据服务目录| +--data // 主题数据服务目录| | --Sample.php // 示例| +--page // 页面数据服务目录| --Sample.php // 示例| --SampleApi.php // api示例+--script // 脚本目录| --sampleScript.php // 示例脚本+--test // 测试目录+--Bootstrap.php // ap框架的引导文件+--build.sh // 打包脚本+--index.php // 入口文件+--Makefile // 自动部署脚本--readme.txt // readme文件，告诉你如何部署和运行 DB，通过对应参数的不同，实现自动拼接不同的SQL语句。 1234567891011121314$tables 数据表列表，可以是数组或者字符串$fields 字段列表，可以是数组或者字符串$conds 条件列表，可以是数组或者字符串$options 选项列表，可以是数组或者字符串$appends 结尾操作列表，可以是数组或者字符串例如：$table = ‘student’;$fields = array(‘number’, ‘class’);$conds = array(‘grade =‘ =&gt;‘three’, ‘school =‘ =&gt; ‘希望小学’);调用select($table, $fields, $conds); 就会生成 select number, class from student where grade = ‘three’ and school = ‘希望小学’ 的sql语句并执行之。 逻辑分层之间的调用关系，只能向后依赖，不能向前依赖或者跨层之间依赖。此次逻辑分层从前到后，依次为：Action、PageService、DataService、Dao。具体来说便是指，Dao不能依赖于DataService,PageService,Action；DataService不能依赖于PageService和Action；PageService不能依赖于Action。Action不能直接调用DataService，也不能直接调用Dao；PageService不能直接调用Dao。 SAF是ODP环境提供的业务层框架，SAF框架建立的目的是为了把业务逻辑开发过程中一些共性的问题抽取出来，并提供统一的解决方案。SAF框架包含控制器组件，通用业务组件（参数处理，session处理，日志打印等）以及通用配置组件。可以将SAF框架理解为一个工具库，SAF为我们提供了很多的通用功能例如验证用户的登录信息，接收用户提交的数据，更改用户的信息，记录用户的操作行为等。 SAF框架提供了一个很重要的功能就是钩子（Hook）机制，通过在钩子函数中覆写对应的钩子函数,可以实现对cgi(GET POST等)数据的特殊处理，对登陆信息的校验/修改以及对输出到log日志文件的内容的修改等功能。 RAL是一个支持多种交互协议和打包格式的php扩展。RAL规定了一套高度抽象的交互过程规范，将整个后端交互过程分成了交互协议和数据打包/解包两大块，可以支持一些常用的后端交互协议，标准化协议扩充的开发过程，促进代码复用。RAL集成了负载均衡、健康检查等功能，让上游端不需要再关注这些繁琐的通用逻辑，同时实现版本可以在性能方面有更优的表现。 参考文章： ODP教程 | 百度外卖手册 Python之Tornado 自己做Sug平台展示时候选择的Python Web框架 Tornado 和现在的主流 Web 服务器框架（包括大多数 Python 的框架）有着明显的区别：它是非阻塞式服务器，而且速度相当快。得利于其 非阻塞的方式和对 epoll 的运用，Tornado 每秒可以处理数以千计的连接，这意味着对于实时 Web 服务来说，Tornado 是一个理想的 Web 框架。我们开发这个 Web 服务器的主要目的就是为了处理 FriendFeed 的实时功能 ——在 FriendFeed 的应用里每一个活动用户都会保持着一个服务器连接。 参考文章： Tornado Nginx 无论做什么框架，很多时候都离不开nginx NGINX 有一个主进程（它执行特权操作，如读取配置和绑定端口）和一些工作进程与辅助进程。 反向代理反向代理应该是Nginx做的最多的一件事了。反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 12345678910server &#123; listen 80; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://localhost:8080; proxy_set_header Host $host:$server_port; &#125; &#125; 负载均衡负载均衡也是Nginx常用的一个功能，负载均衡其意思就是分摊到多个操作单元上进行执行从而共同完成工作任务。简单而言就是当有2台或以上服务器时，根据规则随机的将请求分发到指定的服务器上处理，负载均衡配置一般都需要同时配置反向代理，通过反向代理跳转到负载均衡。而Nginx目前支持自带3种负载均衡策略，还有2种常用的第三方策略。 RR每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 1234567891011121314upstream test &#123; server localhost:8080; server localhost:8081;&#125;server &#123; listen 81; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; 权重指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 1234upstream test &#123; server localhost:8080 weight=9; server localhost:8081 weight=1;&#125; ip_hashiphash的每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 12345upstream test &#123; ip_hash; server localhost:8080; server localhost:8081;&#125; fair按后端服务器的响应时间来分配请求，响应时间短的优先分配。 12345upstream backend &#123; fair; server localhost:8080; server localhost:8081;&#125; url_hash按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 123456upstream backend &#123; hash $request_uri; hash_method crc32; server localhost:8080; server localhost:8081;&#125; 动静分离1234567891011121314151617181920212223242526272829upstream test&#123; server localhost:8080; server localhost:8081; &#125; server &#123; listen 80; server_name localhost; location / &#123; root e:wwwroot; index index.html; &#125; # 所有静态请求都由nginx处理，存放目录为html location ~ .(gif|jpg|jpeg|png|bmp|swf|css|js)$ &#123; root e:wwwroot; &#125; # 所有动态请求都转发给tomcat处理 location ~ .(jsp|do)$ &#123; proxy_pass http://test; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root e:wwwroot; &#125; &#125; 参考文章： 全面了解 Nginx 主要应用场景]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>frame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[尘封已久的新浪微博实习面试问题]]></title>
    <url>%2Fmerge_file_by_multi_threads.html</url>
    <content type="text"><![CDATA[尘封已久的新浪微博实习面试问题 假设本地磁盘目录中有若干文本文件（每行存储一个字符串），要求实现一个多线程的应用程序，将这些文本文件合并为一个文件文件。 备注：编程语言为必须Java，考虑代码注释、日志打印及异常处理，可以忽略内存限制。 请将代码以PDF附件的形式回复，谢谢。 代码思路 最简单的想法，一个线程读一个线程写 其次的想法，多个线程读同一个文件，一个线程写（单线程消费, 这样就严格保证了顺序） 多线程分组读取，是否要考虑文件的顺序？如果考虑顺序该如何？ 线程池、生产者消费者模型 NIO 预先对目录中的文件进行分类；线程间通信，全局变量判断文件读取状态，避免重复读取 同步锁、队列 注释、日志、异常处理 思路参考 java多线程读取多个文件 导入数据库 对于处理大数据量的记录，并将处理结果写入文件中的处理方案：方案一(适合于处理和输出的数据量都很大的情况)：生产者：多个线程 读取一定量的数据并处理，然后将处理结果封装成一个队列元素，装进阻塞队列中消费者: 一个线程 取元素 追加写文件(csv) (多个线程写文件是不安全的) 方案二(目前在使用的，适用于需要处理的数据量大，但输出的数据量不大的情况)：生产者：一个线程，分页查询部分数据，将其封装成队列元素装进队列中消费者：多个线程 ，从队列中取出数据元素并处理，存储处理结果。生产者和消费者执行完毕后，再集中将消费者处理的结果一个个输出到相应文件中 java 使用线程池处理文件夹下面的文件 由于LinkedBlockingQueue实现是线程安全的，实现了先进先出等特性，是作为生产者消费者的首选。 LinkedBlockingQueue 可以指定容量，不指定的话，默认最大是Integer.MAX_VALUE，其中主要用到put和take方法，put方法在队列满的时候会阻塞直到有队列成员被消费，take方法在队列空的时候会阻塞， 直到有队列成员被放进来。 java多线程批量读取文件(一) 新浪面试题-多线程合并文件 java:NIO读写文件的示例 IntelliJ IDEA 之 jdk Language level 代码实现代码感悟 看了那么多资料，最后还是最简单的最好理解，一个线程读取然后再写，感觉向一个伪多线程 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133package com.sinatest;import java.io.*;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;import java.util.ArrayList;import java.util.List;import java.util.concurrent.BlockingQueue;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.LinkedBlockingQueue;class fileWorker implements Runnable &#123; private File inputFile; private String outputFile; //构造函数 public fileWorker(File inputFile, String outputFile) &#123; this.inputFile = inputFile; this.outputFile = outputFile; &#125; @Override public synchronized void run() &#123; File output = new File(outputFile); if (!output.exists()) &#123; try &#123; output.createNewFile(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; FileInputStream fin = null; FileOutputStream fout = null; FileChannel fic = null; FileChannel foc = null; try &#123; fin = new FileInputStream(inputFile); fout = new FileOutputStream(output, true); // 从FileInputStream创建用于输入的FileChannel fic = fin.getChannel(); // 从FileOutputStream 创建用于输出的FileChannel foc = fout.getChannel(); // 16KB缓冲区 ByteBuffer bb = ByteBuffer.allocate(1024 &lt;&lt; 4); // 根据 read返回实际读出的字节数 中止循环 while (fic.read(bb) &gt; 0) &#123; // 缓冲区翻转用于输出到foc bb.flip(); foc.write(bb); // 清空缓冲区用于下次读取 bb.clear(); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; // 安全释放资源 if (null != fic) try &#123; fic.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if (null != foc) try &#123; foc.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if (null != fin) try &#123; fin.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if (null != fout) try &#123; fout.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;public class Main &#123; //线程池线程数量 public static final int THREAD_POOL_SIZE = 5; //遍历文件夹 public static List&lt;File&gt; filePathsList = new ArrayList&lt;File&gt;(); //缓存队列 private static final BlockingQueue BLOCKING_QUEUE = new LinkedBlockingQueue(); //1、遍历文件夹 //2、线程池读取 //3、线程写入 public static void main(String[] args) &#123; //读取文件类型 String fileSuffix = ".txt"; //读取文件目录 String fileFolder = "D://Project//Java//MergeFile//datatest"; //合并文件夹路径 String outputFilePath = "D://Project//Java//MergeFile//output.txt"; //遍历文件夹 getFileList(fileFolder, fileSuffix); //创建线程池 ExecutorService es = Executors.newFixedThreadPool(THREAD_POOL_SIZE); //每一个线程读取一个文件 for (File filePath : filePathsList) &#123; es.execute(new fileWorker(filePath, outputFilePath)); &#125; &#125; public static void getFileList(String fileFolder, String fileSuffix) &#123; File f = new File(fileFolder); File[] filePaths = f.listFiles(); for (File s : filePaths) &#123; if (s.isDirectory()) &#123; getFileList(s.toString(), fileSuffix); &#125; else &#123; if (-1 != s.getName().lastIndexOf(fileSuffix)) &#123; filePathsList.add(s); &#125; &#125; &#125; &#125;&#125; TODO [ ] 解决写入顺序问题 [ ] 采用生产者消费者模式实现，究竟是多线程读取一个文件，还是多线程读取多个文件，还是多线程写一个文件，哪一个在实际生产环境中是最合适的 [ ] 日志记录以及异常处理]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>multi-thread</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python命令行查询成都铁路局12306检票口信息]]></title>
    <url>%2Ffind_train_check_in.html</url>
    <content type="text"><![CDATA[Github地址： [Find-Train-Ticket-Check-in]: https://github.com/lingma1993/Find-Train-Ticket-Check-in 背景近日多地大雨导致铁路多趟列车停运，自己关注成都铁路12306微信公众号，发现一个新的侯乘信息查询功能。 该功能提供了动车和高铁的列车检票口的信息，正好圆了自己的之前挖的坑。 如何获取接口链接地址： chrome F12 点击查询在Network中找到相关接口的信息 http://www.cd-rail.com:9090/CTKF/GeneralProServlet?code=C50101&amp;login=[&quot;10.192.111.79&quot;,&quot;hhs&quot;,&quot;hhs&quot;]&amp;sql=[&quot;20170703&quot;,&quot;ICW&quot;]&amp;where=[]&amp;order=[] 通过Postman模拟数据请求获取数据获取信息请求的格式 接口参数 车站查询 参数名称 参数含义 参数示例 code C50101 login 登陆信息 [“10.192.111.79”,”hhs”,”hhs”] sql 查询语句 [“20170703”,”ICW”] where [] order [] 车次查询 参数名称 参数含义 参数示例 code C5010 login 登陆信息 [“10.192.111.79”,”hhs”,”hhs”] sql 查询语句 [“20170703”,”8503”,”ICW”] where [] order 车站名称查询 参数名称 参数含义 参数示例 code C50102 login 登陆信息 [“10.192.111.79”,”hhs”,”hhs”] sql 查询语句 [] where [] order [] 解析接口返回结果1[&#123;"CHECK_STATUS":"停止检票","CHECK_TICKET":"A10、A11","END_CHECK_TIME":"2017/07/04 09:30:00","END_STN":"重庆北","END_STN_CODE":"CUW","IN_DATE":"2017/07/04 18:59:52","START_CHECK_TIME":"2017/07/04 09:14:00","START_STN":"成都东","START_STN_CODE":"ICW","STATUS_TRAIN":"正点","STN_CODE":"ICW","TD_DATE_ARR":"00:00:00","TD_DATE_DEP11":"09:33","TRAIN_DEP":"G8503","WAIT_ROOM":"2层候车区","WGQBZ":"0"&#125;] Python urlib、urlib2 构建Header，模拟Post请求 prettytable 处理返回数据格式化为表格 re 匹配站点名称以及检索结果 展示]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记如何折腾让博客加HTTPS]]></title>
    <url>%2Fhexo_https.html</url>
    <content type="text"><![CDATA[记如何折腾让博客加HTTPS 折腾了一晚上的博客的HTTPS问题，最后才明白一个道理，世上没有免费的午餐，如果有，那么也是你用最宝贵的时间或是其他换来的。 最终折腾完一圈，回到原点的时候，明白了需求分析而不是说走就走的重要。 背景1 - Google 抓取重定向301在Google搜索site:lincentma.men，查找自己的网站是否被Google收录。 自己的在Google Search Console 中，自己添加了sitemap之后，尝试抓取网站页面，显示： http://lincentma.men/ 请求编入索引 Googlebot类型： 桌面 已重定向 抓取时间：2017年6月26日星期一 GMT-7 下午8:25:37 此网址已重定向至：https://lincentma.men/ 1234567HTTP/1.1 301 Moved PermanentlyContent-Type: application/x-gzipLocation: https://lincentma.men/Server: Coding PagesVary: Accept-EncodingDate: Tue, 27 Jun 2017 03:25:39 GMTContent-Length: 57 导致在Google上没有自己博客更新的文章。 原因：Coding Pages 产生中间页从上面的HTTP响应头部的标志可以看出，Google重定向到了一个Coding Pages的服务商，也就是我双线同步博客的coding pages。 在其官网上我发现： 银牌会员的 Coding Pages 在访问时默认会先加载 Pages 跳转页，您可选择在网站首页任意位置放置「Hosted by Coding Pages」的文字版或图片版，然后勾选下方的「已放置 Hosted by Coding Pages」选项，通过审核后您的 Pages 将不会显示跳转页。请务必将「Hosted by Coding Pages」持续保留在网站首页，撤掉后跳转页会再次出现。 也就是没有银牌会员或者没有在博客上加载广告的时候，就会先跳转到中间页。Google在抓取页面的时候就会发现和目标地址不一致，导致抓取失败。所以没有免费的午餐。 2 - 百度 HTTPS站点认证失败失败详情：您的站点有链接未通过https检验。 知识点 抓取操作不会跟踪重定向。如果您抓取的网页存在重定向，您将需要手动前往重定向到的网址。 301转向(或叫301重定向，301跳转)是当用户或搜索引擎向网站服务器发出浏览请求时，服务器返回的HTTP数据流中头信息(header)中的状态码的一种，表示本网页永久性转移到另一个地址。 302重定向是暂时的重定向，搜索引擎会抓取新的内容而保留旧的网址。因为服务器返回302代码，搜索引擎认为新的网址只是暂时的。301重定向是永久的重定向，搜索引擎在抓取新内容的同时也将旧的网址替换为重定向之后的网址。302 重定向会造成网址URL 劫持现象。 HTTP请求格式： 请求头 说明 Host 接受请求的服务器地址，可以是IP:端口号，也可以是域名 User-Agent 发送请求的应用程序名称 Connection 指定与连接相关的属性，如Connection:Keep-Alive Accept-Charset 通知服务端可以发送的编码格式 Accept-Encoding 通知服务端可以发送的数据压缩格式 Accept-Language 通知服务端可以发送的语言 HTTP响应格式： 响应头 说明 Server 服务器应用程序软件的名称和版本 Content-Type 响应正文的类型（是图片还是二进制字符串） Content-Length 响应正文长度 Content-Charset 响应正文使用的编码 Content-Encoding 响应正文使用的数据压缩格式 Content-Language 响应正文使用的语言 HTTPS：HTTPS 协议就是 HTTP+SSL/TLS，即在 HTTP 基础上加入 SSL /TLS 层，提供了内容加密、身份认证和数据完整性3大功能，目的就是为了加密数据，用于安全的数据传输。 其中一个问题：网站改用 HTTPS 以后，由 HTTP 跳转到 HTTPS 的方式增加了用户访问耗时（多数网站采用 301、302 跳转） [推荐文章]: http://support.upyun.com/hc/kb/article/1044299/ Google宣布了，从2017年1月份正式发布的Chrome 56开始，Google将把某些包含敏感内容的https页面标记为“不安全”。 HTTPS是趋势，然而Github不支持自定义域名的强制HTTPS，Coding国内的必须加广告才能强制HTTPS且没有中间跳转页。所以自己就开始了折腾HTTPS的道路。 折腾HTTPS：折腾：配置CloudFare失败终究会有免费的午餐，那就是CloudFare。 配置教程知识点 CloudFlare作为一家CDN提供商，他为免费用户提供的服务室不完整的，根据官网SSL服务的介绍，CloudFlare仅会在浏览器与CloudFlare的通讯中加密，CloudFlare与本地服务器的通讯本身并没有加密。这也是Flexible和Full模式的区别所在。 SSL：在客户端与服务器间传输的数据是通过使用对称算法（如 DES 或 RC4）进行加密的。公用密钥算法（通常为 RSA）是用来获得加密密钥交换和数字签名的，此算法使用服务器的SSL数字证书中的公用密钥。有了服务器的SSL数字证书，客户端也可以验证服务器的身份。SSL 协议的版本 1 和 2 只提供服务器认证。版本 3 添加了客户端认证，此认证同时需要客户端和服务器的数字证书。 关于Let&#39;s Encrypt：[给站点添加 https 小绿锁]: http://www.cnblogs.com/xinpureZhu/articles/lets-encrypt-to-add-https-site-little-green-lock.html 但是，都设置好了。然后就无法访问网页的了。DNS的锅。 发现阿里云的DNS修改后仍处于未更改的状态，即使CloudFlare显示Status：Active。 自己认为可能的是国外DNS被墙，或者是强制HTTPS生效时间过长导致没有及时生效。 返回Coding Pages 加广告无奈返回去添加广告。 知识点 Hexo ，快速、简单且功能强大的 Node.js 博客框架。 页面布局 Hexo中自己感觉一个很有意思的特点是，在_config.yml以及md文件中，通过对于指定字段的true或者false的设置来实现功能，对于使用者来说是黑盒操作，降低了使用难度，提高了使用的体验。这是一种很好的方式。 阿里云更改域名配置买了一个域名：lincentma.men 参考文章：[hexo博客添加域名实现双线部署（github和coding)]: http://blog.csdn.net/qiuchengjia/article/details/52923156 知识点 域名解析就是国际域名或者国内域名以及中文域名等域名申请后做的到IP地址的转换过程。IP地址是网路上标识您站点的数字地址，为了简单好记，采用域名来代替ip地址标识站点地址。域名的解析工作由DNS服务器完成。 域名解析的A：A (Address) 记录是用来指定主机名（或域名）对应的IP地址记录。用户可以将该域名下的网站服务器指向到自己的web server上。同时也可以设置您域名的二级域名。 域名解析的CNAME：别名记录。这种记录允许您将多个名字映射到另外一个域名。通常用于同时提供WWW和MAIL服务的计算机。 A记录就是把一个域名解析到一个IP地址（Address，特制数字IP地址），而CNAME记录就是把域名解析到另外一个域名。 还有MX记录（邮件记录）和NS记录（解析服务器记录，NS记录只对子域名生效） TODO [ ] 站点地图的HTTPS是安全的，其他页面是信息不安全的，why？ [ ] Ngnix与SSL [ ] 自己在Github博客申请SSL证书]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>https</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【解析】Java Skills For Interview]]></title>
    <url>%2Fjava_skills_for_interview.html</url>
    <content type="text"><![CDATA[【解析】Java Skills For Interview 从网上偶得之，把每个函数弄明白，源码是怎么写的，为什么这么写。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140//import不要使用通配符，需要import哪一个就import哪一个。import java.util.*; public class JavaSkillsForInterview &#123; public static void main(String[] args) &#123; // String String s = "abc"; s.charAt(0);//返回指定索引处的 char 值。 s.length();//返回此字符串的长度。 s.substring(0, 1);//返回字符串的子字符串。beginIndex - 起始索引（包括），endIndex - 结束索引（不包括）。左闭右开。 s.substring(1);//返回beginIndex - 起始索引（包括）到字符串末尾的子字符串。 s.equals("b");//将此字符串与指定的对象比较。 s = s.trim();//返回字符串的副本，忽略前导空白和尾部空白。用于删除字符串的头尾空白符。 s.indexOf("a");//返回指定字符在此字符串中第一次出现处的索引。如果此字符串中没有这样的字符，则返回-1。 s.indexOf("a", 1);//返回在此字符串中第一次出现指定字符处的索引，从指定的索引开始搜索。 s.lastIndexOf("a");//返回指定字符在此字符串中最后一次出现处的索引。 s.lastindexOf("a", 1);//返回指定字符在此字符串中最后一次出现处的索引，从指定的索引处开始进行反向搜索。 s.toCharArray();//【常用】将此字符串转换为一个新的字符数组。 Integer.valueOf(s); // returns an Integer object，valueOf会返回一个Integer（整型）对象 //【注意】Integer类有一个静态缓存，存储了256个特殊的Integer对象——每个对象分别对应`-128 和127之间的一个值。 // Integer.valueOf("127")==Integer.valueOf("127");【true】 // Integer.valueOf("128")==Integer.valueOf("128");【false】 Integer.parseInt(s); // returns an int primitive， 将形参s转化为整数。Interger.parseInt("1")=1; String.valueOf(s); // integer to string 返回指定参数的字符串表示形式。 // StringBuilder StringBuilder sb = new StringBuilder(); sb.append("a");//将指定的字符串追加到此字符序列。 sb.insert(0, "a"); //在index指示的字符之前插入指定的字符串。 sb.deleteCharAt(sb.length() - 1); //在这个序列中的指定位置，此方法将删除字符。 sb.reverse(); //此方法会导致此字符序列被替换为该序列的反转序列。 sb.toString(); //该方法返回一个字符串，它表示这个序列中的数据。 //String 长度大小不可变 //StringBuffer 和 StringBuilder 长度可变 //StringBuffer 线程安全 StringBuilder 线程不安全 //StringBuilder 速度快 // Array int[] a = new int[10];//初始化方式1 char[] b = &#123;'a', 'b'&#125;;//初始化方式2 int[][] c = new int[10][10];//二维数组 int m = a.length;//数组长度 int n = c[0].length;//二维数组的列数 int max = Integer.MAX_VALUE; //MAX_VALUE = 0x7fffffff （Java语言规范规定int型为4字节） int min = Integer.MIN_VALUE; //MIN_VALUE = 0x80000000 Arrays.sort(a);//数组升序排序 （import java.util.Arrays;）int[]，double[]，char[]等基数据类型的数组，只提供了默认的升序排列，没有提供相应的降序排列方法。 for (int i = 0; i &lt; c.length; i++) &#123; System.out.println(c[i]); &#125;// 遍历数组输出元素 // List //List是一个接口，而ArrayList是一个类。 ArrayList继承并实现了List。 List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();//List初始化 ArrayList&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); //ArrayList初始化 List&lt;List&lt;Integer&gt;&gt; list2 = new ArrayList&lt;List&lt;Integer&gt;&gt;(); //而为List list.add(0);//指定元素E追加到列表的末尾。此方法返回true。 list.add(0, 1);//方法将指定的元素E在此列表中的指定位置。此方法不返回任何值。 list.addAll(list1);//方法会将所有指定集合中的元素添加到此列表的结尾。 list.get(0);//此方法返回在此列表中的指定位置的元素。 list.size();//此方法返回此列表中的元素数。 list.remove(list.size() - 1);//此方法返回从列表中移除的元素。 //Collections是一个类而Collection是一个接口。 Collections.sort(list);//方法用于指定列表按升序进行排序，根据其元素的自然顺序。 Collections.sort(list, Collections.reverseOrder());//反序排列。 //自定义排序。根据Collections.sort重载方法来实现。 Collections.sort(list, new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer o1, Integer o2) &#123; return o1 - o2;// 0-&gt;1 // return o2 - o1; 1-&gt;0 &#125; &#125;); // Stack // import java.util.Stack; Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); stack.push(0);//把项item压入栈顶，返回item参数。 stack.pop();//返回位于堆栈顶部的元素，在这个过程中除去它。 stack.peek();//返回到堆栈顶部的元素，但不会将其删除。 stack.isEmpty();//栈是否为空。空返回true，否则false。 // isEmpty() 和 empty()的区别：命名区别。 // For example, it was named empty() in original class but was named isEmpty() of Collection interface. stack.size(); //栈中元素的个数。 stack.search("code"); //此方法返回从1开始的位置，一个对象在栈中。栈顶位置为1。 // Queue add ‐‐‐‐‐‐&gt; remove, peek // import java.util.Queue; import java.util.LinkedList; Queue&lt;Integer&gt; q = new LinkedList&lt;Integer&gt;(); q.add(0);//增加一个元索。 q.remove();//移除并返回队列头部的元素。 q.peek();//返回队列头部的元素。 //Queue使用也可以offer()来加入元素，使用poll()来获取并移出元素。 q.isEmpty();//返回队列是否为空。 q.size();//返回队列中元素的个数。 // HashMap // import java.util.HashMap; HashMap&lt;Character, Integer&gt; map = new HashMap&lt;Character, Integer&gt;();//初始化。 map.put('c', 1);//关联与此映射中的指定键指定的值。添加映射。 map.get('c');//返回指定键映射在此标识哈希映射，或者null，如果映射不包含此键的值。 if (map.containsKey('c')) &#123;//如果此映射包含指定键的映射关系返回true。 &#125; if (map.containsValue(1)) &#123;//如果此映射一个或多个键映射到指定值返回true。 &#125; for (Character d : map.keySet()) &#123;//遍历Hashmap的键集合。Set keySet()返回此映射中包含的键的set视图。 &#125; for (Integer i : map.values()) &#123;//遍历Hashmap的值集合。Collection values()返回此映射中包含的值的collection视图。 &#125; map.isEmpty();//如果此映射不包含键 - 值映射关系返回true。 map.size();//返回键 - 值映射关系在这个映射中的数量。 // HashSet // HashSet借助HashMap来实现的，利用HashMap中Key的唯一性，来保证HashSet中不出现重复值。 // HashMap中的Key是根据对象的hashCode() 和 euqals()来判断是否唯一的。 // import java.util.HashSet; HashSet&lt;Integer&gt; set = new HashSet&lt;Integer&gt;();//初始化。 set.add(0);//将指定的元素添加到此集合，如果它是不存在的。 set.remove(0);//从集合中删除指定的元素（如果存在）。 if (set.contains(0)) &#123;//如果此set包含指定的元素，则返回true。 &#125; set.isEmpty();//返回true如果此set不包含任何元素。 set.size();//返回元素的数目（它的基数）。 // mini heap // import java.util.PriorityQueue; PriorityQueue&lt;Integer&gt; pq = new PriorityQueue&lt;Integer&gt;();//初始化。 pq.add(0);//该方法调用返回true(所指定的Collection.add(E)) pq.offer(0);//该方法调用返回true(所指定的Queue.offer(E)) pq.remove();//该方法调用返回true，如果此队列由于调用而更改的结果。 pq.peek();//在方法调用返回此队列的头部，或null，如果此队列为空。但它不会将其删除。 pq.poll();//方法用于检索并移除此队列的头，则返回null，如果此队列为空。 pq.isEmpty();//判读是否为空。 pq.size();//在方法调用返回的元素在此集合数 while (!pq.isEmpty()) &#123; &#125; &#125;&#125; 阿里巴巴Java开发规范阿里巴巴Java开发规范]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树遍历那点事]]></title>
    <url>%2Fbinary_tree_traversal.html</url>
    <content type="text"><![CDATA[二叉树遍历那点事 刷leetcode，碰见二叉树，看了一下午二叉树遍历，还在茫然着，写出来就明白了。好脑子不如烂笔头。 二叉树结构123456789101112131415161718192021222324252627282930313233343536373839// Definition for binary treepublic class BinaryTreeNode &#123; private int data; private BinaryTreeNode left; private BinaryTreeNode right; public BinaryTreeNode() &#123;&#125; public BinaryTreeNode(int data, BinaryTreeNode left, BinaryTreeNode right) &#123; super(); this.data = data; this.left = left; this.right = right; &#125; public int getData() &#123; return data; &#125; public void setData(int data) &#123; this.data = data; &#125; public BinaryTreeNode getLeft() &#123; return left; &#125; public void setLeft(BinaryTreeNode left) &#123; this.left = left; &#125; public BinaryTreeNode getRight() &#123; return right; &#125; public void setRight(BinaryTreeNode right) &#123; this.right = right; &#125;&#125; 数组转换为二叉树 二叉树上的元素存放位置在数组中是固定的。 如果树的i位置（从0开始按层编号）有元素，就放在数组的i号位置，没有元素，数组对应的位置就空着。i的左右子树的编号为2i+1和2i+2。 123456789101112131415161718192021222324252627282930public class BinaryTree &#123; BinaryTreeNode root; class TreeNode &#123; int value; TreeNode left; TreeNode right; public TreeNode(int paraValue) &#123; this.value = paraValue; &#125; &#125; public BinaryTree(int[] array) &#123; root = createBinaryTreeByArray(array, 0); &#125; private TreeNode createBinaryTreeByArray(int[] array, int index) &#123; TreeNode tn = null; if (index &lt; array.length) &#123; int value = array[index]; tn = new TreeNode(value); tn.left = createBinaryTreeByArray(array, 2 * index + 1); tn.right = createBinaryTreeByArray(array, 2 * index + 2); return tn; &#125; return tn; &#125;&#125; 递归与非递归一言以蔽之，非递归比递归难太多。 前序递归遍历算法：访问根结点–&gt;递归遍历根结点的左子树–&gt;递归遍历根结点的右子树中序递归遍历算法：递归遍历根结点的左子树–&gt;访问根结点–&gt;递归遍历根结点的右子树后序递归遍历算法：递归遍历根结点的左子树–&gt;递归遍历根结点的右子树–&gt;访问根结点 明白了思路，递归代码几行就能搞定了。 但是问题来了： 摘自知乎： 递归是函数自身调用自身，涉及到保护现场（变量入栈，记录地址等），时间和空间开销较大，而这操作都是在栈上，调用层级太多很容易溢出。 迭代（非递归）虽然也是用栈，可是这个栈和递归栈可不是一个概念，这个栈完全可以在堆上开辟，空间更大，不容易溢出。迭代也不涉及函数调用，效率也更高。 如何写出非递归的遍历呢？如何把手写遍历结果的过程用代码表达出来。 非递归方法需要借助栈 前序二叉树递归前序遍历的递归定义：先根节点，后左子树，再右子树。 1234567public void preOrder(BinaryTreeNode root) &#123; if (null != root) &#123; System.out.print(root.val); preOrder(root.getLeft()); preOrder(root.getRight()); &#125;&#125; 非递归在手写结果的过程中，我们可以总结出以下规律： 画出二叉树的图，输出顺序都是沿左下方向的直线，输出顺序LIFO，后进先出，满足栈的定义。 借助栈来保存节点，通过节点来打印右子树的数据。 只有入栈时才输出数据 1234567891011121314151617public void preOrderNonRecursive(BinaryTreeNode root) &#123; Stack&lt;BinaryTreeNode&gt; stack = new Stack&lt;BinaryTreeNode&gt;(); while (true) &#123; //首先从根节点开始遍历所有的左子树，并输出节点数据 while (root != null) &#123; System.out.print(root.getData() + "\t"); stack.push(root); root = root.getLeft(); &#125; //循环终止条件：栈容量为0，说明右子树以及遍历完全，跳出。 if (stack.isEmpty()) break; //当上面的while循环完成后，也就是左子树输出完成，栈弹出元素，也就是离得最近的有右子树的节点。 root = stack.pop(); //返回循环，以该节点作为新的root循环输出。 root = root.getRight(); &#125;&#125; 中序二叉树递归中序遍历的递归定义：先左子树，后根节点，再右子树。 1234567public static void inorder(BinaryTreeNode root) &#123; if (root != null) &#123; inorder(root.nodeLeft); System.out.print(root.val); inorder(root.nodeRight); &#125;&#125; 非递归在手写结果的过程中，我们可以总结出以下规律： 与先序类似，只不过输出数据的时机发生了改变。 出栈的时候输出数据 123456789101112131415161718public void inOrderNonRecursive(BinaryTreeNode root) &#123; Stack&lt;BinaryTreeNode&gt; stack = new Stack&lt;BinaryTreeNode&gt;(); while (true) &#123; //遍历左子树，并保存到栈中 while (root != null) &#123; stack.push(root); root = root.getLeft(); &#125; //遍历跳出条件 if (stack.isEmpty()) break; //当上面的while循环完成后，也就是左子树输出完成，栈弹出元素，也就是离得最近的有右子树的节点。 root = stack.pop(); //根据中序遍历的输出顺序，先输出左子树的节点的值 System.out.print(root.getData() + "\t"); //以最左下的节点为例，它的右节点为null，那么再次循环时，就会跳过遍历左子树的循环，栈出栈的节点为它的父节点，也就是根节点，输出其值，再转为其右子树，完成中序遍历的输出顺序。 root = root.getRight(); &#125;&#125; 后序二叉树递归后序遍历的递归定义：先左子树，后右子树，再根节点。 1234567public static void postOrder(BinaryTreeNode root)&#123; if (root != null)&#123; postOrder(root.nodeLeft); postOrder(root.nodeRight); System.out.print(root.val); &#125;&#125; 非递归在手写结果的过程中，我们可以总结出以下规律： 与前面区别在于，判读出栈时，要考虑该节点是否在之前已经访问过。 在stack中，最后添加的数据需要通过lastElement方法获取。 1234567891011121314151617181920212223242526272829303132333435363738public void postOrderNonRecursive(BinaryTreeNode root)&#123; Stack&lt;BinaryTreeNode&gt; stack=new Stack&lt;BinaryTreeNode&gt;(); while(true)&#123; //遍历左子树，并保存到栈中 if(root!=null)&#123; stack.push(root); root=root.getLeft(); &#125;else&#123;//此时，当遍历到最左下的叶节点时，它的左子树是null。 //后序遍历的顺序 if(stack.isEmpty()) return; // 遍历终止条件1：栈空 //栈中最近添加的元素的右子树为空则弹出并输出该元素 if(null==stack.lastElement().getRight())&#123; root=stack.pop(); //输出最开始左子树的叶节点，也就是遍历顺序中的左子树（以及代入的右子树值） System.out.print(root.getData()+"\t"); //while循环终止条件1：出栈后的最近添加的元素则为父节点的右子树与之前出栈的元素相等 while(root==stack.lastElement().getRight())&#123; //root（右子树遍历完成）与此时栈中的最近添加的元素（父节点）的右子树相等，那么说明lastElement为根节点 //输出根节点 System.out.print(stack.lastElement().getData()+"\t"); //出栈（右子树的叶节点） root=stack.pop(); // while终止条件2：栈空 if(stack.isEmpty())&#123; break; &#125; &#125; &#125; //栈不空，则root赋值为栈最近添加元素的右子树，开始从右子树继续遍历 if(!stack.isEmpty()) root=stack.lastElement().getRight(); else //栈空，赋值为null，确保进入遍历结束条件判断的地方 root=null; &#125; &#125;&#125; PS：感觉有点绕，stack.lastElement()这个也可以用pre节点来代替，这样可能更便于理解。 非递归的统一化非递归是否也可以像递归一样，只用修改语句的位置，实现不同方式的遍历呢？ 答案是肯定的，大牛就是多。 统一三种更简单的非递归遍历方法的基本思想：有重合元素的局部有序一定能导致整体有序。 三种非递归遍历唯一不同的就是局部入栈的三行代码的先后顺序。所以不管是根-&gt;左-&gt;右,左-&gt;根-&gt;右,左-&gt;右-&gt;根,甚至是根-&gt;右-&gt;左,右-&gt;根-&gt;左,右-&gt;左-&gt;根定义的新顺序，算法实现都无变化，除了改变局部入栈顺序。 123456789101112131415161718192021222324void postorderTraversalNew(TreeNode *root, vector&lt;int&gt; &amp;path)&#123; stack&lt; pair&lt;TreeNode *, bool&gt; &gt; s; s.push(make_pair(root, false)); bool visited; while(!s.empty()) &#123; root = s.top().first; visited = s.top().second; s.pop(); if(root == NULL) continue; if(visited) &#123; path.push_back(root-&gt;val); &#125; else &#123; s.push(make_pair(root, true)); s.push(make_pair(root-&gt;right, false)); s.push(make_pair(root-&gt;left, false)); &#125; &#125;&#125; PS: 有时间改写成Java版本的。 12 参考文章]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>binary tree</tag>
        <tag>recursive</tag>
        <tag>iterate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java、Python与PHP的虚拟机异同]]></title>
    <url>%2Fprogram_language_virtual_machine.html</url>
    <content type="text"><![CDATA[Java-JVM定义 JDK(Java Development Kit) 是 Java 语言的软件开发工具包（SDK）。JDK 物理存在，是 programming tools、JRE 和 JVM 的一个集合 JRE（Java Runtime Environment）Java 运行时环境，JRE 物理存在，主要由Java API 和 JVM 组成，提供了用于执行 java 应用程序最低要求的环境。 JVM(Java Virtual Machine) 是一种软件实现，执行像物理机程序的机器（即电脑）。JVM 通过执行 Java bytecode 可以使 java 代码在不改变的情况下运行在各种硬件之上。JVM是基于栈的。 JVM 执行 加载代码 验证代码 执行代码 提供运行环境 JVM 生命周期 启动：任何一个拥有main函数的class都可以作为JVM实例运行的起点 运行：main函数为起点，程序中的其他线程均有它启动，包括daemon守护线程和non-daemon普通线程。daemon是JVM自己使用的线程比如GC线程，main方法的初始线程是non-daemon。 消亡：所有线程终止时，JVM实例结束生命。 JVM结构及内存模型 名词解释： Class Loader：类加载器负责加载程序中的类型（类和接口），并赋予唯一的名字。为什么使用双亲委托模型——ClassLoader 隔离问题。 Execution Engine：执行引擎。执行引擎以指令为单位读取 Java 字节码。它就像一个 CPU 一样，一条一条地执行机器指令。 Runtime Data Areas:：运行时数据区。 PS：想起面试的时候被问到过这样的问题：你在使用java过程中是否遇到过OOM的情况？当时一阵懵比。现在总结下： PC寄存器（PC Register）是唯一一个在 Java 虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 JVM 栈（Java Virtual Machine Stack）：如果JVM Stack可以动态扩展，但是在尝试扩展时无法申请到足够的内存去完成扩展，或者在建立新的线程时没有足够的内存去创建对应的虚拟机栈时抛出。 本地方法栈(Native method stack)：如果本地方法栈可以动态扩展，并且扩展的动作已经尝试过，但是目前无法申请到足够的内存去完成扩展，或者在建立新的线程时没有足够的内存去创建对应的本地方法栈，那Java虚拟机将会抛出一个OutOfMemoryError异常。 方法区(Method area)：如果方法区的内存空间不能满足内存分配请求，那Java虚拟机将抛出一个OutOfMemoryError异常。 运行时常量池(Runtime constant pool)：当创建类和接口时，如果构造运行时常量池所需的内存空间超过了方法区所能提供的最大内存空间后就会抛出OutOfMemoryError 堆(Heap)：如果实际所需的堆超过了自动内存管理系统能提供的最大容量时抛出。 总结一下就是无法申请到足够的内存以及超出最大容量两方面原因 垃圾回收 新生代新生代由 Eden 与 Survivor Space（S0，S1）构成，大小通过-Xmn参数指定，Eden 与 Survivor Space 的内存大小比例默认为8:1，可以通过-XX:SurvivorRatio 参数指定，比如新生代为10M 时，Eden分配8M，S0和S1各分配1M。 Eden：希腊语，意思为伊甸园，在圣经中，伊甸园含有乐园的意思，根据《旧约·创世纪》记载，上帝耶和华照自己的形像造了第一个男人亚当，再用亚当的一个肋骨创造了一个女人夏娃，并安置他们住在了伊甸园。 大多数情况下，对象在Eden中分配，当Eden没有足够空间时，会触发一次Minor GC，虚拟机提供了-XX:+PrintGCDetails参数，告诉虚拟机在发生垃圾回收时打印内存回收日志。 Survivor：意思为幸存者，是新生代和老年代的缓冲区域。当新生代发生GC（Minor GC）时，会将存活的对象移动到S0内存区域，并清空Eden区域，当再次发生Minor GC时，将Eden和S0中存活的对象移动到S1内存区域。 存活对象会反复在S0和S1之间移动，当对象从Eden移动到Survivor或者在Survivor之间移动时，对象的GC年龄自动累加，当GC年龄超过默认阈值15时，会将该对象移动到老年代，可以通过参数-XX:MaxTenuringThreshold 对GC年龄的阈值进行设置。 老年代老年代的空间大小即-Xmx 与-Xmn 两个参数之差，用于存放经过几次Minor GC之后依旧存活的对象。当老年代的空间不足时，会触发Major GC/Full GC，速度一般比Minor GC慢10倍以上。 永久代在JDK8之前的HotSpot实现中，类的元数据如方法数据、方法信息（字节码，栈和变量大小）、运行时常量池、已确定的符号引用和虚方法表等被保存在永久代中，32位默认永久代的大小为64M，64位默认为85M，可以通过参数-XX:MaxPermSize进行设置，一旦类的元数据超过了永久代大小，就会抛出OOM异常。 虚拟机团队在JDK8的HotSpot中，把永久代从Java堆中移除了，并把类的元数据直接保存在本地内存区域（堆外内存），称之为元空间。 这样做有什么好处？有经验的同学会发现，对永久代的调优过程非常困难，永久代的大小很难确定，其中涉及到太多因素，如类的总数、常量池大小和方法数量等，而且永久代的数据可能会随着每一次Full GC而发生移动。 而在JDK8中，类的元数据保存在本地内存中，元空间的最大可分配空间就是系统可用内存空间，可以避免永久代的内存溢出问题，不过需要监控内存的消耗情况，一旦发生内存泄漏，会占用大量的本地内存。 判断垃圾回收 引用计数法：在对象上添加一个引用计数器，每当有一个对象引用它时，计数器加1，当使用完该对象时，计数器减1，计数器值为0的对象表示不可能再被使用。引用计数法实现简单，判定高效，但不能解决对象之间相互引用的问题。 可达性分析法：通过一系列称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，搜索路径称为 “引用链”，以下对象可作为GC Roots： 本地变量表中引用的对象 方法区中静态变量引用的对象 方法区中常量引用的对象 Native方法引用的对象 当一个对象到 GC Roots 没有任何引用链时，意味着该对象可以被回收。 垃圾回收算法 标记-清除算法对待回收的对象进行标记。算法缺点：效率问题，标记和清除过程效率都很低；空间问题，收集之后会产生大量的内存碎片，不利于大对象的分配。 复制算法复制算法将可用内存划分成大小相等的两块A和B，每次只使用其中一块，当A的内存用完了，就把存活的对象复制到B，并清空A的内存，不仅提高了标记的效率，因为只需要标记存活的对象，同时也避免了内存碎片的问题，代价是可用内存缩小为原来的一半。 标记-整理算法在老年代中，对象存活率较高，复制算法的效率很低。在标记-整理算法中，标记出所有存活的对象，并移动到一端，然后直接清理边界以外的内存。 参考文章 [清蒸 JVM （一）]: http://www.importnew.com/23658.html [Java GC的那些事（上）]: http://www.importnew.com/23633.html [Java GC的那些事（下）]: http://www.importnew.com/23640.html PythonPVMPVM是Python的运行引擎。他通常表现为python系统的一部分。并且他是实际运行脚本的组件。 编译器：将源码编译成运行在虚拟机上执行的opcode(pyc文件)，pyc文件是在python虚拟机上执行的一种跨平台字节码。 运行时：虚拟机解释器把opcode(pyc文件)解释成具体机器的机器码，执行。 JVM与PVM Java代码从源程序到执行，要经过的过程是：编译器(javac)把源代码转化为字节码，然后解释器（Java.exe）把字节码转换为计算机理解的机器码来执行。其中编译器和解释器都是Java虚拟机（JVM）的一部分，由于针对不同的硬件与OS，Java解释器有所不同，因此可以实现“一次编译、到处执行”。所以JVM是Java跨平台特性的关键所在。 对于Python，其源代码到执行也要经过如下过程：源代码—&gt;字节码—&gt;机器码。与Java不同的是，Python使用的虚拟机是基于其他语言实现的，比如我们一般使用的Python实际为Cpython，也就是其虚拟机由C实现，这个虚拟机负责把Python源码编译为字节码，再解释执行。另外，还有Jypython、Ironpython等。 PHP-Zend&amp;HHVM Zend引擎默认做法，是先编译为opcode，然后再逐条执行，通常每条指令对应的是C语言级别的函数。如果我们产生大量重复的opcode（纯PHP写的代码和函数），对应的则是Zend多次逐条执行这些C代码。 HHVM生成和执行PHP的中间字节码（HHVM生成自己格式的中间字节码），执行时通过JIT（Just In Time，即时编译是种软件优化技术，指在运行时才会去编译字节码为机器码）转为机器码执行。JIT将大量重复执行的字节码在运行的时候编译为机器码，达到提高执行效率的目的。通常，触发JIT的条件是代码或者函数被多次重复调用。 写在最后时间匆忙，囫囵吞枣，努力完善。 后端开发离不开Java，python和php，深入学习原理，比较异同，最佳使用。 2017.06.23]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>java</tag>
        <tag>virtual machine</tag>
        <tag>php</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几个小问题的解答]]></title>
    <url>%2Flearn_for_some_questions.html</url>
    <content type="text"><![CDATA[她给我提了一下几个问题，自己一眼看上去觉得简单，但是说不出来所以然，犯了眼高手低的毛病，记录自省。 [x] 一共10级楼梯，每次可以走一步或两步，求一共多少种走法。 [x] 一个细胞，一个小时分裂一次，生命周期是3小时，求n小时后容器内，有多少细胞。 [x] 斐波那契数列的实现。 [x] 递归算法的核心 Question1—走楼梯思路楼梯问题，一眼看上去，应该是一个递归问题跑不了了。 那么该如何递归，找到其中的规律呢？ 逆向思考。 要想走到M(M=10)级,可以分为2种情况。 从m-2级迈两步 从m-1级迈一步 那么对于m-2和m-1的情况也是各自分为两种，以此类推。 那么走法的和就是m-2的走法和m-1的走法之和。 那么递归到最基本的（当前人在第0阶台阶） 第0阶台阶：0 第1阶台阶：1 第2阶台阶：2（1+1或者2） 得到公式，也就是斐波那契数列。$$f(n)=f(n-1)+f(n-2)$$ 代码1234567891011121314151617181920212223242526272829import java.util.Scanner;/** * Created by ml on 2017/6/21. */public class taijie &#123; public static int countNumber(int stepsNum) &#123; int sum = 0; if (stepsNum == 0) &#123; return 0; &#125; if (stepsNum == 1) &#123; return 1; &#125; else if (stepsNum == 2) &#123; return 2; &#125; else if (stepsNum &gt; 2) &#123; return countNumber(stepsNum - 2) + countNumber(stepsNum - 1); &#125; return sum; &#125; public static void main(String[] args) &#123; long startMili=System.currentTimeMillis(); for (int i = 0; i &lt;= 10; i++) &#123; System.out.println("楼梯台阶数:" + i + ", 走法有:" + countNumber(i)); &#125; long endMili=System.currentTimeMillis(); System.out.println("耗时:" + String.valueOf(endMili - startMili) + "毫秒"); &#125;&#125; 1234567891011121314楼梯台阶数:0, 走法有:0楼梯台阶数:1, 走法有:1楼梯台阶数:2, 走法有:2楼梯台阶数:3, 走法有:3楼梯台阶数:4, 走法有:5楼梯台阶数:5, 走法有:8楼梯台阶数:6, 走法有:13楼梯台阶数:7, 走法有:21楼梯台阶数:8, 走法有:34楼梯台阶数:9, 走法有:55楼梯台阶数:10, 走法有:89耗时:0毫秒Process finished with exit code 0 其他的方法该算法的运算时间是指数级增长的，还有其他方法吗？ 动态规划？ 其实动态规划（dynamicprogramming）是通过组合子问题而解决整个问题的解。乍一看和递归的写法差不多，都是相加。但是递归式是包含了许多重复计算的步骤，对应台阶就是每一个台阶计算前面都是重复的。动态规划算法对每个子问题只求解一次，将其结果保存起来，从而避免每次遇到各个子问题时重新计算答案。 代码如下： 1234567891011121314151617181920212223242526272829import java.util.Scanner;/** * Created by ml on 2017/6/21. */public class taijie &#123; public static int climbStairs(int n) &#123; if (n == 0 || n == 1 || n == 2) &#123; return n; &#125; //注意坐标起始点的区别 int[] r = new int[n+1]; r[1] = 1; r[2] = 2; for (int i = 3; i &lt;= n; i++) &#123; r[i] = r[i-1] + r[i-2]; &#125; return r[n]; &#125; public static void main(String[] args) &#123; long startMili=System.currentTimeMillis(); for (int i = 0; i &lt;= 10; i++) &#123; System.out.println("楼梯台阶数:" + i + ", 走法有:" + climbStairs(i)); &#125; long endMili=System.currentTimeMillis(); System.out.println("耗时:" + String.valueOf(endMili - startMili) + "毫秒"); &#125;&#125; 1234567891011121314楼梯台阶数:0, 走法有:0楼梯台阶数:1, 走法有:1楼梯台阶数:2, 走法有:2楼梯台阶数:3, 走法有:3楼梯台阶数:4, 走法有:5楼梯台阶数:5, 走法有:8楼梯台阶数:6, 走法有:13楼梯台阶数:7, 走法有:21楼梯台阶数:8, 走法有:34楼梯台阶数:9, 走法有:55楼梯台阶数:10, 走法有:89耗时:0毫秒Process finished with exit code 0 之前数学课本上如何思考的？ 先求出走完台阶需要几步？ 再求出总步数中，走一个台阶是几步，走两个台阶式几步，不同种类为排列组合计算 类似问题再看一个硬币的凑法： 用1分、2分和5分的硬币凑成1元，共有多少种不同的凑法？（华为机试题） 它和走楼梯的区别在于，楼梯不同顺序是不同的走法，硬币则与顺序无关，程序可用穷举法。 123456789101112131415161718192021public class coin &#123; public static int getKinds()&#123; int num=0; for (int i = 0; i &lt; 21; i++) &#123; for (int j = 0; j &lt;= 100-5*i; j++) &#123; for (int k = 0; k &lt;= 100-5*i-2*j; k++) &#123; if(5*i+2*j+k==100)&#123; num++; &#125; &#125; &#125; &#125; return num; &#125; public static void main(String[] args) &#123; System.out.println(getKinds()); &#125;&#125; 那么这个问题可以推广为一般性问题吗？M阶，一次走a阶或者b阶或者。。。，求不同走法。 参考文章Question2—细胞分裂非递归思路乍一看，关键是如何处理生命周期是3小时。如何在分裂的同时提出死亡的细胞。 代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import java.util.ArrayList;import java.util.HashMap;import java.util.Iterator;import java.util.Map;/** * Created by ml on 2017/6/22. * 思路：每个细胞定义为一维数组的元素，元素的值为细胞的生存时间，每分裂一次，在数组末尾添加新的元素，初始值为0，同时原来的元素值加1 * 当元素的值为3时，将该元素剔除 * 最后返回该数组的大小 */public class xibao &#123; public static int cellNum2(int n) &#123; ArrayList&lt;Integer&gt; cell = new ArrayList&lt;Integer&gt;(); //定义cellnum int cellnum = 0; //加入初始细胞 cell.add(0); for (int i = 0; i &lt; n; i++) &#123; //记录当前数组大小（剔除完成后的数组） int size = cell.size(); //计算分裂后新的数组的大小 cellnum = 2 * cell.size(); //原有生存时间加1 for (int j = 0; j &lt; cell.size(); j++) &#123; cell.set(j, cell.get(j) + 1); &#125; //遍历剔除元素值为3 PS：ArrayList中遍历可以删除的只有迭代器 Iterator&lt;Integer&gt; it = cell.iterator(); while (it.hasNext()) &#123; if (it.next().equals(3)) it.remove(); &#125; //在数组末尾添加新的元素 for (int j = size; j &lt; cellnum; j++) &#123; cell.add(0); &#125; &#125; return cell.size(); &#125; public static int cellNum(int n) &#123; HashMap&lt;Integer, Integer&gt; cell = new HashMap&lt;&gt;(); int cellnum = 1; for (int i = 1; i &lt;= n; i++) &#123; cellnum *= 2; int size = cell.size(); for (int j = size; j &lt; cellnum; j++) &#123; cell.put(j, 0); &#125; Iterator&lt;HashMap.Entry&lt;Integer, Integer&gt;&gt; iter = cell.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = iter.next(); int key = (int) entry.getKey(); int val = (int) entry.getValue(); if (key &lt; size) &#123; cell.put(key, val + 1); &#125; &#125; &#125; return cell.size(); &#125; public static void main(String[] args) &#123; long startMili = System.currentTimeMillis(); int n = 6; System.out.println(n + "小时后容器细胞:" + cellNum2(n) + "个"); long endMili = System.currentTimeMillis(); System.out.println("耗时:" + String.valueOf(endMili - startMili) + "毫秒"); &#125;&#125; 12343小时后容器细胞:7个耗时:0毫秒Process finished with exit code 0 关于ArrayList 迭代器是作为当前集合的内部类实现的，当迭代器创建的时候保持了当前集合的引用； 集合内部维护一个字段叫modiCount，用来记录集合被修改的次数，比如add，remove，set等都会使该字段递增； 迭代器内部也维护着当前集合的修改次数的字段，迭代器创建时该字段初始化为集合的modiCount值 当每一次迭代时，迭代器会比较迭代器维护的字段和modiCount的值是否相等，如果不相等就抛ConcurrentModifiedException异常； 当然，如果用迭代器调用remove方法，那么集合和迭代器维护的修改字数都会递增，以保持两个状态的一致。 这就是为什么你只可以用迭代器来删除，而不能用其他方式来修改集合。 这类问题如何递归？公式法：$$n=t/a~~~~y=2^n$$但是这个只适用于最简单的没有任何附加状态条件的情况。 递归如何来分析？ 细胞的生存周期是3个小时，那我们就可以把细胞在题目中状态分为以下几个状态： a：刚分裂态——1 b：分裂1小时态——a分裂出b和a c：分裂2小时态——b分裂出c和a d：分裂3小时态——死亡（停止分裂） 那么，我们就可以根据细胞状态设定函数。分析每一个状态的来源是哪里即可。$$a(t)=a(t-1)+b(t-1)+c(t-1)\\b(t)=a(t-1)\\c(t)=b(t-1)\\d(t)=d(t-1)+c(t-1)$$容器中存活的细胞数目就是a、b、c三种状态数量的总和。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.util.ArrayList;import java.util.HashMap;import java.util.Iterator;import java.util.Map;/** * Created by ml on 2017/6/22. */public class xibao &#123; public static int aStatus(int t) &#123; if (t == 0) &#123; return 1; &#125; return aStatus(t - 1) + bStatus(t - 1) + cStatus(t - 1); &#125; public static int bStatus(int t) &#123; if (t == 0) &#123; return 0; &#125; return aStatus(t-1); &#125; public static int cStatus(int t) &#123; if (t == 0) &#123; return 0; &#125; if (t == 1) &#123; return 0; &#125; return bStatus(t-1); &#125; public static int dStatus(int t) &#123; if (t == 0) &#123; return 0; &#125; if (t == 1) &#123; return 0; &#125; if (t == 2) &#123; return 0; &#125; return dStatus(t-1) + cStatus(t-1); &#125; public static void main(String[] args) &#123; long startMili = System.currentTimeMillis(); int n = 3; int res = aStatus(n) + bStatus(n) + cStatus(n); System.out.println(n + "小时后容器细胞:" + res + "个"); long endMili = System.currentTimeMillis(); System.out.println("耗时:" + String.valueOf(endMili - startMili) + "毫秒"); &#125;&#125; 12343小时后容器细胞:7个耗时:0毫秒Process finished with exit code 0 参考文章Question3—Fibonacci数列经典问题：求Fibonacci数列前n项。$${an}：a1=1，a2=1，a_{n+2}=a_{n+1}+a_n（n≥1）。$$ PS: Markdown 中公式书写规则 \\符号后接的字符为上标 ^符号后接的字符为上标 _符号后接的字符为下标 如果同时有两个下标，则需要使用{}来将符号括起来 代码1234567891011121314151617181920212223242526272829303132333435public class fab &#123; public static void main(String[] args) &#123; for(int i = 1; i &lt; 10; i ++)&#123; System.out.print(recursion(i) + " "); &#125; System.out.println(); for(int i = 1; i &lt; 10; i ++)&#123; System.out.print(loop(i) + " "); &#125; &#125; /** * 递归 */ public static int recursion(int n)&#123; if(n &lt;= 2)&#123; return 1; &#125; return recursion(n -1) + recursion(n -2); &#125; /** * 循环 */ public static int loop(int n)&#123; if(n &lt;= 2)&#123; return 1; &#125; int s1 = 1,s2 = 1, sum = 0; for(int i = 0 ; i &lt; n - 2; i ++)&#123; sum = s1 + s2; s1 = s2; s2 = sum; &#125; return sum; &#125;&#125; 1231 1 2 3 5 8 13 21 34 1 1 2 3 5 8 13 21 34 Process finished with exit code 0 值得注意的是（递归方法有栈溢出的风险）。 参考文章Question4—递归核心那么总结一下，递归算法的核心是什么呢？ 那就是： 在有限次可预见性结果中，找到结果与上一次结果之间的关系。 f(n)与f(n-1)的关系有时候很简单，如同走楼梯，状态单一；又有时如同细胞分裂，多种状态组合影响结果。 关键在于梳理清楚本次结果和上一次结果的关系有哪些方面或是因素，刚开始看的时候可能会很乱。 在草稿纸上写出前几次的结果，或者画图，这样更容易找到规律，这种规律实际上就是递归方程。 *在算法的分析中，当一个算法中包含递归调用时，其时间复杂度的分析会转化成为一个递归方程的求解。而对递归方程的求解，方法多种多样，不一而足。 动态规划就是在递归的基础上，保存每一步的数据，避免重复计算。在递归计算调用次数过多时，可以考虑更换其他方法解答。]]></content>
      <categories>
        <category>开发笔记</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rank小记]]></title>
    <url>%2Flearn_rank.html</url>
    <content type="text"><![CDATA[搜索引擎，你每天用的，知其然，不知其所以然。 ——读《搜索引擎 : 信息检索实践》 信息检索的定义信息检索是关于信息的结构、分析、组织、存储、搜索和检索的领域——Gerard Salton 搜索引擎的指标处理数十亿网页的商业化网络搜索引擎时代的今天，搜索引擎的指标体现在以下五个方面： 全 新 准 快 稳 其中Rank的目标就是准 Rank如何所搜即所得是Rank的目的。 海量网页快速排序 相关性：搜索结果与用户需求的匹配程度 多样性（Query对应多个结果，通过用户行为数据进行选择和匹配） 权威性（被引用次数，链接分析。类似于学术文章的因子。） 时效性（Query的频次随时间的变化趋势） 个性化（构建用户个人数据，计算结果与用户喜好匹配程度） 用户成本 如何排序自己的理解，排序就是算分，按照分数来进行排序，key是算分。 经典IR模型:TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。距离特征 Boolean Model：布尔（Boolean）模型是基于集合论和布尔代数的一种简单检索模型。它的特点是查找那些于某个查询词返回为“真”的文档。在该模型中，一个查询词就是一个布尔表达式，包括关键词以及逻辑运算符。通过布尔表达式，可以表达用户希望文档所具有的特征。 Vetor-Space:VSM概念简单，把对文本内容的处理简化为向量空间中的向量运算，并且它以空间上的相似度表达语义的相似度，直观易懂。当文档被表示为文档空间的向量，就可以通过计算向量之间的相似性来度量文档间的相似性。文本处理中最常用的相似性度量方式是余弦距离 Rank关键技术倒排索引在线计算转为离线计算分布式计算系统 Cache辣鸡信息：找出特征，计算分数，剔除。 人工识别与机器学习。Learning to Rank 找了一些文档博客，慢慢学习~ Point-Wise：RankSVMPair-Wise：RankNetList-Wise：RankForest 努力学算法一定可以搞懂的！]]></content>
      <categories>
        <category>intern</category>
      </categories>
      <tags>
        <tag>rank</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为找工作而准备着]]></title>
    <url>%2Fprepare_for_work.html</url>
    <content type="text"><![CDATA[为找工作而准备着行业了解互联网+行业——改变传统行业 未来的行业是属于数据驱动的行业： 物流行业（快递物流数据），物流成本仍有巨大空间优化 金融行业（金融数据），普惠金融有广阔的空间 电商行业（交易数据），定制化，差异化，个性化会随着物质条件的不断提高而不断重要 视频行业（视频数据），视频对于文件具有更有效地传播 农业（作物，天气等数据），高品质农业作物的需求不断上升，小作坊式模式对于行情波动的适应性有巨大的改善空间 云（云数据），理论上任何公司的数据都可以上云来进行弹性管理，提高效率 新兴科技（新兴技术），探索无止境，更多广阔的市场是由新兴技术所开创 公司了解 物流行业：菜鸟物流 金融行业：蚂蚁金融、支付宝、微信支付 电商行业：阿里巴巴、京东、美团点评、网易严选考拉 视频行业：今日头条、腾讯视频、B站 农业：网易未央、京东、联想农业 云：阿里云、腾讯云 新兴科技：大疆科技 部门了解 技术营销：通过基础数据分析，找到赋能新方法 基础研发：如何让数据更加适合的展现 产品研发：如何研发用用户不易察觉的方式解决用户痛点的产品 内部研发：研发如何提高开发效率的工具 技术栈了解Unix/Linux 后端必须掌握的操作系统。建议的书籍：《Linux编程》《Unix环境高级编程》 网络编程 建议书籍：《Unix网络编程》《TCP/IP协议详解》 脚本语言PHP、Python深入学习。 数据库 无论是关系型数据库还是非关系型数据库，都是必须要吃透牢牢掌握的东西 工作使命感互联网的技术日新月异的目标就是让人们更加方面的获取一切，享受科技带来的便利； 不可否认互联网技术同样是一把双刃剑，人们在低成本享受的同时，同时也会出现键盘侠、恶意抹黑、舆论控制等让社会阴暗面放大的趋势。 如同微博让人们无所不谈，同时也会有水军，造谣；大疆无人机让我们获得前所未有的影像视角，同时也被用于装备军队送上战场。 在致力于互联网技术和产品不断发展的同时，同时还需要致力于让技术和产品如何正确地服务于人，而避免用于歧途。 数据亦然，发掘数据蕴含的价值创造正能量价值，尽所能。]]></content>
      <categories>
        <category>job</category>
      </categories>
      <tags>
        <tag>job</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在百度实习的日子]]></title>
    <url>%2Finternship_in_baidu.html</url>
    <content type="text"><![CDATA[在百度实习的日子 转眼从百度离开回到了在学校的日子。也应该记录下自己在帝都，在百度的255天。 在百度糯米在百度糯米第一次接触了规范的开发流程，公司庞大的知识储备库，让自己如同海绵一样广泛吸水般吸收知识。有非常nice团队来让我迅速适应这样的开发环境，让我熟悉开发流程，带我一起和产品交流、和运营交流、和测试交流。 在百度搜索在百度搜索我接触到了搜索是如何运转，自己尝试接触Sug的海量数据进行数据分析监控，第一次理解亲身理解大数据所蕴含的价值。一次次的技术分享，让我更加深刻理解百度搜索里面的核心技术的深度，自己仍有很长的路要走。 在百度目前为止我见过最好的办公环境，nice的伙食，健身设备，班车交通，让我每个周末也来公司给自己充电。每天的问好，晚上的坐上班车返回住所，每一天任务的完成带给自己的充实感让自己不断向前。体验过996，体验过封闭开发，体验过部门内分享，体验过酸甜苦辣。感谢百度，感谢曾经的同事，感谢让自己不断成长。]]></content>
      <categories>
        <category>intern</category>
      </categories>
      <tags>
        <tag>intern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo之重新初始化]]></title>
    <url>%2Fhexo_init.html</url>
    <content type="text"><![CDATA[站在巨人的肩膀上，搭建Hexo，事半功倍。但是巨人太多，选择好的巨人很重要，不然就不得其法，回到原点。 Hexo参考文档 [NexT]: http://theme-next.iissnan.com/getting-started.html “NexT” [NexT优化]: http://shenzekun.cn/hexo%E7%9A%84next%E4%B8%BB%E9%A2%98%E4%B8%AA%E6%80%A7%E5%8C%96%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html Hexo搭建问题记录 多说评论替换 背景效果 腾讯云搭建Hexo Hexo 性能优化 [hexo博客进阶－性能优化]: https://www.liuxinggang.com/2016-12-06-hexo%E5%8D%9A%E5%AE%A2%E8%BF%9B%E9%98%B6%EF%BC%8D%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/ 自己更换电脑，没有保存Hexo的源文件，借此机会重新部署学习，完善自己的博客。]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>